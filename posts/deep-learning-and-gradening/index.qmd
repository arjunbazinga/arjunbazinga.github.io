---
title: "On Deep Learning and Gardening: We're Still Early"
layout: post
description: "What agriculture can teach us about AI development"
categories: [Analogy, Future]
date: "2025-03-05"
---

# On Deep Learning and Gardening: We're Still Early

This is one of those posts where I explore what happens when we map concepts from one field to another. Today's question: What if deep learning is fundamentally like gardening?

## Building vs Growing: Two Ways to Create

There are two fundamentally different ways to make things:

**Engineering:** You understand how components work and compose them deliberately - like building a table from wood and screws, or scaling up to rockets and engines.

**Cultivation:** You can't directly build a sunflower by assembling parts, but you can grow one by preparing soil, planting a seed, and providing water and sunlight.

Deep learning is like cultivation. We prepare environments, plant architectures, and nurture them to grow capabilities. We're still early in understanding what truly feeds these systems.

## The Gardening Analogy Unpacked

The parallels run deep:

- **Hyperparameters are growing conditions** - Learning rates are watering schedules, batch sizes are pot sizes
- **Optimization methods are cultivation techniques** - SGD is traditional farming (reliable but inefficient), Adam is drip irrigation (precise resource delivery), and second-order methods are hydroponics (expensive but enabling growth where traditional methods fail, training models that wouldn't otherwise converge)
- **Compute clusters are greenhouses** - TPUs are industrial setups, consumer GPUs are home gardens
- **Regularization is pruning** - Techniques like dropout trim complexity to improve generalization

## Genetic Limits of Architectures

Different neural architectures have inherent capability ceilings, similar to plant height limits:

- **CNNs are bamboo** - Fast growth, early results, but plateau at lower performance ceilings
- **RNNs are height-limited trees** - Solid structure but inherent limitations (vanishing gradients)
- **Transformers are redwoods** - Higher ultimate potential, though still bound by architectural ceilings

This explains why both scaling AND architectural innovation remain crucial. Just as no amount of perfect soil makes a shrub grow into a sequoia, architectural limitations constrain what even the best-trained models can accomplish.

## Different Plants for Different Soils

Architectures, like plant species, specialize for specific environments. We've mastered text "soil" (RNNs→Transformers) but lack equally effective architectures for video and audio data - we're growing text-optimized plants in non-text soils.

Breakthroughs may come from specialized architectures for these untapped resources, like nitrogen-fixing plants evolved symbiotic relationships with soil bacteria. These video/audio architectures would scale as effectively as Transformers do for text.

Nitrogen-fixing plants mostly resemble regular plants with specialized roots. Similarly, effective non-text architectures likely resemble Transformers with modified components (tokenizers, output layers).

Like crop varieties revolutionizing agriculture in new soil types, architectural "species" for non-textual data could unlock vast untapped potential.

## The Pre-Fertilizer Era of AI

We're still in what could be called a "pre-fertilizer" era, working with what nature provides:
- Collecting datasets "as they come" (raw soil)
- Filtering out obvious problems (removing stones)
- Scaling up quantity rather than transforming quality (using more land)

The agricultural revolution didn't happen by using more land or working harder. It came from understanding exactly what nutrients crops needed.

## The Key Insight: Fertilizer Isn't Soil

Synthetic examples don't need to be "realistic" to be effective. Fertilizer looks nothing like soil, yet delivers precisely what plants need.

Look at Microsoft's Phi models: trained on synthetic "textbook-quality" data rather than massive web scrapes. The result? Phi-1.5 (1.3B parameters) outperforms models 25× its size on reasoning tasks. This is the equivalent of concentrated fertilizer outperforming vast amounts of natural soil.

Other examples:
* Stanford's 1,000 carefully crafted reasoning examples creating stronger capability than millions of natural samples
* Chain-of-thought prompting creating "deliberate thinking paths" unlike natural human text
* Training signals that isolate specific features without the noise found in natural data

The future isn't more realistic synthetic data - it's more effective synthetic data optimized for neural learning dynamics, not human recognizability.

## Implications and Future Directions

If this analogy holds, history suggests:

- **The Fertilizer Revolution** - Engineered data will transform model performance with less raw material
- **Monoculture Risks** - AI needs architectural diversity. iPhone zero-days are scary; "Transformer zero-days" would be worse - adversarial attacks compromising all similar AI simultaneously
- **Hybridization Breakthroughs** - Major advances may come from crossing architectural approaches

## Where Will Value Accrue?

Looking at agriculture's evolution offers clues:

- **Seed Companies** - AI differs as techniques aren't easily patented, favoring those who operationalize quickly
- **Fertilizer Producers** - Companies developing novel data generation could become crucial
- **Value-Added Processing** - ChatGPT is valuable like McDonald's is valuable - both transform low-margin raw materials (beef/base models) into high-margin end products through consistent delivery, packaging, and user experience
- **Distribution Networks** - Unlike agriculture, digital distribution is nearly costless (bits vs. biomass). However, inference still requires substantial infrastructure - you need "land" to grow models and different "land" to keep them producing value, making compute providers more like utility companies than distributors

## Final Thoughts

A fundamental breakthrough will likely come from reimagining what constitutes effective training data.

When we truly understand the "nutrients" that feed neural networks, we'll create synthetic training approaches that seem almost incomprehensible by today's standards - concentrated learning signals that don't resemble test data but drive exponential improvements in capability.

While innovations will continue across architectures, optimization methods, and deployment, this "fertilizer revolution" represents one of the most underexplored and potentially transformative frontiers in AI development.

## Where the Analogy Breaks Down

Analogies are mental models with necessary simplifications. Key differences include:

* **Distribution economics** - Digital distribution has near-zero cost vs. physical agriculture
* **No decay** - AI models don't rot
* **Expandable "land" (temporary)** - We can build compute clusters rapidly, resembling early frontier expansion, before eventual physical/regulatory constraints
* **Rapid improvements (temporary)** - Models dramatically outperform predecessors, like early selective breeding before improvements became incremental

These differences mainly affect short-term dynamics. Long-term, parallels strengthen as AI faces agriculture-like constraints: limited resources, regulation, and incremental improvements.

This suggests agricultural history can still inform AI's future trajectory, despite different timescales and mechanisms.