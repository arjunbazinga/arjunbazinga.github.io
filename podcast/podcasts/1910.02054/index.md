---
title: "ZeRO Memory Optimizations: Toward Training Trillion Parameter Models"
categories: [Performance, Systems, Memory, Large Scale Model Training, Deep Learning]
date: "2024-07-08"
arxiv-paper-id: "1910.02054"
---

The paper introduces ZeRO, a novel approach to optimize memory usage when training massive language models. ZeRO-DP and ZeRO-R components effectively reduce memory redundancy and allow for training models with up to 170 billion parameters efficiently. The technique shows superlinear scalability, user-friendly implementation, and has the potential to democratize large model training in AI research.