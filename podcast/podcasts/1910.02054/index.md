---
title: "ZeRO Memory Optimizations: Toward Training Trillion Parameter Models"
categories: ['Systems and Performance', 'Deep Learning', 'Natural Language Processing']
date: 2024-07-08T19:18:11+0530
arxiv-paper-id: "1910.02054"
---

The paper introduces ZeRO, a novel approach to optimize memory usage when training massive language models. ZeRO-DP and ZeRO-R components effectively reduce memory redundancy and allow for training models with up to 170 billion parameters efficiently. The technique shows superlinear scalability, user-friendly implementation, and has the potential to democratize large model training in AI research.