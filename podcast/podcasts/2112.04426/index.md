---
title: "Retrieval-Enhanced Transformers (RETRO): A Semi-Parametric Approach to Enhance Performance of Large Language Models"
categories: ['Natural Language Processing', 'Deep Learning', 'Systems and Performance']
date: "2024-07-20"
arxiv-paper-id: 2112.04426
---
The paper introduces the RETRO model, which leverages retrieval from a massive text database to enhance large language model performance without increasing model size. Key takeaways include the benefits of linear time complexity for retrieval, the use of frozen BERT for efficient retrieval, and the importance of addressing test set leakage in evaluation.
