[
  {
    "objectID": "posts/newbie-quiz/index.html",
    "href": "posts/newbie-quiz/index.html",
    "title": "Newbie quiz",
    "section": "",
    "text": "Newbie Quiz\n\n\nIndraneel and I had prepared this quiz as a way of introducting quizzing to new people and narrowing down potential recruits. The quiz covers a broad range of topics, from pop culture, sports, technology, politics\nLots of fun!\n\nQuestions\nGet the questions.\n\n\nAnswers\nGet the answers.\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Writings",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nOn SaaS and Swimming: Hidden Factors of Success\n\n\n\n\n\n\nBusiness\n\n\n\nHow to figure out what matters in business and sports\n\n\n\n\n\nAug 12, 2023\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentation Platforms\n\n\n\n\n\n\nExperimentation\n\n\n\nAn Overview of Experimentation Platforms.\n\n\n\n\n\nMar 21, 2021\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Schedule Generator\n\n\n\n\n\n\nIIT Indore\n\n\n\nA website which adds courses to your personal calendar, for students of IIT Indore.\n\n\n\n\n\nJan 8, 2020\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nHarry Potter quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA beginner friendly quiz on the wizarding world\n\n\n\n\n\nAug 26, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nThe state of reading in 2018 and beyond.\n\n\n\n\n\n\nFuture\n\n\n\n\n\n\n\n\n\nAug 5, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nFriend Or Foe\n\n\n\n\n\n\nAI\n\n\nProjects\n\n\n\nAn interactive game inspired by the paper “AI Safety Grid Worlds”\n\n\n\n\n\nJul 28, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nInfant Mortality In India\n\n\n\n\n\n\nAnalysis\n\n\n\nAn analysis of infant mortality rates across India.\n\n\n\n\n\nJun 15, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nJargonizer\n\n\n\n\n\n\nProjects\n\n\n\nGenerate sentences that nobody can read.\n\n\n\n\n\nJun 12, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Machine Learning\n\n\n\n\n\n\nAI\n\n\n\n\n\n\n\n\n\nNov 12, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nMulti Armed Bandits\n\n\n\n\n\n\nAI\n\n\n\nor how to balance exploration and exploitation more formally\n\n\n\n\n\nNov 12, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nGame Of Thrones quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz made for cultural week\n\n\n\n\n\nSep 4, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nNewbie quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz to introduce quizzing to new people and narrowing down potential recruits.\n\n\n\n\n\nAug 20, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nMental Health Checklists\n\n\n\n\n\n\nProjects\n\n\n\nA website which allows quick tracking of mental health.\n\n\n\n\n\nJan 8, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this website\n\n\n\n\n\n\nMeta\n\n\n\na meta post about this website\n\n\n\n\n\nJan 1, 2016\n\n\nArjun Srivastava\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/infant-mortality/index.html",
    "href": "posts/infant-mortality/index.html",
    "title": "Infant Mortality In India",
    "section": "",
    "text": "I thought about this after reading Doing good better.\nSoon after I found out about kepler.gl which made this the perfect project to test it out.\nI extracted the data I needed for my analysis from a PDF of an annual summary report by the Government of India, available here, as I couldn’t find a comprehensive and user-friendly source.\nYou can read the analysis here.\nHere is the kepler.gl map I created for this analysis.\nIt contains the following layers:\n\nTotal Lives Lost: The number of infants who died in each state. The height of each region is proportional to the number of lives lost, with redder regions having a higher infant mortality rate.\nTotal Population : The population of each state. The height of each region is proportional to the population, with bluer regions having a higher birth rate.\nTotal Lives Lost Rural: The same as Total Lives Lost, but only for rural areas in each state.\nTotal Lives Lost Urban: The same as Total Lives Lost, but only for urban areas in each state.\n\nOnce the map loads you can click on the layers on the right to toggle them on and off. You can also click on the layers to see the data for each region.\nThe json file for the map is available here.\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/mental-health-checklist/index.html",
    "href": "posts/mental-health-checklist/index.html",
    "title": "Mental Health Checklists",
    "section": "",
    "text": "Some quizes I made so it’s easy to keep track of mental health.\n\nBurn’s depression Checklist use it here\nNovaco’s Anger Scale use it here\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/saas-and-swimming/index.html",
    "href": "posts/saas-and-swimming/index.html",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "",
    "text": "Sometimes, insight comes from unexpected places. Today, it came from the pool.\nToday while swimming, I had a realization: there’s a lot in common between swimming well and running a successful subscription business."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-basics-drag-thrust-and-speed",
    "href": "posts/saas-and-swimming/index.html#the-basics-drag-thrust-and-speed",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Basics: Drag, Thrust, and Speed",
    "text": "The Basics: Drag, Thrust, and Speed\nWhen you first start swimming, you think it’s all about moving your hands and legs as powerfully as possible. More force equals more speed, right? This is partly true, but it misses a crucial element: drag.\nWhat matters when swimming is your average velocity over a distance. If you have too much drag, you’ll return to zero velocity after each stroke. But when your drag is low enough, you will accumulate speed with each stroke, reaching a top speed much higher than you would even with your strongest stroke.\nIn business terms:\n\nDrag is your Churn rate, i.e. the opposite of retention rate\nThrust is your marketing effort at any given time\nSpeed in water is your active user base\n\nWhile marketing is necessary, your active userbase (or speed in the water) is a combination of your past marketing efforts and your retention rate.\nHere’s a visual comparison of efficient vs inefficient swimming:"
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-hidden-drivers",
    "href": "posts/saas-and-swimming/index.html#the-hidden-drivers",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Hidden Drivers",
    "text": "The Hidden Drivers\nHere’s something counterintuitive: in swimming, your most crucial muscles aren’t the ones you see moving. Your arms and legs create the splash, but it’s your core and hips that truly drive performance. They work silently beneath the surface, maintaining your streamlined form and minimizing drag keeping your body long and horizontal in the water.\nThe same principle applies in business. It’s easy to fixate on the visible elements - your marketing campaigns, landing pages, and ads. These are your business’s arms and legs, visibly pushing through the market. But your customer support, product reliability, and user experience? They’re your core and hips - less flashy, but absolutely critical.\nJust as a swimmer with weak core muscles will struggle regardless of arm strength, a business with poor user experience will flounder despite a massive marketing budget."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-paradox-of-effort",
    "href": "posts/saas-and-swimming/index.html#the-paradox-of-effort",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Paradox of Effort",
    "text": "The Paradox of Effort\nEven in swimming, there are situations where you need to decide between increasing thrust and reducing drag. A prime example is the use of legs. While leg movement contributes about 15% to overall thrust, it dramatically increases drag and often worsens your form. Counterintuitively, the advice for swimming well, especially when starting out, is to use your legs less – sometimes barely moving them at all.\nThis paradox has a direct parallel in business. Many companies engage in practices that seem to increase “thrust” but actually create more “drag”:\n\nAggressive upselling and cross-selling\nIntrusive advertising\nUser interface “dark patterns” that trick users into spending more\n\nYes, these tactics might bring in additional revenue (power) in the short term. But in the long run, they increase your drag by driving users away from your product. It’s a classic case of short-term gain leading to long-term pain. For a deeper dive into this concept and how to approach it, I recommend reading the insightful article on user disengagement by Zerodha."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#reducing-business-drag",
    "href": "posts/saas-and-swimming/index.html#reducing-business-drag",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "Reducing Business Drag",
    "text": "Reducing Business Drag\nIn business, reducing drag means focusing on retention. I’ve found this means:\n\nUnderstand why people leave. Talk to unhappy customers.\nIdentify unmet needs and fix what’s broken.\nDo the unglamorous work that’s important to customers.\nTreat customers as you’d want to be treated, be respectful of their time and attention.\n\nThese actions might not feel as impactful as big marketing pushes, or as cool as working on bleeding-edge technology, but they’re crucial for long-term success. They’re the equivalent of perfecting your swimming form - less visible, more effective."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#optimizing-for-success",
    "href": "posts/saas-and-swimming/index.html#optimizing-for-success",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "Optimizing for Success",
    "text": "Optimizing for Success\nAnother parallel: You can only optimize one aspect at a time. There’s a limit to what your brain can focus on. In swimming, you might want to improve your arm stroke, leg kick, hip rotation, and breathing simultaneously. In business, you’re juggling product improvements, customer support, marketing, and operations.\nTrying to fix everything at once leads to slow progress and ingrained bad habits. Instead:\n\nChoose one specific area for improvement\nPractice with perfect form and intense focus\nRepeat until the action becomes automatic\n\nThis applies to both swimming and running a company, but with a key difference. In swimming, “automatic” means the movement becomes part of your neural structure, performed subconsciously. In business, it’s about setting up the right culture, people, processes, and incentives so things run smoothly without constant oversight.\nThis approach might seem slow, but it leads to sustainable, compounding improvements. You’re not just doing things right when actively focusing - you’re making high standards your default state.\nYou’ll notice Olympic swimmers cross the pool in minimal strokes - that’s mastery in action. In business, it might look like efficient processes, strong culture, the right people, and well-set incentives."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-long-game",
    "href": "posts/saas-and-swimming/index.html#the-long-game",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Long Game",
    "text": "The Long Game\nIn swimming and in business, what looks like effort isn’t always what moves you forward.\nSometimes, the key to speed is doing less. Moving your legs less in the water. Pushing your customers less.\nFocus on reducing drag. In SaaS, that means understanding your users. Solving their problems. Respecting their time.\nIt’s not as visible as marketing. But it’s what separates great products from the rest."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#conclusions",
    "href": "posts/saas-and-swimming/index.html#conclusions",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "Conclusions",
    "text": "Conclusions\nReflecting on these parallels between swimming and business, I’m reminded of Miyamoto Musashi’s quote, which could be a fitting end to this post.\n\n“If you know the way broadly you will see it in everything.”\n\nBut I’m also reminded of Abraham Maslow.\n\n“If the only tool you have is a hammer, you tend to see every problem as a nail.”\n\nBecause while there’s wisdom in drawing parallels, there’s also wisdom in knowing when to put down the metaphorical hammer.\nHere are three important ways where the analogy breaks down which you should keep in mind.\n\nEnvironmental Consistency: Water provides a consistent environment for swimmers. In contrast, the business landscape is constantly changing, more akin to swimming in a dynamic, unpredictable ocean. your drag (churn) could suddenly rapidly increase because of competitors, regulation, effectiveness of your channels etc.\nFeedback Loops: In swimming, improvements in technique have a relatively linear effect on performance. In business, positive feedback loops (like network effects) can lead to exponential growth, a phenomenon not typically seen in swimming.\nNature of the End Goal: In swimming, the objective is typically to minimize time over a fixed distance. This is a well-defined, closed-ended goal. In business, particularly for SaaS, the goal is often open-ended growth. There’s no fixed “finish line” - the objective is continuous expansion and improvement, potentially without limit.\n\nSo take this post with a pinch of chlorine, maybe it’ll help you cut through the drag in your business (or your breaststroke), or maybe it’ll just give you a chuckle the next time you’re at the pool.\nEither way, happy swimming - and happy entrepreneur-ing!\nPS: If you’re learning to swim, I highly recommend “Total Immersion: The Revolutionary Way to Swim Better, Faster, and Easier”."
  },
  {
    "objectID": "posts/learning-machine-learning/index.html",
    "href": "posts/learning-machine-learning/index.html",
    "title": "Learning Machine Learning",
    "section": "",
    "text": "This post will talk about resources for how I’m going about learning machine learning.\n\nBooks\n\n“Hands-On Machine Learning with Scikit-Learn and TensorFlow” by Aurélien Géron\n“Deep Learning” by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\n“The Elements of Statistical Learning” by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\n\n\n\nBlogs\n\nDistill.pub: Clear, interactive explanations of machine learning concepts\nSebastian Ruder’s blog: In-depth articles on NLP and deep learning\nAndrej Karpathy’s blog: Excellent posts on deep learning\n\n\n\nPodcasts\nThese podcasts are amazing,and what got me interested in the first place. Get a podcast app, I love podcast addict (android). Some awesome podcasts:\n\nPartially Derivative\nLinear Digressions\nData Skeptic\n\nSome that are supposed to be good but never tried:\n\nNot so standard deviations\nData science at home\nTalking machines\n\n\n\nOnline Courses\nA few awesome courses.\n\nAndrew Ng Coursera\nA good first course, which teaches you bottom up, from basics to advanced techniques. Matlab/Octave.\nFast.ai\nA course which aims to teach by coding, and takes a top down approach.\nCS231n: Convolutional Neural Networks for Visual Recognition While focused on computer vision, this Stanford course provides an excellent introduction to deep learning concepts.\n\n\n\nStaying Up-to-Date\nOne of the best things about the machine learning field is how much work happens in the open.\nMany researchers publish their work on arXiv months or even years before it appears in journals or at conferences. Following key researchers and institutions on Twitter is an excellent way to stay informed about the latest developments as they happen.\nSome great accounts to follow: @goodfellow_ian, @ylecun, @karpathy, @gwern.\nDon’t be afraid of reading arXiv papers, they might seem intimidating in the beginning but they get easier over time.\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html",
    "href": "posts/exerimentations-platforms/index.html",
    "title": "Experimentation Platforms",
    "section": "",
    "text": "Trustworthy Online Controlled Experiments"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#books",
    "href": "posts/exerimentations-platforms/index.html#books",
    "title": "Experimentation Platforms",
    "section": "",
    "text": "Trustworthy Online Controlled Experiments"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#resources",
    "href": "posts/exerimentations-platforms/index.html#resources",
    "title": "Experimentation Platforms",
    "section": "Resources",
    "text": "Resources\n\nTop Challenges from the first Practical Online Controlled Experiments Summit\nA/B Testing Pitfalls: Getting Numbers You Can Trust is Hard\nUSF Business Analytics Forum - Ron Kohavi\nA/B Testing at Scale: Accelerating Software Innovation\nTrustworthy Online Controlled Experiments at Large Scale\nAlways Valid Inference: Continuous Monitoring of A/B Tests"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#companies",
    "href": "posts/exerimentations-platforms/index.html#companies",
    "title": "Experimentation Platforms",
    "section": "Companies",
    "text": "Companies\n\nNetflix\n\nNetflix Articles tagged Experimentation\nIt’s All A/Bout Testing: The Netflix Experimentation Platform\nReimagining Experimentation Analysis at Netflix\nSuccess stories from a democratized experimentation platform\nKey Challenges with Quasi Experiments at Netflix\nData Compression for Large-Scale Streaming Experimentation\nPage Simulation for Better Offline Metrics at Netflix\nStreaming Video Experimentation at Netflix: Visualizing Practical and Statistical Significance\nInnovating Faster on Personalization Algorithms at Netflix Using Interleaving\n\n\n\nMicrosoft\n\nExP Experimentation Platform Accelerating software innovation through trustworthy experimentation\nOnline Experimentation at Microsoft\nExperimentation Platform\nA/B Testing and Covid-19: Data-Driven Decisions in Times of Uncertainty\nPatterns of Trustworthy Experimentation: Pre-Experiment Stage\n\n\n\nTwitter\n\nTwitter experimentation: technical overview\n\n\n\nGoogle\n\nOverlapping Experiment Infrastructure: More, Better, Faster Experimentation\n\n\n\nFacebook\n\nPlanOut is a library and interpreter for designing online experiments.\nAdaptive Experimentation Platform\n\n\n\nSpotify\n\nSpotify’s New Experimentation Platform part 1\nSpotify’s New Experimentation Platform part 2\nLarge Scale Experimentation at Spotify\n\n\n\nTinder\n\nPhoenix — Tinder’s Testing Platform, Part — I\nPhoenix — Tinder’s Testing Platform — Part II\nPhoenix — Tinder’s Testing Platform — Part III\n\n\n\nLinkedIn\n\nOur evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn\nMaking the LinkedIn experimentation engine 20x faster\n\n\n\nUber\n\nUnder the Hood of Uber’s Experimentation Platform\nA/B testing at Uber: How we built a BYOM (bring your own metrics) platform\nBuilding an Intelligent Experimentation Platform with Uber Engineering\n\n\n\nAirBnB\n\n4 Principles for Making Experimentation Count\nScaling Airbnb’s Experimentation Platform\nExperiment Reporting Framework\n\n\n\nInstagram\n\nLessons Learned at Instagram Stories and Feed Machine Learning\n\n\n\nGo-Jek\n\nIntroducing Litmus: GOJEK’s Own Experimentation Platform\n\n\n\nInstaCart\n\nRandomized, controlled experiments and multivariate regression are used to continuously improve the grocery delivery engine\n\n\n\nPintrest\n\nBuilding Pinterest’s A/B testing platform"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#conferences",
    "href": "posts/exerimentations-platforms/index.html#conferences",
    "title": "Experimentation Platforms",
    "section": "Conferences",
    "text": "Conferences\n\nExperimentation Culture Awards"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#sass-solutions",
    "href": "posts/exerimentations-platforms/index.html#sass-solutions",
    "title": "Experimentation Platforms",
    "section": "SASS solutions",
    "text": "SASS solutions\n\nSplit.io\nOptimizely"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#when-you-cant-run-ab-tests",
    "href": "posts/exerimentations-platforms/index.html#when-you-cant-run-ab-tests",
    "title": "Experimentation Platforms",
    "section": "When you can’t run A/B Tests",
    "text": "When you can’t run A/B Tests\n\nQuasi Experimentation at Netflix\nKey Challenges with Quasi Experiments at Netflix\nMostly Harmless Econometrics: An Empiricist’s Companion"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#notes",
    "href": "posts/exerimentations-platforms/index.html#notes",
    "title": "Experimentation Platforms",
    "section": "Notes",
    "text": "Notes\n\n2020 CODE@MIT Experimentation platforms\n\nGaussian processes\nMulti touch attributions\nHeterogenous treatment effect\nInteraction effects\nOverlapping experiments\nWhat are potential over evaluation criteria ?\nWhat are good guardrail metrics ?\nrun A/A tests.\n\nCanary Deploys by using Experimentation platform to tell when you break guard rail metrics"
  },
  {
    "objectID": "posts/course-schedule-generator/index.html",
    "href": "posts/course-schedule-generator/index.html",
    "title": "Course Schedule Generator",
    "section": "",
    "text": "A website which allows students of IIT Indore to add courses they are interested in to their calendar.\nI was tired of doing this manually every time.\nUse it here\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nOn SaaS and Swimming: Hidden Factors of Success\n\n\n\n\n\n\nBusiness\n\n\n\nHow to figure out what matters in business and sports\n\n\n\n\n\nAug 12, 2023\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentation Platforms\n\n\n\n\n\n\nExperimentation\n\n\n\nAn Overview of Experimentation Platforms.\n\n\n\n\n\nMar 21, 2021\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Schedule Generator\n\n\n\n\n\n\nIIT Indore\n\n\n\nA website which adds courses to your personal calendar, for students of IIT Indore.\n\n\n\n\n\nJan 8, 2020\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nHarry Potter quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA beginner friendly quiz on the wizarding world\n\n\n\n\n\nAug 26, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nThe state of reading in 2018 and beyond.\n\n\n\n\n\n\nFuture\n\n\n\n\n\n\n\n\n\nAug 5, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nFriend Or Foe\n\n\n\n\n\n\nAI\n\n\nProjects\n\n\n\nAn interactive game inspired by the paper “AI Safety Grid Worlds”\n\n\n\n\n\nJul 28, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nInfant Mortality In India\n\n\n\n\n\n\nAnalysis\n\n\n\nAn analysis of infant mortality rates across India.\n\n\n\n\n\nJun 15, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nJargonizer\n\n\n\n\n\n\nProjects\n\n\n\nGenerate sentences that nobody can read.\n\n\n\n\n\nJun 12, 2018\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Machine Learning\n\n\n\n\n\n\nAI\n\n\n\n\n\n\n\n\n\nNov 12, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nGame Of Thrones quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz made for cultural week\n\n\n\n\n\nSep 4, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nNewbie quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz to introduce quizzing to new people and narrowing down potential recruits.\n\n\n\n\n\nAug 20, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\n\n\n\n\n\n\nMental Health Checklists\n\n\n\n\n\n\nProjects\n\n\n\nA website which allows quick tracking of mental health.\n\n\n\n\n\nJan 8, 2017\n\n\nArjun Srivastava\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "podcast/podcasts/2209.11895/index.html",
    "href": "podcast/podcasts/2209.11895/index.html",
    "title": "In-context Learning and Induction Heads",
    "section": "",
    "text": "The emergence of induction heads in transformer models is strongly correlated with a significant improvement in in-context learning abilities. Directly manipulating the formation of induction heads in models led to changes in their in-context learning performance, highlighting the crucial role of these mechanisms in adapting to new tasks without explicit retraining.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2406.11066/index.html",
    "href": "podcast/podcasts/2406.11066/index.html",
    "title": "Metadata-based Color Harmonization for Multi-camera Surround View Systems",
    "section": "",
    "text": "The paper introduces a metadata-based approach to address color inconsistencies in multi-camera surround view systems, crucial for accurate perception in autonomous driving. The method significantly outperforms traditional techniques in visual quality and runtime, making it more efficient and robust for real-time applications.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2310.08566/index.html",
    "href": "podcast/podcasts/2310.08566/index.html",
    "title": "Supervised Pretraining for In-Context Reinforcement Learning with Transformers",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper are: Supervised pretraining with transformers can efficiently approximate prevalent RL algorithms, transformers demonstrate the potential for near-optimal regret bounds, and the research highlights the importance of model capacity and distribution divergence in in-context reinforcement learning.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2212.09095/index.html",
    "href": "podcast/podcasts/2212.09095/index.html",
    "title": "Rethinking Scale for In-Context Learning in Large Language Models",
    "section": "",
    "text": "Engineers and specialists can consider the findings of this research to explore the efficiency of large language models. By identifying key components like ‘induction heads’ critical for in-context learning, there is potential to optimize model design for better performance. The study indicates that a focus on enhancing these crucial components could lead to more resource-friendly and effective language models.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2408.00714/index.html",
    "href": "podcast/podcasts/2408.00714/index.html",
    "title": "SAM 2: Segment Anything in Images and Videos",
    "section": "",
    "text": "SAM 2 outperformed previous approaches in video segmentation by achieving higher accuracy with fewer user interactions, making it faster and more accurate. The model shows promise in tasks like interactive video object segmentation and long-term video object segmentation, demonstrating its efficiency and ability to handle diverse objects and scenarios.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2310.10616/index.html",
    "href": "podcast/podcasts/2310.10616/index.html",
    "title": "How Transformers Learn In-Context Beyond Simple Functions",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper include the development of theoretical constructions for transformers to implement in-context ridge regression on representations efficiently. This research showcases the modularity of transformers in decomposing complex tasks into distinct learnable modules, providing strong evidence for their adaptability in handling complex learning scenarios.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2301.00774/index.html",
    "href": "podcast/podcasts/2301.00774/index.html",
    "title": "SparseGPT: One-shot Pruning of Large Language Models",
    "section": "",
    "text": "SparseGPT offers a one-shot pruning approach that avoids costly retraining, making it significantly more efficient for compressing large language models like GPT variants. The method can achieve high sparsity levels while maintaining minimal accuracy loss, providing a promising solution for improving the deployment of powerful language models.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2407.01781/index.html",
    "href": "podcast/podcasts/2407.01781/index.html",
    "title": "𝑓VDB: A Deep-Learning Framework for Sparse, Large-Scale, and High-Performance Spatial Intelligence",
    "section": "",
    "text": "Engineers and specialists can benefit from 𝑓VDB by leveraging its memory-efficient IndexGrid structure and specialized convolution kernels optimized for different sparsity patterns. The framework provides significant speed and memory efficiency improvements over existing frameworks, enabling more effective handling of large-scale, sparse 3D datasets in deep learning applications.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2212.09720/index.html",
    "href": "podcast/podcasts/2212.09720/index.html",
    "title": "Optimizing Quantization of Large Language Models for Efficiency and Accuracy",
    "section": "",
    "text": "Engineers and specialists can leverage 4-bit precision quantization with techniques such as quantile quantization and floating-point representation to significantly reduce the memory footprint and improve inference speed of large language models. Understanding the trade-off between accuracy and efficiency is crucial for deploying powerful NLP technologies in resource-constrained environments and expanding their applications to real-world scenarios.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2403.13187/index.html",
    "href": "podcast/podcasts/2403.13187/index.html",
    "title": "Evolutionary Optimization of Model Merging Recipes",
    "section": "",
    "text": "Engineers and specialists can leverage the Evolutionary Model Merge method to automate the process of combining pre-trained models, eliminating the need for human intuition and expanding the search space for potential model combinations. This approach opens up possibilities for developing more efficient, cost-effective, and powerful AI systems with emergent capabilities.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2208.01066/index.html",
    "href": "podcast/podcasts/2208.01066/index.html",
    "title": "In-Context Learning Capabilities of Transformers",
    "section": "",
    "text": "The key takeaways for engineers/specialists are that Transformers demonstrate robust in-context learning capabilities for various function classes, showing flexibility and adaptability without the need for fine-tuning. The study emphasizes the importance of model capacity and the potential benefits of curriculum learning for training efficiency.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2304.02643/index.html",
    "href": "podcast/podcasts/2304.02643/index.html",
    "title": "Segment Anything: A Paradigm Shift in Image Segmentation",
    "section": "",
    "text": "The key takeaways for engineers/specialists include the innovative concept of promptable segmentation, the development of SAM with components like Image Encoder, Prompt Encoder, and Mask Decoder, and the significant results showcasing SAM’s impressive zero-shot transfer capabilities in various image segmentation tasks. It highlights the potential impact of SAM on generalizing to new tasks and datasets efficiently while providing insights into addressing limitations through future research areas.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2304.08069/index.html",
    "href": "podcast/podcasts/2304.08069/index.html",
    "title": "RT-DETR: Real-Time Object Detection with Transformer",
    "section": "",
    "text": "RT-DETR is a groundbreaking end-to-end real-time object detector based on Transformers that combines the speed of YOLO with the accuracy of DETR. Key takeaways for engineers include the efficient hybrid encoder approach, which improves multi-scale feature interactions, and the uncertainty-minimal query selection scheme, enhancing accuracy in both classification and localization. Despite outperforming traditional CNN-based methods, RT-DETR faces challenges in detecting small objects, prompting future research directions like knowledge distillation.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2402.07945/index.html",
    "href": "podcast/podcasts/2402.07945/index.html",
    "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
    "section": "",
    "text": "The key takeaways for engineers/specialists are: 1. ScreenAgent enables VLMs to control real computer screens by generating plans and translating them into low-level commands. 2. ScreenAgent outperforms other models in precise UI positioning, showing promise for more accurate interaction with computer interfaces. 3. Future research directions include enhancing visual localization capabilities, improving planning mechanisms, and expanding capabilities to handle videos and multi-frame images.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1712.01208/index.html",
    "href": "podcast/podcasts/1712.01208/index.html",
    "title": "The Case for Learned Index Structures",
    "section": "",
    "text": "Learned indexes offer significant performance gains and memory savings compared to traditional structures across various datasets. The Recursive Model Index (RMI) architecture helps improve prediction accuracy, and the potential for hybrid indexing combining neural networks and traditional techniques showcases a promising future for enhancing database systems’ efficiency and scalability.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2406.08691/index.html",
    "href": "podcast/podcasts/2406.08691/index.html",
    "title": "Unsupervised Occupancy Fields for Perception and Forecasting",
    "section": "",
    "text": "The paper ‘UnO: Unsupervised Occupancy Fields for Perception and Forecasting’ introduces a novel approach to perception and forecasting in self-driving vehicles using unsupervised learning from raw LiDAR data. By leveraging occupancy fields and deformable attention mechanisms, the UnO model outperformed existing methods on point cloud forecasting and semantic occupancy tasks, showing promise for enhancing the robustness and safety of autonomous systems especially in scenarios where labeled data is limited or rare events occur.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2406.02532/index.html",
    "href": "podcast/podcasts/2406.02532/index.html",
    "title": "Speculative Execution for Efficient Inference in Large Language Models on Consumer Devices",
    "section": "",
    "text": "SpecExec introduces a two-step parallel processing method using draft and target models to speed up inference on consumer devices. It achieved impressive interactive inference speeds, providing real-time responses for applications like chatbots. The approach addresses the limitations of existing speculative decoding methods and holds promise for democratizing access to powerful language models.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2403.03507/index.html",
    "href": "podcast/podcasts/2403.03507/index.html",
    "title": "Gradient Low-Rank Projection (GaLore): Revolutionizing Memory-Efficient LLM Training",
    "section": "",
    "text": "GaLore offers a breakthrough in memory-efficient LLM training by reducing memory usage significantly while achieving performance comparable to full-rank training. It enables training of large models on limited hardware resources, democratizing LLM research and development. Future research directions include applying GaLore to various model architectures, enhancing memory efficiency further, and exploring elastic data distributed training using consumer-grade hardware.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2103.00020/index.html",
    "href": "podcast/podcasts/2103.00020/index.html",
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "section": "",
    "text": "Engineers and specialists can utilize CLIP’s contrastive learning approach to create more efficient and scalable computer vision systems. The paper highlights the importance of ethical considerations and bias mitigation strategies in developing AI technologies.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2205.14135/index.html",
    "href": "podcast/podcasts/2205.14135/index.html",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    "section": "",
    "text": "FlashAttention is a novel algorithm that addresses the efficiency of Transformer models by improving speed and memory efficiency through IO-awareness. It reduces the number of memory accesses by dividing data into smaller blocks and loading them into fast memory, achieving practical speedups and enabling training on longer sequences. The algorithm also incorporates recomputation during the backward pass to minimize memory usage, delivering significant improvements in training large models like BERT and GPT-2.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1805.08941/index.html",
    "href": "podcast/podcasts/1805.08941/index.html",
    "title": "AutoPruner: End-to-End Trainable Filter Pruning for Efficient Deep Neural Networks",
    "section": "",
    "text": "AutoPruner presents a significant advancement in filter pruning for deep neural networks by integrating the filter selection process into model training, eliminating the need for separate pruning steps. The methodology outperformed state-of-the-art methods, showcasing superior accuracy and compression ratios on standard datasets like CUB200-2011 and ImageNet ILSVRC-12. The innovative approach of AutoPruner could lead to more efficient and accessible deep learning models across various applications.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2306.14892/index.html",
    "href": "podcast/podcasts/2306.14892/index.html",
    "title": "Decision-Pretrained Transformer: Bridging Supervised Learning and Reinforcement Learning",
    "section": "",
    "text": "Engineers and specialists can leverage the DPT methodology to design more versatile and efficient RL agents. By learning a decision-making strategy through supervised pretraining, DPT demonstrates adaptability to new environments, ability to explore and exploit, and strong generalization capabilities. This approach offers a promising path towards practical and efficient Bayesian RL methods.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2401.14159/index.html",
    "href": "podcast/podcasts/2401.14159/index.html",
    "title": "Grounded SAM: A Novel Approach to Open-Set Segmentation",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper are: 1. Grounded SAM combines the strengths of Grounding DINO for object detection and SAM for zero-shot segmentation, outperforming existing models. 2. The model’s potential extends beyond segmentation, enabling integration with other models for tasks like image annotation, image editing, and human motion analysis.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2407.10956/index.html",
    "href": "podcast/podcasts/2407.10956/index.html",
    "title": "Spider2-V: Automated Multimodal Agents for Data Science Workflows",
    "section": "",
    "text": "The paper highlights that even advanced VLMs struggle to automate full data workflows, especially in GUI-intensive tasks, with a low success rate of 14%. The study emphasizes the need for improvements in action grounding and training data quality to enhance the performance of AI agents in complex data tasks.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2210.03044/index.html",
    "href": "podcast/podcasts/2210.03044/index.html",
    "title": "Unmasking the Lottery Ticket Hypothesis",
    "section": "",
    "text": "The key takeaways for engineers/specialists include understanding the role of the pruning mask in guiding training, the importance of SGD robustness in navigating the error landscape, and the relationship between the Hessian eigenspectrum and the maximum pruning ratio for efficient network pruning.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1806.09055/index.html",
    "href": "podcast/podcasts/1806.09055/index.html",
    "title": "DARTS: Differentiable Architecture Search",
    "section": "",
    "text": "Key takeaways for engineers/specialists: DARTS introduces a continuous relaxation approach to architecture search, leveraging gradient descent for efficient optimization. It achieves state-of-the-art results on image classification and language modeling tasks with significantly less computational cost. Challenges include the gap between continuous and discrete architecture representation, computational cost of second-order approximation, and sensitivity to hyperparameters.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2212.08073/index.html",
    "href": "podcast/podcasts/2212.08073/index.html",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "",
    "text": "Engineers and specialists can benefit from this research by understanding the innovative approach of using constitutional principles to guide AI behavior and self-correct harmful outputs. The study shows that CAI models outperformed traditional methods in terms of harmlessness while maintaining comparable levels of helpfulness, indicating a promising direction for developing more ethical and trustworthy AI systems.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2310.08370/index.html",
    "href": "podcast/podcasts/2310.08370/index.html",
    "title": "UniPAD: A Universal Pre-training Paradigm for Autonomous Driving",
    "section": "",
    "text": "UniPAD is a novel self-supervised learning framework designed for autonomous driving, focusing on learning effective representations from 3D data such as LiDAR point clouds and multi-view images. The framework consists of a modality-specific encoder, a mask generator for challenging training, a unified 3D volumetric representation, and a neural rendering decoder. UniPAD showed promising results in improving performance on tasks like 3D object detection and semantic segmentation, outperforming other pre-training methods and offering potential for broader applications beyond autonomous driving.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2005.14165/index.html",
    "href": "podcast/podcasts/2005.14165/index.html",
    "title": "Language Models are Few-Shot Learners",
    "section": "",
    "text": "Key takeaways include the model’s ability to generalize from a few examples (few-shot learning), the comprehensive evaluation of GPT-3’s performance across various NLP tasks, and the importance of responsible research and development to address ethical challenges and risks associated with advanced language models.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1904.00420/index.html",
    "href": "podcast/podcasts/1904.00420/index.html",
    "title": "Single Path One-Shot (SPOS): Efficient Neural Architecture Search with Simplified Supernet",
    "section": "",
    "text": "SPOS addresses limitations of existing NAS methods by simplifying the supernet structure, utilizing an evolutionary algorithm, and incorporating channel search and mixed-precision quantization. The approach outperforms previous methods in accuracy, complexity, and resource efficiency. It demonstrates strong correlation between supernet and individual architecture performance, enhancing the search process efficiency.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2006.06373/index.html",
    "href": "podcast/podcasts/2006.06373/index.html",
    "title": "The limits to learning a diffusion model",
    "section": "",
    "text": "Don’t be confused by the title, diffusion here is not referring to diffusion as we use it today in context of image generation process, but more about modelling diffusive processes (like virus spread)\n  This paper answers the question about ‘how much data do we need, before we can figure out the final affected value’ turns out this is a lot more thant people expect.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2109.10665/index.html",
    "href": "podcast/podcasts/2109.10665/index.html",
    "title": "Survey on reinforcement learning in reccomender systems",
    "section": "",
    "text": "Goes over some of the different places RL can be used in RecSys.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2212.10156/index.html",
    "href": "podcast/podcasts/2212.10156/index.html",
    "title": "Planning-Oriented Autonomous Driving",
    "section": "",
    "text": "The paper introduces UniAD, a planning-oriented framework for autonomous driving that focuses on integrating perception, prediction, and planning tasks to optimize for safe and efficient driving. UniAD outperforms existing state-of-the-art methods in motion forecasting, occupancy prediction, and planning, showcasing the benefits of joint optimization and query-based communication between modules. Key challenges for future research include addressing computational complexity, handling long-tail scenarios, and exploring additional tasks like depth estimation and behavior prediction.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1810.05270/index.html",
    "href": "podcast/podcasts/1810.05270/index.html",
    "title": "Rethinking the Value of Network Pruning",
    "section": "",
    "text": "Key takeaways for engineers and specialists include the importance of shifting focus from weight selection to architecture search in network pruning. Training pruned models from scratch can often yield comparable or better results than fine-tuning, particularly for structured pruning methods. Automatic pruning methods offer an efficient way to identify more parameter-efficient network structures, potentially leading to the development of more scalable and powerful deep learning models.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2403.15378/index.html",
    "href": "podcast/podcasts/2403.15378/index.html",
    "title": "Long-CLIP: Extending Text Length for Improved Vision-Language Modeling",
    "section": "",
    "text": "Long-CLIP significantly extends the text length without disrupting existing representations, improving recall rates on long and short caption retrieval tasks. Its plug-and-play nature enables integration into various downstream applications, showing promise in enhancing image generation models and opening up possibilities for realistic and detailed content creation.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2406.17345/index.html",
    "href": "podcast/podcasts/2406.17345/index.html",
    "title": "NerfBaselines: A Framework for Standardized Evaluation of Novel View Synthesis Methods in Computer Vision",
    "section": "",
    "text": "NerfBaselines addresses the inconsistent evaluation protocols in comparing novel view synthesis methods by providing a unified interface, ensuring reproducibility through containerization, and standardizing the evaluation protocol. By enabling the sharing of pre-trained checkpoints, it reduces computational costs and environmental impact. However, it relies on methods exposing the same interface and future directions involve exploring advanced evaluation metrics and addressing the computational cost of training.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "TIL: Today I Learned",
    "section": "",
    "text": "A collection of links, notes, snippets on things I am reading, watching.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I believe we are still in the early stages of algorithmic progress in deep learning systems. We have been able to make useful models by scaling up, we are yet to discover far more compute and data-efficient approaches. We are in the early phase of “Simplicity does not precede complexity, but follows it” cycle, and we will soon see a wave of simpler more cost-effective and performant models.\nMy current interests lie in the inference and training of large language models on smaller machines, mechanisitic-interpretability, and in-context learning.\nI’m also fascinated by marketplaces and mechanism design, influenced by books like “Dataclysm” by Christian Rudder and “Who Gets What–And Why” by Alvin E. Roth I am interested in how we can leverage ML to create better marketplaces (reputation systems, matching algorithms, spam detection, pricing algorithms)."
  },
  {
    "objectID": "about.html#woven-by-toyota",
    "href": "about.html#woven-by-toyota",
    "title": "About",
    "section": "Woven By Toyota",
    "text": "Woven By Toyota\nAt Woven, I lead multiple projects in machine learning and infrastructure for things like adaptive assessments, and course recommendation, talent matching systems, as one of the founding engineers of MS1, a startup focused on talent mobility."
  },
  {
    "objectID": "about.html#bookmyshow",
    "href": "about.html#bookmyshow",
    "title": "About",
    "section": "BookMyShow",
    "text": "BookMyShow\nPreviously, at BookMyShow, I worked on the development of real-time data pipelines for discovery and personalization systems, helping millions of users find entertainment experiences."
  },
  {
    "objectID": "podcast/podcasts/2103.01775/index.html",
    "href": "podcast/podcasts/2103.01775/index.html",
    "title": "No-Transaction Band Network A Neural Network Architecture for Efficient Deep Hedging",
    "section": "",
    "text": "The paper introduces a deep hedging approach using neural networks to optimize hedging strategies for derivatives in imperfect markets. The key takeaway is the development of the ‘no-transaction band network’ to address action dependence and improve efficiency in hedging, showcasing superior performance compared to traditional methods in terms of expected utility and price efficiency, and faster training. Future research focuses on addressing limitations such as non-linear transaction costs and discontinuous payoffs, as well as challenges in data availability and model explainability for real-world applications.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1706.03741/index.html",
    "href": "podcast/podcasts/1706.03741/index.html",
    "title": "Training Deep Reinforcement Learning Systems with Human Preferences",
    "section": "",
    "text": "The paper introduces a method that significantly reduces the need for human oversight in training deep RL agents, allowing them to learn complex behaviors with minimal human input. This approach has shown promising results in both simulated robotics and Atari games, achieving human-level performance with a fraction of the human effort required by traditional RL methods.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2303.04129/index.html",
    "href": "podcast/podcasts/2303.04129/index.html",
    "title": "Foundation Models in Decision Making: Roles, Challenges, and Opportunities",
    "section": "",
    "text": "The paper proposes a framework for understanding the various roles of foundation models in decision making, including conditional generative models, representation learners, and interactive agents. Key takeaways include the use of foundation models for behavioral priors, world modeling, and generalization of knowledge across tasks and environments.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2310.13810/index.html",
    "href": "podcast/podcasts/2310.13810/index.html",
    "title": "A Better Match for Drivers and Riders Reinforcement Learning at Lyft",
    "section": "",
    "text": "The paper demonstrates the successful application of reinforcement learning to improve the efficiency of driver-rider matching in ride-sharing platforms. The use of online RL allows for real-time adaptation, resulting in decreased wait times for riders, increased earnings for drivers, and overall higher user satisfaction. The research paves the way for more intelligent systems in the ride-sharing industry, with potential for further optimization and expansion into various other aspects of the ecosystem.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1606.06565/index.html",
    "href": "podcast/podcasts/1606.06565/index.html",
    "title": "Practical Research Problems in AI Safety",
    "section": "",
    "text": "The key takeaways for engineers/specialists are: the need for focused research on practical AI safety problems, the importance of developing robust and scalable oversight mechanisms, safe exploration strategies, and systems that are robust to changes in data distribution. The paper provides a valuable framework for addressing these crucial concerns.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1905.12784/index.html",
    "href": "podcast/podcasts/1905.12784/index.html",
    "title": "Geometric Properties of Data Representations in Deep Neural Networks",
    "section": "",
    "text": "Key takeaways for engineers/specialists include the discovery of a ‘hunchback’ shape for intrinsic dimensionality across layers of Convolutional Neural Networks (CNNs), with a strong correlation between the ID in the final layer and performance on unseen data. The findings indicate that deep networks compress information into low-dimensional manifolds to generalize effectively, involving non-linear transformations for achieving linearly separable representations.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2111.15397/index.html",
    "href": "podcast/podcasts/2111.15397/index.html",
    "title": "NeuralProphet Explainable Forecasting at Scale",
    "section": "",
    "text": "‘Successor’ of Prophet (by facebook) for time series modelling.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2211.02131/index.html",
    "href": "podcast/podcasts/2211.02131/index.html",
    "title": "SafePathNet: Learning a Distribution of Trajectories for Safe and Comfortable Autonomous Driving",
    "section": "",
    "text": "SafePathNet introduces a novel approach that models the distribution of future trajectories for both the self-driving vehicle and other road agents using a unified neural network architecture. By incorporating a ‘Mixture of Experts’ framework, the model can learn diverse driving strategies and prioritize safety in real-time decision-making. The use of Transformer networks and imitation learning further enhances the model’s ability to handle complex and unpredictable driving scenarios.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2407.02524/index.html",
    "href": "podcast/podcasts/2407.02524/index.html",
    "title": "Training Large Language Models for Compiler Optimization",
    "section": "",
    "text": "The research paper discusses the development of LLM Compiler, a model specifically trained on compiler IRs and assembly code for optimizing code efficiently. This approach outperforms traditional techniques and existing LLMs in tasks like flag tuning and disassembly, showing potential for automating and improving the optimization process in software engineering.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1906.04358/index.html",
    "href": "podcast/podcasts/1906.04358/index.html",
    "title": "Exploring Weight Agnostic Neural Networks",
    "section": "",
    "text": "The research presents a paradigm shift towards designing networks with inherent capabilities, emphasizing architecture over weight optimization. WANNs demonstrate high performance on various tasks with random weights, suggesting potential for efficient learning and broader generalization in deep learning applications.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2306.00248v1/index.html",
    "href": "podcast/podcasts/2306.00248v1/index.html",
    "title": "TransAct Transformer-based Realtime User Action Model for Recommendation at Pinterest",
    "section": "",
    "text": "Pinterest home feed reccomendation system. Needs to react to both long term interests + short term (even single session only) interests.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2002.11252/index.html",
    "href": "podcast/podcasts/2002.11252/index.html",
    "title": "AutoEmb Automated Embedding Dimensionality Searchg in Streaming Recommendations",
    "section": "",
    "text": "AutoEmb is about using different lenghts of embedding vectors for different items, use less memory + potentially learn more robust stuff for items with less data, and learn more nuanced stuff for popular items.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1312.5602/index.html",
    "href": "podcast/podcasts/1312.5602/index.html",
    "title": "Playing Atari with Deep Reinforcement Learning",
    "section": "",
    "text": "The key takeaways for engineers/specialists from this paper are: 1. Deep Q-learning (DQN) with a convolutional neural network can successfully learn to control agents directly from high-dimensional sensory input 2. The combination of deep learning with reinforcement learning showcased human-level performance on Atari games, surpassing traditional methods and even expert human players. 3. The paper laid the foundation for developing more general, adaptable AI systems that can learn and adapt to various complex tasks.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2210.05675/index.html",
    "href": "podcast/podcasts/2210.05675/index.html",
    "title": "Generalization Patterns of Transformers in In-Weights Learning and In-Context Learning",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper are: 1. In-context learning in large language models tends to be rule-based, suggesting the influence of language structure. 2. Model size and training data structure play crucial roles in shaping the inductive biases of transformers. 3. Pretraining strategies can be used to induce rule-based generalization from context.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2406.12214/index.html",
    "href": "podcast/podcasts/2406.12214/index.html",
    "title": "Robustness Evaluation of HD Map Constructors under Sensor Corruptions for Autonomous Driving",
    "section": "",
    "text": "The paper focuses on evaluating the robustness of HD map constructors under various sensor corruptions using a comprehensive benchmark called MapBench. It highlights the vulnerability of existing methods to real-world challenges and suggests the importance of advanced data augmentation techniques and new network architectures to enhance robustness for autonomous driving applications.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1810.00826/index.html",
    "href": "podcast/podcasts/1810.00826/index.html",
    "title": "Graph Isomorphism Networks: A Theoretical Framework and Architecture",
    "section": "",
    "text": "Engineers and specialists should take note of the importance of designing GNN architectures with highly expressive aggregation schemes like the injective multiset functions used in GIN. Understanding the theoretical underpinnings of GNNs and their limitations is crucial for developing more powerful and sophisticated models in the future.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1609.09106/index.html",
    "href": "podcast/podcasts/1609.09106/index.html",
    "title": "Hyper Networks: A Novel Approach to Learning Weights in Deep Neural Networks",
    "section": "",
    "text": "The key takeaways for engineers/specialists are: Hyper Networks introduce a meta-network (hypernetwork) that learns to generate weight structures for deep neural networks, providing flexibility and efficiency. Dynamic hypernetworks allow weights to adapt to input sequences, improving performance on sequential tasks. End-to-end training of hypernetworks with the main network leads to collaborative optimization and comparable or better performance with fewer parameters.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2406.07550/index.html",
    "href": "podcast/podcasts/2406.07550/index.html",
    "title": "TiTok: A Transformer-based 1D Tokenization Approach for Image Generation",
    "section": "",
    "text": "TiTok introduces a novel 1D tokenization method for image generation, enabling the representation of images with significantly fewer tokens while maintaining or surpassing the performance of existing 2D grid-based methods. The approach leverages a Vision Transformer architecture, two-stage training with proxy codes, and achieves remarkable speedup in training and inference. The research opens up new possibilities for efficient and high-quality image generation, with implications for various applications in computer vision and beyond.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1707.06347/index.html",
    "href": "podcast/podcasts/1707.06347/index.html",
    "title": "Proximal Policy Optimization Algorithms",
    "section": "",
    "text": "Engineers and specialists can benefit from PPO’s balancing act between simplicity and effectiveness, enabling more stable and efficient training with less data. Additionally, the clipping mechanism allows for smoother updates and multiple minibatch updates, enhancing the algorithm’s sample complexity and performance compared to traditional policy gradient methods.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2003.08934/index.html",
    "href": "podcast/podcasts/2003.08934/index.html",
    "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
    "section": "",
    "text": "Key takeaways for engineers and specialists from the paper include the efficiency of using a continuous 5D representation instead of discrete meshes or voxel grids, the importance of differentiable volume rendering in training neural networks for scene representation, and the potential of NeRF to revolutionize how 3D content is created and experienced.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2006.11239/index.html",
    "href": "podcast/podcasts/2006.11239/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The paper leverages denoising score matching to simplify the training objective for diffusion models, leading to faster and more stable training processes and higher-quality image generation results. Additionally, the paper highlights the potential of diffusion models as efficient lossy compressors, opening up possibilities in data compression applications.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2402.12289/index.html",
    "href": "podcast/podcasts/2402.12289/index.html",
    "title": "DriveVLM: Vision-Language Models for Autonomous Driving in Urban Environments",
    "section": "",
    "text": "The paper introduces DriveVLM, a system that leverages Vision-Language Models for scene understanding in autonomous driving. It comprises modules for Scene Description, Scene Analysis, and Hierarchical Planning to handle complex driving scenarios. DriveVLM outperformed other models in handling uncommon objects and unexpected events, while DriveVLM-Dual achieved state-of-the-art performance in planning tasks, showing promise for future improvements in autonomous driving.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1911.01547/index.html",
    "href": "podcast/podcasts/1911.01547/index.html",
    "title": "On the Measure of Intelligence",
    "section": "",
    "text": "Key takeaways for engineers/specialists include the importance of skill-acquisition efficiency in measuring intelligence, the emphasis on building systems with adaptability and generalization capabilities, and the potential impact of such research on areas like education, healthcare, and robotics.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1611.02779/index.html",
    "href": "podcast/podcasts/1611.02779/index.html",
    "title": "RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning",
    "section": "",
    "text": "Engineers and specialists can benefit from RL2 by understanding how meta-learning can bridge the gap between slow deep reinforcement learning and fast human learning speeds. This approach offers a way to encode prior knowledge in an RNN to make RL algorithms more efficient, adaptable, and scalable to complex real-world scenarios.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2404.05719/index.html",
    "href": "podcast/podcasts/2404.05719/index.html",
    "title": "Ferret-UI: Multimodal Large Language Model for Mobile User Interface Understanding",
    "section": "",
    "text": "Ferret-UI is the first UI-centric MLLM capable of executing referring, grounding, and reasoning tasks, making it adept at identifying specific UI elements, understanding relationships, and deducing overall screen function. It breaks down screens into sub-images using the ‘any resolution’ approach, providing detailed understanding of UI elements and interactions.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2304.11277/index.html",
    "href": "podcast/podcasts/2304.11277/index.html",
    "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel",
    "section": "",
    "text": "FSDP addresses memory capacity challenges by sharding parameters across devices, employs communication optimizations to enhance efficiency, includes a rate limiter feature to control memory impact, offers user-friendly APIs for easy integration, achieved promising results on large models, enables broader applications in various domains, faces challenges in mathematical equivalence and handling shared parameters, and has potential research directions in adaptive sharding strategies, new communication primitives, and combining with other parallelism paradigms.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1910.02054/index.html",
    "href": "podcast/podcasts/1910.02054/index.html",
    "title": "ZeRO Memory Optimizations: Toward Training Trillion Parameter Models",
    "section": "",
    "text": "The paper introduces ZeRO, a novel approach to optimize memory usage when training massive language models. ZeRO-DP and ZeRO-R components effectively reduce memory redundancy and allow for training models with up to 170 billion parameters efficiently. The technique shows superlinear scalability, user-friendly implementation, and has the potential to democratize large model training in AI research.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2305.11627/index.html",
    "href": "podcast/podcasts/2305.11627/index.html",
    "title": "Efficient Compression of Large Language Models using LLM-Pruner",
    "section": "",
    "text": "LLM-Pruner utilizes structural pruning and a post-training method called LoRA to compress LLMs without task-specific retraining. The framework demonstrates promising results in maintaining model performance even with pruning up to 20% of parameters.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2401.10241/index.html",
    "href": "podcast/podcasts/2401.10241/index.html",
    "title": "Zero Bubble Pipeline Parallelism",
    "section": "",
    "text": "Core idea is think about backward pass into two flows, one to compute grad wrt to parameters, and one to compute grad wrt to output of last layer, schedule so that you are always working instead of waiting (bubble).\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/1803.03635/index.html",
    "href": "podcast/podcasts/1803.03635/index.html",
    "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
    "section": "",
    "text": "Engineers and specialists can explore the potential of training more efficient, smaller neural networks by identifying and utilizing winning tickets. The iterative pruning with resetting technique can help in finding these winning tickets, showcasing the importance of proper initialization in network efficiency. Additionally, the use of dropout in conjunction with pruning can enhance the effectiveness of the process, leading to more resource-friendly and faster AI models.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2112.04426/index.html",
    "href": "podcast/podcasts/2112.04426/index.html",
    "title": "Retrieval-Enhanced Transformers (RETRO): A Semi-Parametric Approach to Enhance Performance of Large Language Models",
    "section": "",
    "text": "The paper introduces the RETRO model, which leverages retrieval from a massive text database to enhance large language model performance without increasing model size. Key takeaways include the benefits of linear time complexity for retrieval, the use of frozen BERT for efficient retrieval, and the importance of addressing test set leakage in evaluation.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2302.05543/index.html",
    "href": "podcast/podcasts/2302.05543/index.html",
    "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
    "section": "",
    "text": "ControlNet addresses the challenge of achieving fine-grained control in text-to-image generation by allowing users to provide direct visual input alongside text prompts. Its unique trainable copies of encoding layers and zero convolution layers ensure efficient learning with limited data. The experimental results demonstrate ControlNet’s superiority over existing methods and its potential to rival industrially trained models with fewer computational resources.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2407.02945/index.html",
    "href": "podcast/podcasts/2407.02945/index.html",
    "title": "Extrapolated View Synthesis for Urban Scene Reconstruction",
    "section": "",
    "text": "The paper introduces Extrapolated View Synthesis (EVS) for urban scene reconstruction, addressing limitations in current methods by using 3D Gaussian Splatting for scene representation. By incorporating surface normal information and leveraging diffusion models, the proposed method, VEGS, outperforms existing approaches in generating visually realistic and accurate renderings for urban environments.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2310.01801/index.html",
    "href": "podcast/podcasts/2310.01801/index.html",
    "title": "Models tell you what to discard",
    "section": "",
    "text": "This paper introduces FastGen, a novel method that uses lightweight model profiling and adaptive key-value caching to significantly reduce memory footprint without noticeable quality loss.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/podcasts/2212.07677/index.html",
    "href": "podcast/podcasts/2212.07677/index.html",
    "title": "Unraveling the Connection between In-Context Learning and Gradient Descent in Transformers",
    "section": "",
    "text": "On how Transformers leverage in-context learning mechanisms through gradient descent, enabling them to adapt to new tasks efficiently. Understanding this connection can help improve model generalization, enhance few-shot learning capabilities, and potentially lead to the development of more intelligent and adaptable AI systems.\n  \n  \n\n\n\n  \n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n    \n\n    Listen on your favorite platforms"
  },
  {
    "objectID": "podcast/index.html",
    "href": "podcast/index.html",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "",
    "text": "I love Arxiv, but often find myself with pockets of time when I can’t sit down and read. This podcast bridges that gap, offering bite-sized explorations of individual papers.\nA few things to note:\nThis podcast aims to spark curiosity and make cutting-edge research more accessible. It’s perfect for those moments when you want to learn but can’t dive into a full paper.\nEnjoy the exploration of ideas, and let it fuel your interest in further reading!"
  },
  {
    "objectID": "podcast/index.html#listen-everywhere",
    "href": "podcast/index.html#listen-everywhere",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "Listen everywhere",
    "text": "Listen everywhere"
  },
  {
    "objectID": "podcast/index.html#the-team",
    "href": "podcast/index.html#the-team",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "The Team",
    "text": "The Team\n\nAlex Askwell: Our curious and knowledgeable moderator, always ready with the right questions to guide our exploration.\nDr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and results.\nProf. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n\nJoin them as they break down complex research into byte-sized breakthroughs!"
  },
  {
    "objectID": "posts/friend-or-foe/index.html",
    "href": "posts/friend-or-foe/index.html",
    "title": "Friend Or Foe",
    "section": "",
    "text": "Co-opeartive and Adversarial Environments\n\n\nThis is a mini interactive “game”, I made inspired by the paper, without giving too much away, I urge you to play.\nAI Safety Grid Worlds,\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/harry-potter-quiz/index.html",
    "href": "posts/harry-potter-quiz/index.html",
    "title": "Harry Potter quiz",
    "section": "",
    "text": "Pottermania 2018\n\n\nThe goal of the is to be something easy that anybody with little experience in quizzing can take part in. created with Bitan, and Keyur as part of Quiz club.\n\nQuestions\nGet the questions.\nGet the video for question number 14\nGet the Music for question number 20\n\n\nAnswers\nGet the answers.\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html",
    "title": "Multi Armed Bandits",
    "section": "",
    "text": "Imagine you are at a casino, and you have N slot machines to play, each slot machine gives rewards according to a fixed probability distribution. What strategy should you play with to maximise your total reward ?\nThis problem is known as Multi Armed Bandit problem.\n\n# Importing numpy for math, and matplotlib for plots\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#problem",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#problem",
    "title": "Multi Armed Bandits",
    "section": "",
    "text": "Imagine you are at a casino, and you have N slot machines to play, each slot machine gives rewards according to a fixed probability distribution. What strategy should you play with to maximise your total reward ?\nThis problem is known as Multi Armed Bandit problem.\n\n# Importing numpy for math, and matplotlib for plots\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#arms",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#arms",
    "title": "Multi Armed Bandits",
    "section": "Arms",
    "text": "Arms\nAn arm when pulled, gives a random number from a normal distribution with fixed mean(mu) and deviation(sigma). When pulled many times the frequency of the rewards look like this:\n X axis is the magnitude of reward\nY axis is it’s frequency.\nThe Arm class provides an arm with these properties.\n\nclass Arm:\n\n    def __init__(self, mu=None, sigma=None):\n        if mu is None:\n            self.mu = np.absolute(np.random.uniform())\n        else:\n            self.mu = mu\n        \n        \n        if sigma is None:\n            self.sigma=np.absolute(np.random.uniform())\n        else:\n            self.sigma = sigma\n\n\n    def pull(self):\n        reward = np.random.normal(self.mu, self.sigma, 1)\n        return reward\n\n\ndef get_arms(k):\n    # returns a list of arms\n    arms = []\n    for i in range(k):\n        arms.append(Arm())\n    return arms"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#agents",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#agents",
    "title": "Multi Armed Bandits",
    "section": "Agents",
    "text": "Agents\nAn agent here is a player who pulls arms to play. It has a policy, which is a list of probabilities associated with each arm.\nThe agent class makes designing agents fast. The object is initialised with arms and whether it should play all arms once as part of the initialisation.\nFeatures provided by this class:\nAttributes: * expectations[i]: gives the expected reward on playing arm[i] * times_played[i]: gives the number of times the agent has played arm[i] * N = Total number of times agent has played * reward_history : list of rewards earned by the agent * choice_history : list of choices made by the agent\nMethods: * gamble(i): Plays for i iterations while updating it’s policy. * play(i): Pulls arm[i] and updates reward_history, N , times_played * select_arm(): returns index of an arm by sampling probability distribution given by the policy\n\nclass agent:\n    def __init__(self, arms, play_once=1):\n        self.expectations = np.zeros(len(arms))\n        self.times_played = np.zeros(len(arms))\n        self.arms = arms\n\n        self.number_of_arms = len(arms)\n        self.N = 0\n\n        self.reward_history = []\n        self.choice_history = []\n\n        if play_once == 1:\n            for i in range(self.number_of_arms):\n                self.expectations[i] = self.play(i)\n\n    def play(self, index):\n        reward = self.arms[index].pull()\n\n        self.times_played[index] += 1\n        self.N += 1\n\n        self.choice_history.append(index)\n        self.reward_history.append(reward)\n\n        return reward\n\n    def policy(self):\n        pass\n\n    def update_expectations(self, reward, index):\n        self.expectations[index] += (reward - self.expectations[index])/self.N\n\n    def select_arm(self):\n        options = range(self.number_of_arms)\n        i = np.random.choice(options, p=self.policy(), replace=False)\n        return i\n\n    def gamble(self, iterations):\n        for i in range(iterations):\n            index = self.select_arm()\n            reward = self.play(index)\n            self.update_expectations(reward, index)\n\n\nExample agents\nTo make a new agent we inherit the agent class.\nTime to make some agents!\n\n\nFirst up: epsilon-greedy\nThis agent plays the arm with the highest expected reward with 1 - epsilon probability, and plays a random arm with epsilon probability\nSo\nepsilon = 1 =&gt; random choices\nepsilon = 0 =&gt; greedy choices\n\nclass epsilon_greedy(agent):\n\n    def __init__(self, arms, play_once=1, epsilon=0.1):\n        super().__init__(arms, play_once)\n        self.epsilon = epsilon\n        \n    def __str__(self):\n        return \"Epsilon-Greedy Agent, epsilon= \"+str(self.epsilon)\n    \n    def policy(self):\n        temp = np.zeros_like(self.expectations)\n        temp[np.argmax(self.expectations)] = 1-self.epsilon\n        ans = temp + self.epsilon/self.number_of_arms\n        return ans\n\n\n\nBeta-Softmax\nThis agent plays an arm[i] with probability proportional to: e^(expected_reward(arm[i])/beta)\nWe normalise the whole thing by the sum over all the arms.\n\nclass softmax(agent):\n\n    def __init__(self, arms, play_once=1, beta=1):\n        super().__init__(arms, play_once)\n        self.beta = beta\n        \n    def __str__(self):\n        return \"Softmax agent, beta= \"+ str(self.beta)\n\n    def policy(self):\n        temp = np.exp(self.expectations/self.beta)\n        ans = temp / np.sum(temp, axis=0)\n        return ans\n\n\n\nUpper Confidence Bound (UCB1)\nUCB1 agent plays the arm with the highest metric, where metric of arm i is : metric[i] = expected_reward[i] + sqrt(2*log(N)/times_played[i])\nNote Best peformance when rewards are between 0 and 1\n\nclass ucb(agent):\n\n    def __init__(self, arms, play_once=1):\n        super().__init__(arms, play_once)\n\n    def __str__(self):\n        return \"UCB1 agent\"\n    \n    def policy(self):\n        temp = self.expectations + np.sqrt(2*np.log(self.N)/self.times_played)\n        ans = np.zeros_like(temp)\n        ans[np.argmax(temp)] = 1\n        return ans"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#metrics",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#metrics",
    "title": "Multi Armed Bandits",
    "section": "Metrics",
    "text": "Metrics\nMetric : A scalar number, makes comparison easier.\nTo compare the performance of our agents we can use these metrics\n\navg_reward[i] : this gives the average reward till i+1 iteration.\nmax_reward : this tells us the maximum expected reward\neuclid_distance : we can think of as learnt policy and optimal policy as vectors and compute the distance between them , smaller is better\ncosine_simmilarity : compute the cos(q) between the policies. larger is better\n\n\ndef maxreward(arms):\n    #Max rewards\n    a= [arm.mu for arm in arms]\n    return max(a)\n\ndef avg_reward(rewards):\n    ans = []\n    ans.append(rewards[0])\n    for i in range(1,len(rewards)):\n        ans.append(ans[i-1]+rewards[i])\n    for i in range(len(ans)):\n        ans[i]/=i+1\n    return ans\n\ndef cosine_similarity(a,b):\n    temp = a*b\n    temp/=(euclid_distance(a)* euclid_distance(b))\n    return np.sum(temp, axis=0)\n    \ndef euclid_distance(a):\n    return np.sqrt(np.sum(a*a, axis=0))\n\n\nTest\nThis function takes a list of agents and the number of iterations. Makes each agent play, and prints its metrics.\n\ndef test(agents, iterations):\n    for agent in agents:\n        \n        agent.gamble(iterations)\n        \n        temp = [ arm.mu for arm in levers] \n        optimal_policy = np.zeros_like(agent.expectations)\n        optimal_policy[temp.index(max(temp))] = 1\n        \n        avg_rewards_earned = avg_reward(agent.reward_history)\n        \n        print(agent)\n        print(\"maximum possible reward:\", maxreward(levers))\n        print(\"average reward:\", avg_rewards_earned[-1])\n        print(\"cosine similarity\" ,cosine_similarity(agent.policy(), optimal_policy))\n        euclid_norm = euclid_distance(agent.policy()-optimal_policy)/len(optimal_policy)\n        print(\"euclidian norm \",euclid_norm)\n        \n        \n        plt.plot(avg_rewards_earned)\n        plt.ylabel('Average Reward')\n        plt.xlabel('Iteration')\n        plt.show()\n        print(\"\\n\")\n    \n        # print(\"optimal policy:\" , optimal)\n        # print(\"learnt policy:\" ,agent.policy())\n        \n    \n        \n        # plt.scatter(range(len(agent.choice_history)),y=agent.choice_history)\n        # plt.title(\"Choices\")\n        # plt.xlabel(\"time\")\n        # plt.ylabel(\"arm\")\n        # plt.show()\n        # print(\"\\n\")\n    \n    \n\n\nlevers = get_arms(10)\n\nagents = [\n    epsilon_greedy(levers, epsilon=1),\n    epsilon_greedy(levers, epsilon=0),\n    softmax(levers, beta=0.1),\n    ucb(levers)\n\n]\n\n\nplt.plot([ arm.mu for arm in levers] )\nplt.title(\"distribution of expected value of arms\")\n\nText(0.5, 1.0, 'distribution of expected value of arms')\n\n\n\n\n\n\n\n\n\n\ntest(agents, 5000)\n\nEpsilon-Greedy Agent, epsilon= 1\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.47962497]\ncosine similarity 0.3162277660168379\neuclidian norm  0.09486832980505139\n\n\n\n\n\n\n\n\n\n\n\nEpsilon-Greedy Agent, epsilon= 0\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.98686237]\ncosine similarity 1.0\neuclidian norm  0.0\n\n\n\n\n\n\n\n\n\n\n\nSoftmax agent, beta= 0.1\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.91348264]\ncosine similarity 0.9992727823574249\neuclidian norm  0.008915931500017809\n\n\n\n\n\n\n\n\n\n\n\nUCB1 agent\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.89258379]\ncosine similarity 0.0\neuclidian norm  0.1414213562373095\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimental stuff:\nBelow are a few agents I wrote for fun.\n\n\nclass softmax_with_exponentiation(agent):\n\n    def __init__(self, arms, play_once=1, beta=1, exp=1):\n        super().__init__(arms, play_once)\n        self.beta = beta\n        self.exp = exp\n\n    def policy(self):\n        temp = np.exp(self.expectations/self.beta)\n        ans = temp / np.sum(temp, axis=0)\n        ans = ans**self.exp\n        ans /= np.sum(ans, axis=0)\n        return ans\n\n\nclass softmax_with_reccurence(agent):\n\n    def __init__(self, arms, play_once=1, beta=1):\n        super().__init__(arms, play_once)\n        self.old_policy = np.ones_like(self.expectations)/self.l\n        self.beta = beta\n\n    def policy(self):\n        temp = np.exp(self.expectations/self.beta)\n        new_policy = temp / np.sum(temp, axis=0)\n\n        result = np.multiply(new_policy, self.old_policy)\n        result /= np.sum(result, axis=0)\n        self.old_policy = result\n\n        return result\n\n\nclass greedy_with_reccurence(agent):\n    # alpha = number &lt; 1; will sum over a number of observations and will keep\n    # osiclating.\n    # alpha = N will allow the algo to converge to an arm, greedy doesn't\n    # really need this, kind of always give one answer.\n\n    def __init__(self, arms, play_once=1, alpha=1):\n        super().__init__(arms, play_once)\n        self.old_policy = np.ones_like(self.expectations)\n        self.alpha = alpha\n\n    def policy(self):\n        new_policy = np.zeros_like(self.expectations)\n        new_policy[np.argmax(self.expectations)] = 1\n\n        new_policy = (1-self.alpha)*new_policy + self.alpha*self.old_policy\n\n        new_policy /= np.sum(new_policy, axis=0)\n        self.old_policy = new_policy\n\n        return new_policy\n\n# class magic(agent):\n#    def __init__(self, arms, play_once=1, exp=1):\n#        super().__init__(arms, play_once)\n#        self.old_policy = np.ones_like(self.expectations)/self.l\n#        self.exp = exp\n#\n#    def policy(self):\n#        new_policy = f(old_policy, g(expectations))"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html",
    "href": "posts/game-of-thrones-quiz/index.html",
    "title": "Game Of Thrones quiz",
    "section": "",
    "text": "Game Of Thrones Quiz\nThis quiz was created as part of cultural week, right as Game of Thrones season 7 ended.\nIt is broken into 3 seperate rounds"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html#first-round",
    "href": "posts/game-of-thrones-quiz/index.html#first-round",
    "title": "Game Of Thrones quiz",
    "section": "First round",
    "text": "First round\nQuestions\nAnswers"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html#second-round",
    "href": "posts/game-of-thrones-quiz/index.html#second-round",
    "title": "Game Of Thrones quiz",
    "section": "Second round",
    "text": "Second round\nSlides\nvideo for question 1\nmusic for question 2\nvideo for question 3"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html#third-round",
    "href": "posts/game-of-thrones-quiz/index.html#third-round",
    "title": "Game Of Thrones quiz",
    "section": "Third round",
    "text": "Third round\nConnect"
  },
  {
    "objectID": "posts/about-this-website/index.html",
    "href": "posts/about-this-website/index.html",
    "title": "About this website",
    "section": "",
    "text": "This post is a bit of a meta post about arjunsriva.com\nA lot of my thinking on personal websites has been influenced by other great websites like Gwern’s, please read this if you’re interested in it.\nMy goals for this websites are:\n\nTo share things I learned that I personally found useful\nTo provide a playground for me to flesh out rough ideas and speculate.\nTo help me increase the clarity of my thinking by the act of writing something out.\n\non the implementation side my goals are\n\nMake it easy for me to never lose data\n\nmost writing is stored as simple text files, version controlled by git\n\nMake it easy to mix code and prose to explain certain concepts\n\nSome things that I plan on implementing later as I write more are:\n\nAutomatic link archiving to prevent link rot\nConfidence tags to show how certain I am of different things\nBetter sections / tagging to differentiate different kinds of posts, eg. reading list and notes\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/state-of-reading/index.html",
    "href": "posts/state-of-reading/index.html",
    "title": "The state of reading in 2018 and beyond.",
    "section": "",
    "text": "Audio books are seeing a resurgence, because of companies like Audible.\nText to speech is good, and is getting better .\nWireless Bluetooth earphones are becoming common place, the increase in convenience and battery life allows people to have them on longer.\nUnlike content that you have to watch/read, you can work on other stuff while you listen.\nI see a time where we can listen to most of the things we read.\nCreates a whole new kind of medium.\nNews, blog posts and Stories most affected.\nMost of the speech generated by machines.\nMost of the classic texts available for free on the internet.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/jargonizer/index.html",
    "href": "posts/jargonizer/index.html",
    "title": "Jargonizer",
    "section": "",
    "text": "Jargonizer\nWhat if instead of aiming for clarity and conciesness, our goal was to make our sentences as unweildy and hard to understand as possible?\nA simple way to do this is to replace easy to understand phrases with harder ones. Wikipedia maintains1 one such list for us. Here is the website I made so you can play with it online as well, it all runs in your browser, no text is sent to a server.\nReading at the output generated from this, I can’t help but feel like I am reading a legal document."
  },
  {
    "objectID": "posts/jargonizer/index.html#footnotes",
    "href": "posts/jargonizer/index.html#footnotes",
    "title": "Jargonizer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsed to 🥲↩︎"
  }
]