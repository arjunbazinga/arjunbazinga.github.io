[
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "TIL: Today I Learned",
    "section": "",
    "text": "A collection of links, notes, snippets on things I am reading, watching.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "podcast/podcasts/2407.02524/index.html",
    "href": "podcast/podcasts/2407.02524/index.html",
    "title": "Training Large Language Models for Compiler Optimization",
    "section": "",
    "text": "The research paper discusses the development of LLM Compiler, a model specifically trained on compiler IRs and assembly code for optimizing code efficiently. This approach outperforms traditional techniques and existing LLMs in tasks like flag tuning and disassembly, showing potential for automating and improving the optimization process in software engineering.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2406.02532/index.html",
    "href": "podcast/podcasts/2406.02532/index.html",
    "title": "Speculative Execution for Efficient Inference in Large Language Models on Consumer Devices",
    "section": "",
    "text": "SpecExec introduces a two-step parallel processing method using draft and target models to speed up inference on consumer devices. It achieved impressive interactive inference speeds, providing real-time responses for applications like chatbots. The approach addresses the limitations of existing speculative decoding methods and holds promise for democratizing access to powerful language models.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2212.08073/index.html",
    "href": "podcast/podcasts/2212.08073/index.html",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "",
    "text": "Engineers and specialists can benefit from this research by understanding the innovative approach of using constitutional principles to guide AI behavior and self-correct harmful outputs. The study shows that CAI models outperformed traditional methods in terms of harmlessness while maintaining comparable levels of helpfulness, indicating a promising direction for developing more ethical and trustworthy AI systems.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2501.06252/index.html",
    "href": "podcast/podcasts/2501.06252/index.html",
    "title": "Transformer2: Self-Adaptive Large Language Models",
    "section": "",
    "text": "Key takeaways are that SVF outperforms traditional fine-tuning methods like LoRA in efficiency, flexibility, and robustness. The paper also introduces innovative adaptation strategies like Few-Shot Adaptation using the Cross-Entropy Method, showcasing the effectiveness of the Transformer2 framework in adaptive AI systems.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2501.18512v1/index.html",
    "href": "podcast/podcasts/2501.18512v1/index.html",
    "title": "Streaming DiLoCo: Efficient Distributed Training of Large Language Models",
    "section": "",
    "text": "Streaming DiLoCo introduces three main improvements: streaming synchronization reduces peak bandwidth, overlapping communication with computation hides latency, and quantization compresses data exchanged between workers. The research shows similar performance to Data-Parallel training but with significantly reduced bandwidth, making it a promising approach for distributed LLM training.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2306.14892/index.html",
    "href": "podcast/podcasts/2306.14892/index.html",
    "title": "Decision-Pretrained Transformer: Bridging Supervised Learning and Reinforcement Learning",
    "section": "",
    "text": "Engineers and specialists can leverage the DPT methodology to design more versatile and efficient RL agents. By learning a decision-making strategy through supervised pretraining, DPT demonstrates adaptability to new environments, ability to explore and exploit, and strong generalization capabilities. This approach offers a promising path towards practical and efficient Bayesian RL methods.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2501.00663v1/index.html",
    "href": "podcast/podcasts/2501.00663v1/index.html",
    "title": "Titans: Learning to Memorize at Test Time",
    "section": "",
    "text": "The key takeaways for engineers/specialists are that effective memory models need to be dynamic, surprise-driven, and have mechanisms to forget the past. The research showcases how incorporating a neural long term memory module that continuously learns at test time can lead to higher performance in language modeling, common-sense reasoning, needle-in-a-haystack tasks, DNA modeling, and time-series forecasting. By introducing the Titans architecture, the paper provides a framework for effectively integrating such memory modules into various tasks.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1807.05358/index.html",
    "href": "podcast/podcasts/1807.05358/index.html",
    "title": "Efficient Deep Learning Parallelization using SOAP Search Space and FlexFlow Framework",
    "section": "",
    "text": "The SOAP search space allows for flexible parallelization strategies across Sample, Operation, Attribute, and Parameter dimensions, outperforming traditional methods by up to 3.8 times. FlexFlow’s simulator predicts performance without real executions, reducing search time and enhancing efficiency.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2403.13187/index.html",
    "href": "podcast/podcasts/2403.13187/index.html",
    "title": "Evolutionary Optimization of Model Merging Recipes",
    "section": "",
    "text": "Engineers and specialists can leverage the Evolutionary Model Merge method to automate the process of combining pre-trained models, eliminating the need for human intuition and expanding the search space for potential model combinations. This approach opens up possibilities for developing more efficient, cost-effective, and powerful AI systems with emergent capabilities.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2111.15397/index.html",
    "href": "podcast/podcasts/2111.15397/index.html",
    "title": "NeuralProphet Explainable Forecasting at Scale",
    "section": "",
    "text": "‘Successor’ of Prophet (by facebook) for time series modelling.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2310.13810/index.html",
    "href": "podcast/podcasts/2310.13810/index.html",
    "title": "A Better Match for Drivers and Riders Reinforcement Learning at Lyft",
    "section": "",
    "text": "The paper demonstrates the successful application of reinforcement learning to improve the efficiency of driver-rider matching in ride-sharing platforms. The use of online RL allows for real-time adaptation, resulting in decreased wait times for riders, increased earnings for drivers, and overall higher user satisfaction. The research paves the way for more intelligent systems in the ride-sharing industry, with potential for further optimization and expansion into various other aspects of the ecosystem.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2302.05543/index.html",
    "href": "podcast/podcasts/2302.05543/index.html",
    "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
    "section": "",
    "text": "ControlNet addresses the challenge of achieving fine-grained control in text-to-image generation by allowing users to provide direct visual input alongside text prompts. Its unique trainable copies of encoding layers and zero convolution layers ensure efficient learning with limited data. The experimental results demonstrate ControlNet’s superiority over existing methods and its potential to rival industrially trained models with fewer computational resources.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2208.01066/index.html",
    "href": "podcast/podcasts/2208.01066/index.html",
    "title": "In-Context Learning Capabilities of Transformers",
    "section": "",
    "text": "The key takeaways for engineers/specialists are that Transformers demonstrate robust in-context learning capabilities for various function classes, showing flexibility and adaptability without the need for fine-tuning. The study emphasizes the importance of model capacity and the potential benefits of curriculum learning for training efficiency.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/deepseek-r1/index.html",
    "href": "podcast/podcasts/deepseek-r1/index.html",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "section": "",
    "text": "The key takeaways for engineers/specialists are: 1. Powerful reasoning can emerge from pure reinforcement learning without strict supervised fine-tuning. 2. A multi-stage pipeline using cold-start data can significantly improve the results of RL training. 3. Effective distillation techniques allow transferring reasoning knowledge from larger models to smaller, more efficient models for practical deployment.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2406.07550/index.html",
    "href": "podcast/podcasts/2406.07550/index.html",
    "title": "TiTok: A Transformer-based 1D Tokenization Approach for Image Generation",
    "section": "",
    "text": "TiTok introduces a novel 1D tokenization method for image generation, enabling the representation of images with significantly fewer tokens while maintaining or surpassing the performance of existing 2D grid-based methods. The approach leverages a Vision Transformer architecture, two-stage training with proxy codes, and achieves remarkable speedup in training and inference. The research opens up new possibilities for efficient and high-quality image generation, with implications for various applications in computer vision and beyond.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2406.17345/index.html",
    "href": "podcast/podcasts/2406.17345/index.html",
    "title": "NerfBaselines: A Framework for Standardized Evaluation of Novel View Synthesis Methods in Computer Vision",
    "section": "",
    "text": "NerfBaselines addresses the inconsistent evaluation protocols in comparing novel view synthesis methods by providing a unified interface, ensuring reproducibility through containerization, and standardizing the evaluation protocol. By enabling the sharing of pre-trained checkpoints, it reduces computational costs and environmental impact. However, it relies on methods exposing the same interface and future directions involve exploring advanced evaluation metrics and addressing the computational cost of training.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2208.07339/index.html",
    "href": "podcast/podcasts/2208.07339/index.html",
    "title": "Efficient Inference for Large Language Models with LLM.int8()",
    "section": "",
    "text": "Engineers can leverage LLM.int8() to reduce memory requirements and efficiently run large language models without performance degradation, even at scales exceeding billions of parameters. The method incorporates vector-wise quantization and mixed-precision decomposition to maintain full 16-bit performance in perplexity and zeroshot accuracy across large models, demonstrating significant memory savings and modest speedups for inference.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2305.11627/index.html",
    "href": "podcast/podcasts/2305.11627/index.html",
    "title": "Efficient Compression of Large Language Models using LLM-Pruner",
    "section": "",
    "text": "LLM-Pruner utilizes structural pruning and a post-training method called LoRA to compress LLMs without task-specific retraining. The framework demonstrates promising results in maintaining model performance even with pruning up to 20% of parameters.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2212.07677/index.html",
    "href": "podcast/podcasts/2212.07677/index.html",
    "title": "Unraveling the Connection between In-Context Learning and Gradient Descent in Transformers",
    "section": "",
    "text": "On how Transformers leverage in-context learning mechanisms through gradient descent, enabling them to adapt to new tasks efficiently. Understanding this connection can help improve model generalization, enhance few-shot learning capabilities, and potentially lead to the development of more intelligent and adaptable AI systems.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2406.11066/index.html",
    "href": "podcast/podcasts/2406.11066/index.html",
    "title": "Metadata-based Color Harmonization for Multi-camera Surround View Systems",
    "section": "",
    "text": "The paper introduces a metadata-based approach to address color inconsistencies in multi-camera surround view systems, crucial for accurate perception in autonomous driving. The method significantly outperforms traditional techniques in visual quality and runtime, making it more efficient and robust for real-time applications.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2403.15378/index.html",
    "href": "podcast/podcasts/2403.15378/index.html",
    "title": "Long-CLIP: Extending Text Length for Improved Vision-Language Modeling",
    "section": "",
    "text": "Long-CLIP significantly extends the text length without disrupting existing representations, improving recall rates on long and short caption retrieval tasks. Its plug-and-play nature enables integration into various downstream applications, showing promise in enhancing image generation models and opening up possibilities for realistic and detailed content creation.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2211.02131/index.html",
    "href": "podcast/podcasts/2211.02131/index.html",
    "title": "SafePathNet: Learning a Distribution of Trajectories for Safe and Comfortable Autonomous Driving",
    "section": "",
    "text": "SafePathNet introduces a novel approach that models the distribution of future trajectories for both the self-driving vehicle and other road agents using a unified neural network architecture. By incorporating a ‘Mixture of Experts’ framework, the model can learn diverse driving strategies and prioritize safety in real-time decision-making. The use of Transformer networks and imitation learning further enhances the model’s ability to handle complex and unpredictable driving scenarios.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2407.02945/index.html",
    "href": "podcast/podcasts/2407.02945/index.html",
    "title": "Extrapolated View Synthesis for Urban Scene Reconstruction",
    "section": "",
    "text": "The paper introduces Extrapolated View Synthesis (EVS) for urban scene reconstruction, addressing limitations in current methods by using 3D Gaussian Splatting for scene representation. By incorporating surface normal information and leveraging diffusion models, the proposed method, VEGS, outperforms existing approaches in generating visually realistic and accurate renderings for urban environments.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2007.07203/index.html",
    "href": "podcast/podcasts/2007.07203/index.html",
    "title": "Deep Retrieval: Learning Efficient Structures for Large-Scale Recommendation Systems",
    "section": "",
    "text": "Engineers and specialists can benefit from the paper by understanding how DR revolutionizes large-scale recommendation systems through its innovative approach of learning efficient structures directly from user-item interactions. By adopting a path-based mechanism and utilizing multi-path designs, DR can provide accurate recommendations comparable to computationally expensive methods while remaining more efficient. The ability of DR to handle diverse preferences, promote less popular content, and improve user engagement highlights its potential to reshape recommendation systems for better performance and inclusivity.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2310.08370/index.html",
    "href": "podcast/podcasts/2310.08370/index.html",
    "title": "UniPAD: A Universal Pre-training Paradigm for Autonomous Driving",
    "section": "",
    "text": "UniPAD is a novel self-supervised learning framework designed for autonomous driving, focusing on learning effective representations from 3D data such as LiDAR point clouds and multi-view images. The framework consists of a modality-specific encoder, a mask generator for challenging training, a unified 3D volumetric representation, and a neural rendering decoder. UniPAD showed promising results in improving performance on tasks like 3D object detection and semantic segmentation, outperforming other pre-training methods and offering potential for broader applications beyond autonomous driving.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1904.00420/index.html",
    "href": "podcast/podcasts/1904.00420/index.html",
    "title": "Single Path One-Shot (SPOS): Efficient Neural Architecture Search with Simplified Supernet",
    "section": "",
    "text": "SPOS addresses limitations of existing NAS methods by simplifying the supernet structure, utilizing an evolutionary algorithm, and incorporating channel search and mixed-precision quantization. The approach outperforms previous methods in accuracy, complexity, and resource efficiency. It demonstrates strong correlation between supernet and individual architecture performance, enhancing the search process efficiency.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2002.11252/index.html",
    "href": "podcast/podcasts/2002.11252/index.html",
    "title": "AutoEmb Automated Embedding Dimensionality Searchg in Streaming Recommendations",
    "section": "",
    "text": "AutoEmb is about using different lenghts of embedding vectors for different items, use less memory + potentially learn more robust stuff for items with less data, and learn more nuanced stuff for popular items.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2205.14135/index.html",
    "href": "podcast/podcasts/2205.14135/index.html",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    "section": "",
    "text": "FlashAttention is a novel algorithm that addresses the efficiency of Transformer models by improving speed and memory efficiency through IO-awareness. It reduces the number of memory accesses by dividing data into smaller blocks and loading them into fast memory, achieving practical speedups and enabling training on longer sequences. The algorithm also incorporates recomputation during the backward pass to minimize memory usage, delivering significant improvements in training large models like BERT and GPT-2.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2411.15124/index.html",
    "href": "podcast/podcasts/2411.15124/index.html",
    "title": "Tülu 3: Pushing Frontiers in Open Language Model Post-Training",
    "section": "",
    "text": "Key takeaways include the introduction of RLVR for task alignment, emphasis on data quality and decontamination for model generalization, and the significance of releasing comprehensive training resources for transparent and reproducible results.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2310.01801/index.html",
    "href": "podcast/podcasts/2310.01801/index.html",
    "title": "Models tell you what to discard",
    "section": "",
    "text": "This paper introduces FastGen, a novel method that uses lightweight model profiling and adaptive key-value caching to significantly reduce memory footprint without noticeable quality loss.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2304.11277/index.html",
    "href": "podcast/podcasts/2304.11277/index.html",
    "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel",
    "section": "",
    "text": "FSDP addresses memory capacity challenges by sharding parameters across devices, employs communication optimizations to enhance efficiency, includes a rate limiter feature to control memory impact, offers user-friendly APIs for easy integration, achieved promising results on large models, enables broader applications in various domains, faces challenges in mathematical equivalence and handling shared parameters, and has potential research directions in adaptive sharding strategies, new communication primitives, and combining with other parallelism paradigms.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2212.10156/index.html",
    "href": "podcast/podcasts/2212.10156/index.html",
    "title": "Planning-Oriented Autonomous Driving",
    "section": "",
    "text": "The paper introduces UniAD, a planning-oriented framework for autonomous driving that focuses on integrating perception, prediction, and planning tasks to optimize for safe and efficient driving. UniAD outperforms existing state-of-the-art methods in motion forecasting, occupancy prediction, and planning, showcasing the benefits of joint optimization and query-based communication between modules. Key challenges for future research include addressing computational complexity, handling long-tail scenarios, and exploring additional tasks like depth estimation and behavior prediction.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2112.04426/index.html",
    "href": "podcast/podcasts/2112.04426/index.html",
    "title": "Retrieval-Enhanced Transformers (RETRO): A Semi-Parametric Approach to Enhance Performance of Large Language Models",
    "section": "",
    "text": "The paper introduces the RETRO model, which leverages retrieval from a massive text database to enhance large language model performance without increasing model size. Key takeaways include the benefits of linear time complexity for retrieval, the use of frozen BERT for efficient retrieval, and the importance of addressing test set leakage in evaluation.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1911.01547/index.html",
    "href": "podcast/podcasts/1911.01547/index.html",
    "title": "On the Measure of Intelligence",
    "section": "",
    "text": "Key takeaways for engineers/specialists include the importance of skill-acquisition efficiency in measuring intelligence, the emphasis on building systems with adaptability and generalization capabilities, and the potential impact of such research on areas like education, healthcare, and robotics.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1805.08941/index.html",
    "href": "podcast/podcasts/1805.08941/index.html",
    "title": "AutoPruner: End-to-End Trainable Filter Pruning for Efficient Deep Neural Networks",
    "section": "",
    "text": "AutoPruner presents a significant advancement in filter pruning for deep neural networks by integrating the filter selection process into model training, eliminating the need for separate pruning steps. The methodology outperformed state-of-the-art methods, showcasing superior accuracy and compression ratios on standard datasets like CUB200-2011 and ImageNet ILSVRC-12. The innovative approach of AutoPruner could lead to more efficient and accessible deep learning models across various applications.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1707.06347/index.html",
    "href": "podcast/podcasts/1707.06347/index.html",
    "title": "Proximal Policy Optimization Algorithms",
    "section": "",
    "text": "Engineers and specialists can benefit from PPO’s balancing act between simplicity and effectiveness, enabling more stable and efficient training with less data. Additionally, the clipping mechanism allows for smoother updates and multiple minibatch updates, enhancing the algorithm’s sample complexity and performance compared to traditional policy gradient methods.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2402.07945/index.html",
    "href": "podcast/podcasts/2402.07945/index.html",
    "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
    "section": "",
    "text": "The key takeaways for engineers/specialists are: 1. ScreenAgent enables VLMs to control real computer screens by generating plans and translating them into low-level commands. 2. ScreenAgent outperforms other models in precise UI positioning, showing promise for more accurate interaction with computer interfaces. 3. Future research directions include enhancing visual localization capabilities, improving planning mechanisms, and expanding capabilities to handle videos and multi-frame images.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2304.08069/index.html",
    "href": "podcast/podcasts/2304.08069/index.html",
    "title": "RT-DETR: Real-Time Object Detection with Transformer",
    "section": "",
    "text": "RT-DETR is a groundbreaking end-to-end real-time object detector based on Transformers that combines the speed of YOLO with the accuracy of DETR. Key takeaways for engineers include the efficient hybrid encoder approach, which improves multi-scale feature interactions, and the uncertainty-minimal query selection scheme, enhancing accuracy in both classification and localization. Despite outperforming traditional CNN-based methods, RT-DETR faces challenges in detecting small objects, prompting future research directions like knowledge distillation.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2401.10241/index.html",
    "href": "podcast/podcasts/2401.10241/index.html",
    "title": "Zero Bubble Pipeline Parallelism",
    "section": "",
    "text": "Core idea is think about backward pass into two flows, one to compute grad wrt to parameters, and one to compute grad wrt to output of last layer, schedule so that you are always working instead of waiting (bubble).\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2501.12326/index.html",
    "href": "podcast/podcasts/2501.12326/index.html",
    "title": "Bytedance: UI-TARS: End-to-End Model for Automated GUI Interaction",
    "section": "",
    "text": "Key takeaways for engineers/specialists from the paper include the introduction of a novel end-to-end architecture for GUI agents, utilizing enhanced perception for improved understanding of GUI elements, implementing unified action modeling for platform-agnostic interactions, incorporating system-2 reasoning for deliberate decision-making, and utilizing iterative training with reflective online traces to continuously improve model performance.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1610.03013/index.html",
    "href": "podcast/podcasts/1610.03013/index.html",
    "title": "Comprehensive Guide to Real-Time Bidding (RTB): Challenges and Opportunities",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper are the importance of accurate user response prediction for targeted advertising, the need for advanced bidding strategies based on estimated utility, and the significance of dynamic pricing optimization and ad fraud detection techniques to ensure a fair and efficient advertising ecosystem.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1905.12784/index.html",
    "href": "podcast/podcasts/1905.12784/index.html",
    "title": "Geometric Properties of Data Representations in Deep Neural Networks",
    "section": "",
    "text": "Key takeaways for engineers/specialists include the discovery of a ‘hunchback’ shape for intrinsic dimensionality across layers of Convolutional Neural Networks (CNNs), with a strong correlation between the ID in the final layer and performance on unseen data. The findings indicate that deep networks compress information into low-dimensional manifolds to generalize effectively, involving non-linear transformations for achieving linearly separable representations.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1611.02779/index.html",
    "href": "podcast/podcasts/1611.02779/index.html",
    "title": "RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning",
    "section": "",
    "text": "Engineers and specialists can benefit from RL2 by understanding how meta-learning can bridge the gap between slow deep reinforcement learning and fast human learning speeds. This approach offers a way to encode prior knowledge in an RNN to make RL algorithms more efficient, adaptable, and scalable to complex real-world scenarios.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1712.01208/index.html",
    "href": "podcast/podcasts/1712.01208/index.html",
    "title": "The Case for Learned Index Structures",
    "section": "",
    "text": "Learned indexes offer significant performance gains and memory savings compared to traditional structures across various datasets. The Recursive Model Index (RMI) architecture helps improve prediction accuracy, and the potential for hybrid indexing combining neural networks and traditional techniques showcases a promising future for enhancing database systems’ efficiency and scalability.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2406.12214/index.html",
    "href": "podcast/podcasts/2406.12214/index.html",
    "title": "Robustness Evaluation of HD Map Constructors under Sensor Corruptions for Autonomous Driving",
    "section": "",
    "text": "The paper focuses on evaluating the robustness of HD map constructors under various sensor corruptions using a comprehensive benchmark called MapBench. It highlights the vulnerability of existing methods to real-world challenges and suggests the importance of advanced data augmentation techniques and new network architectures to enhance robustness for autonomous driving applications.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nOn SaaS and Swimming: Hidden Factors of Success\n\n\n\n\n\n\nBusiness\n\n\n\nHow to figure out what matters in business and sports\n\n\n\n\n\nAug 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExponentials Everywhere: The S-Curve Challenge in Predicting the Future\n\n\n\n\n\n\nForecasting\n\n\nMath\n\n\n\nS-curves are way trickier than you’d think. Here’s why predicting pandemics, product adoption, and other exponential phenomena is far more challenging than it seems – and what that means for all of us.\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentation Platforms\n\n\n\n\n\n\nExperimentation\n\n\n\nAn Overview of Experimentation Platforms.\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Schedule Generator\n\n\n\n\n\n\nIIT Indore\n\n\n\nA website which adds courses to your personal calendar, for students of IIT Indore.\n\n\n\n\n\nJan 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHarry Potter quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA beginner friendly quiz on the wizarding world\n\n\n\n\n\nAug 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nThe state of reading in 2018 and beyond.\n\n\n\n\n\n\nFuture\n\n\n\n\n\n\n\n\n\nAug 5, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nFriend Or Foe\n\n\n\n\n\n\nAI\n\n\nProjects\n\n\n\nAn interactive game inspired by the paper “AI Safety Grid Worlds”\n\n\n\n\n\nJul 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nInfant Mortality In India\n\n\n\n\n\n\nAnalysis\n\n\n\nAn analysis of infant mortality rates across India.\n\n\n\n\n\nJun 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nJargonizer\n\n\n\n\n\n\nProjects\n\n\n\nGenerate sentences that nobody can read.\n\n\n\n\n\nJun 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Machine Learning\n\n\n\n\n\n\nAI\n\n\n\n\n\n\n\n\n\nNov 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGame Of Thrones quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz made for cultural week\n\n\n\n\n\nSep 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nNewbie quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz to introduce quizzing to new people and narrowing down potential recruits.\n\n\n\n\n\nAug 20, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMental Health Checklists\n\n\n\n\n\n\nProjects\n\n\n\nA website which allows quick tracking of mental health.\n\n\n\n\n\nJan 8, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/jargonizer/index.html",
    "href": "posts/jargonizer/index.html",
    "title": "Jargonizer",
    "section": "",
    "text": "Jargonizer\nWhat if instead of aiming for clarity and conciesness, our goal was to make our sentences as unweildy and hard to understand as possible?\nA simple way to do this is to replace easy to understand phrases with harder ones. Wikipedia maintains1 one such list for us. Here is the website I made so you can play with it online as well, it all runs in your browser, no text is sent to a server.\nReading at the output generated from this, I can’t help but feel like I am reading a legal document."
  },
  {
    "objectID": "posts/jargonizer/index.html#footnotes",
    "href": "posts/jargonizer/index.html#footnotes",
    "title": "Jargonizer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsed to 🥲↩︎"
  },
  {
    "objectID": "posts/saas-and-swimming/index.html",
    "href": "posts/saas-and-swimming/index.html",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "",
    "text": "Sometimes, insight comes from unexpected places. Today, it came from the pool.\nToday while swimming, I had a realization: there’s a lot in common between swimming well and running a successful subscription business."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-basics-drag-thrust-and-speed",
    "href": "posts/saas-and-swimming/index.html#the-basics-drag-thrust-and-speed",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Basics: Drag, Thrust, and Speed",
    "text": "The Basics: Drag, Thrust, and Speed\nWhen you first start swimming, you think it’s all about moving your hands and legs as powerfully as possible. More force equals more speed, right? This is partly true, but it misses a crucial element: drag.\nWhat matters when swimming is your average velocity over a distance. If you have too much drag, you’ll return to zero velocity after each stroke. But when your drag is low enough, you will accumulate speed with each stroke, reaching a top speed much higher than you would even with your strongest stroke.\nIn business terms:\n\nDrag is your Churn rate, i.e. the opposite of retention rate\nThrust is your marketing effort at any given time\nSpeed in water is your active user base\n\nWhile marketing is necessary, your active userbase (or speed in the water) is a combination of your past marketing efforts and your retention rate.\nHere’s a visual comparison of efficient vs inefficient swimming:"
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-hidden-drivers",
    "href": "posts/saas-and-swimming/index.html#the-hidden-drivers",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Hidden Drivers",
    "text": "The Hidden Drivers\nHere’s something counterintuitive: in swimming, your most crucial muscles aren’t the ones you see moving. Your arms and legs create the splash, but it’s your core and hips that truly drive performance. They work silently beneath the surface, maintaining your streamlined form and minimizing drag keeping your body long and horizontal in the water.\nThe same principle applies in business. It’s easy to fixate on the visible elements - your marketing campaigns, landing pages, and ads. These are your business’s arms and legs, visibly pushing through the market. But your customer support, product reliability, and user experience? They’re your core and hips - less flashy, but absolutely critical.\nJust as a swimmer with weak core muscles will struggle regardless of arm strength, a business with poor user experience will flounder despite a massive marketing budget."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-paradox-of-effort",
    "href": "posts/saas-and-swimming/index.html#the-paradox-of-effort",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Paradox of Effort",
    "text": "The Paradox of Effort\nEven in swimming, there are situations where you need to decide between increasing thrust and reducing drag. A prime example is the use of legs. While leg movement contributes about 15% to overall thrust, it dramatically increases drag and often worsens your form. Counterintuitively, the advice for swimming well, especially when starting out, is to use your legs less – sometimes barely moving them at all.\nThis paradox has a direct parallel in business. Many companies engage in practices that seem to increase “thrust” but actually create more “drag”:\n\nAggressive upselling and cross-selling\nIntrusive advertising\nUser interface “dark patterns” that trick users into spending more\n\nYes, these tactics might bring in additional revenue (power) in the short term. But in the long run, they increase your drag by driving users away from your product. It’s a classic case of short-term gain leading to long-term pain. For a deeper dive into this concept and how to approach it, I recommend reading the insightful article on user disengagement by Zerodha."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#reducing-business-drag",
    "href": "posts/saas-and-swimming/index.html#reducing-business-drag",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "Reducing Business Drag",
    "text": "Reducing Business Drag\nIn business, reducing drag means focusing on retention. I’ve found this means:\n\nUnderstand why people leave. Talk to unhappy customers.\nIdentify unmet needs and fix what’s broken.\nDo the unglamorous work that’s important to customers.\nTreat customers as you’d want to be treated, be respectful of their time and attention.\n\nThese actions might not feel as impactful as big marketing pushes, or as cool as working on bleeding-edge technology, but they’re crucial for long-term success. They’re the equivalent of perfecting your swimming form - less visible, more effective."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#optimizing-for-success",
    "href": "posts/saas-and-swimming/index.html#optimizing-for-success",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "Optimizing for Success",
    "text": "Optimizing for Success\nAnother parallel: You can only optimize one aspect at a time. There’s a limit to what your brain can focus on. In swimming, you might want to improve your arm stroke, leg kick, hip rotation, and breathing simultaneously. In business, you’re juggling product improvements, customer support, marketing, and operations.\nTrying to fix everything at once leads to slow progress and ingrained bad habits. Instead:\n\nChoose one specific area for improvement\nPractice with perfect form and intense focus\nRepeat until the action becomes automatic\n\nThis applies to both swimming and running a company, but with a key difference. In swimming, “automatic” means the movement becomes part of your neural structure, performed subconsciously. In business, it’s about setting up the right culture, people, processes, and incentives so things run smoothly without constant oversight.\nThis approach might seem slow, but it leads to sustainable, compounding improvements. You’re not just doing things right when actively focusing - you’re making high standards your default state.\nYou’ll notice Olympic swimmers cross the pool in minimal strokes - that’s mastery in action. In business, it might look like efficient processes, strong culture, the right people, and well-set incentives."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#the-long-game",
    "href": "posts/saas-and-swimming/index.html#the-long-game",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "The Long Game",
    "text": "The Long Game\nIn swimming and in business, what looks like effort isn’t always what moves you forward.\nSometimes, the key to speed is doing less. Moving your legs less in the water. Pushing your customers less.\nFocus on reducing drag. In SaaS, that means understanding your users. Solving their problems. Respecting their time.\nIt’s not as visible as marketing. But it’s what separates great products from the rest."
  },
  {
    "objectID": "posts/saas-and-swimming/index.html#conclusions",
    "href": "posts/saas-and-swimming/index.html#conclusions",
    "title": "On SaaS and Swimming: Hidden Factors of Success",
    "section": "Conclusions",
    "text": "Conclusions\nReflecting on these parallels between swimming and business, I’m reminded of Miyamoto Musashi’s quote, which could be a fitting end to this post.\n\n“If you know the way broadly you will see it in everything.”\n\nBut I’m also reminded of Abraham Maslow.\n\n“If the only tool you have is a hammer, you tend to see every problem as a nail.”\n\nBecause while there’s wisdom in drawing parallels, there’s also wisdom in knowing when to put down the metaphorical hammer.\nHere are three important ways where the analogy breaks down which you should keep in mind.\n\nEnvironmental Consistency: Water provides a consistent environment for swimmers. In contrast, the business landscape is constantly changing, more akin to swimming in a dynamic, unpredictable ocean. your drag (churn) could suddenly rapidly increase because of competitors, regulation, effectiveness of your channels etc.\nFeedback Loops: In swimming, improvements in technique have a relatively linear effect on performance. In business, positive feedback loops (like network effects) can lead to exponential growth, a phenomenon not typically seen in swimming.\nNature of the End Goal: In swimming, the objective is typically to minimize time over a fixed distance. This is a well-defined, closed-ended goal. In business, particularly for SaaS, the goal is often open-ended growth. There’s no fixed “finish line” - the objective is continuous expansion and improvement, potentially without limit.\n\nSo take this post with a pinch of chlorine, maybe it’ll help you cut through the drag in your business (or your breaststroke), or maybe it’ll just give you a chuckle the next time you’re at the pool.\nEither way, happy swimming - and happy entrepreneur-ing!\nPS: If you’re learning to swim, I highly recommend “Total Immersion: The Revolutionary Way to Swim Better, Faster, and Easier”."
  },
  {
    "objectID": "posts/mental-health-checklist/index.html",
    "href": "posts/mental-health-checklist/index.html",
    "title": "Mental Health Checklists",
    "section": "",
    "text": "Some quizes I made so it’s easy to keep track of mental health.\n\nBurn’s depression Checklist use it here\nNovaco’s Anger Scale use it here"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html",
    "href": "posts/exerimentations-platforms/index.html",
    "title": "Experimentation Platforms",
    "section": "",
    "text": "Trustworthy Online Controlled Experiments"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#books",
    "href": "posts/exerimentations-platforms/index.html#books",
    "title": "Experimentation Platforms",
    "section": "",
    "text": "Trustworthy Online Controlled Experiments"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#resources",
    "href": "posts/exerimentations-platforms/index.html#resources",
    "title": "Experimentation Platforms",
    "section": "Resources",
    "text": "Resources\n\nTop Challenges from the first Practical Online Controlled Experiments Summit\nA/B Testing Pitfalls: Getting Numbers You Can Trust is Hard\nUSF Business Analytics Forum - Ron Kohavi\nA/B Testing at Scale: Accelerating Software Innovation\nTrustworthy Online Controlled Experiments at Large Scale\nAlways Valid Inference: Continuous Monitoring of A/B Tests"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#companies",
    "href": "posts/exerimentations-platforms/index.html#companies",
    "title": "Experimentation Platforms",
    "section": "Companies",
    "text": "Companies\n\nNetflix\n\nNetflix Articles tagged Experimentation\nIt’s All A/Bout Testing: The Netflix Experimentation Platform\nReimagining Experimentation Analysis at Netflix\nSuccess stories from a democratized experimentation platform\nKey Challenges with Quasi Experiments at Netflix\nData Compression for Large-Scale Streaming Experimentation\nPage Simulation for Better Offline Metrics at Netflix\nStreaming Video Experimentation at Netflix: Visualizing Practical and Statistical Significance\nInnovating Faster on Personalization Algorithms at Netflix Using Interleaving\n\n\n\nMicrosoft\n\nExP Experimentation Platform Accelerating software innovation through trustworthy experimentation\nOnline Experimentation at Microsoft\nExperimentation Platform\nA/B Testing and Covid-19: Data-Driven Decisions in Times of Uncertainty\nPatterns of Trustworthy Experimentation: Pre-Experiment Stage\n\n\n\nTwitter\n\nTwitter experimentation: technical overview\n\n\n\nGoogle\n\nOverlapping Experiment Infrastructure: More, Better, Faster Experimentation\n\n\n\nFacebook\n\nPlanOut is a library and interpreter for designing online experiments.\nAdaptive Experimentation Platform\n\n\n\nSpotify\n\nSpotify’s New Experimentation Platform part 1\nSpotify’s New Experimentation Platform part 2\nLarge Scale Experimentation at Spotify\n\n\n\nTinder\n\nPhoenix — Tinder’s Testing Platform, Part — I\nPhoenix — Tinder’s Testing Platform — Part II\nPhoenix — Tinder’s Testing Platform — Part III\n\n\n\nLinkedIn\n\nOur evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn\nMaking the LinkedIn experimentation engine 20x faster\n\n\n\nUber\n\nUnder the Hood of Uber’s Experimentation Platform\nA/B testing at Uber: How we built a BYOM (bring your own metrics) platform\nBuilding an Intelligent Experimentation Platform with Uber Engineering\n\n\n\nAirBnB\n\n4 Principles for Making Experimentation Count\nScaling Airbnb’s Experimentation Platform\nExperiment Reporting Framework\n\n\n\nInstagram\n\nLessons Learned at Instagram Stories and Feed Machine Learning\n\n\n\nGo-Jek\n\nIntroducing Litmus: GOJEK’s Own Experimentation Platform\n\n\n\nInstaCart\n\nRandomized, controlled experiments and multivariate regression are used to continuously improve the grocery delivery engine\n\n\n\nPintrest\n\nBuilding Pinterest’s A/B testing platform"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#conferences",
    "href": "posts/exerimentations-platforms/index.html#conferences",
    "title": "Experimentation Platforms",
    "section": "Conferences",
    "text": "Conferences\n\nExperimentation Culture Awards"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#sass-solutions",
    "href": "posts/exerimentations-platforms/index.html#sass-solutions",
    "title": "Experimentation Platforms",
    "section": "SASS solutions",
    "text": "SASS solutions\n\nSplit.io\nOptimizely"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#when-you-cant-run-ab-tests",
    "href": "posts/exerimentations-platforms/index.html#when-you-cant-run-ab-tests",
    "title": "Experimentation Platforms",
    "section": "When you can’t run A/B Tests",
    "text": "When you can’t run A/B Tests\n\nQuasi Experimentation at Netflix\nKey Challenges with Quasi Experiments at Netflix\nMostly Harmless Econometrics: An Empiricist’s Companion"
  },
  {
    "objectID": "posts/exerimentations-platforms/index.html#notes",
    "href": "posts/exerimentations-platforms/index.html#notes",
    "title": "Experimentation Platforms",
    "section": "Notes",
    "text": "Notes\n\n2020 CODE@MIT Experimentation platforms\n\nGaussian processes\nMulti touch attributions\nHeterogenous treatment effect\nInteraction effects\nOverlapping experiments\nWhat are potential over evaluation criteria ?\nWhat are good guardrail metrics ?\nrun A/A tests.\n\nCanary Deploys by using Experimentation platform to tell when you break guard rail metrics"
  },
  {
    "objectID": "posts/infant-mortality/index.html",
    "href": "posts/infant-mortality/index.html",
    "title": "Infant Mortality In India",
    "section": "",
    "text": "I thought about this after reading Doing good better.\nSoon after I found out about kepler.gl which made this the perfect project to test it out.\nI extracted the data I needed for my analysis from a PDF of an annual summary report by the Government of India, available here, as I couldn’t find a comprehensive and user-friendly source.\nYou can read the analysis here.\nHere is the kepler.gl map I created for this analysis.\nIt contains the following layers:\n\nTotal Lives Lost: The number of infants who died in each state. The height of each region is proportional to the number of lives lost, with redder regions having a higher infant mortality rate.\nTotal Population : The population of each state. The height of each region is proportional to the population, with bluer regions having a higher birth rate.\nTotal Lives Lost Rural: The same as Total Lives Lost, but only for rural areas in each state.\nTotal Lives Lost Urban: The same as Total Lives Lost, but only for urban areas in each state.\n\nOnce the map loads you can click on the layers on the right to toggle them on and off. You can also click on the layers to see the data for each region.\nThe json file for the map is available here."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html",
    "href": "posts/exponentials-everywhere/index.html",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "",
    "text": "Today, we’re diving headfirst into the wacky world of S-curves – the Kardashians of the math world. They’re everywhere, they’re unpredictable, and they’ve got more curves than a bag of curly fries.\nAn S-curve is a mathematical tool that helps us understand systems showing exponential growth, which eventually stabilize at a fixed level. This concept can be applied to various areas of life, including:\n\nProduct adoption (from “What’s a smartphone?” to “I can’t live without it!”)\nEpidemics (yeah, we’re all experts now, aren’t we?)\nThe spread of juicy gossip in your office (admit it, we’ve all been there)\nBacteria growth (think of that forgotten sandwich in the back of your fridge)\nTechnological progress (from “The internet will never catch on” to… well, look at us now)\n\nThe S-curve starts low, then grows rapidly before leveling off. It’s like your motivation during a project: starts slow, panic-fueled middle, and then plateaus just before the deadline.\nNow, there are a couple of different questions we’re interested in:\n\nWhat’s the endgame? (N - final level): In pandemic terms, how many people will ultimately catch the sniffles? For your startup, what’s the total market size for your AI-powered nose hair trimmer?\nWhen does it get wild? (t0 - inflection point): This is the “hold my beer” moment of the curve. It’s when you go from “It’s just a flu” to “WHERE’S ALL THE TOILET PAPER?!”\nHow insane is this ride? (k - slope): A high k means your growth is so fast it’ll give you whiplash. A low k? Well, let’s just say you might want to pack a sandwich for this journey.\nAre we there yet? (time to reach N): How long until we can stop holding our breath? For a pandemic, it’s about knowing when it’s safe to emerge from your toilet paper fort. For your business, it’s knowing how long before you can trade in your ramen noodles for caviar."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#a-quick-intro-to-s-curves",
    "href": "posts/exponentials-everywhere/index.html#a-quick-intro-to-s-curves",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "",
    "text": "Today, we’re diving headfirst into the wacky world of S-curves – the Kardashians of the math world. They’re everywhere, they’re unpredictable, and they’ve got more curves than a bag of curly fries.\nAn S-curve is a mathematical tool that helps us understand systems showing exponential growth, which eventually stabilize at a fixed level. This concept can be applied to various areas of life, including:\n\nProduct adoption (from “What’s a smartphone?” to “I can’t live without it!”)\nEpidemics (yeah, we’re all experts now, aren’t we?)\nThe spread of juicy gossip in your office (admit it, we’ve all been there)\nBacteria growth (think of that forgotten sandwich in the back of your fridge)\nTechnological progress (from “The internet will never catch on” to… well, look at us now)\n\nThe S-curve starts low, then grows rapidly before leveling off. It’s like your motivation during a project: starts slow, panic-fueled middle, and then plateaus just before the deadline.\nNow, there are a couple of different questions we’re interested in:\n\nWhat’s the endgame? (N - final level): In pandemic terms, how many people will ultimately catch the sniffles? For your startup, what’s the total market size for your AI-powered nose hair trimmer?\nWhen does it get wild? (t0 - inflection point): This is the “hold my beer” moment of the curve. It’s when you go from “It’s just a flu” to “WHERE’S ALL THE TOILET PAPER?!”\nHow insane is this ride? (k - slope): A high k means your growth is so fast it’ll give you whiplash. A low k? Well, let’s just say you might want to pack a sandwich for this journey.\nAre we there yet? (time to reach N): How long until we can stop holding our breath? For a pandemic, it’s about knowing when it’s safe to emerge from your toilet paper fort. For your business, it’s knowing how long before you can trade in your ramen noodles for caviar."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#the-day-i-tried-to-outsmart-an-s-curve",
    "href": "posts/exponentials-everywhere/index.html#the-day-i-tried-to-outsmart-an-s-curve",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "The Day I Tried to Outsmart an S-curve",
    "text": "The Day I Tried to Outsmart an S-curve\nYou know those days when you think you’ve got it all figured out? Yeah, I had one of those recently. I was staring at a beautiful S-curve, thinking, “I’ve got this. I can predict where this bad boy is going.” Spoiler alert: I couldn’t. And neither can most of us, apparently."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#the-sigmoid-saga-a-tale-of-overconfidence",
    "href": "posts/exponentials-everywhere/index.html#the-sigmoid-saga-a-tale-of-overconfidence",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "The Sigmoid Saga: A Tale of Overconfidence",
    "text": "The Sigmoid Saga: A Tale of Overconfidence\nThis whole adventure started when I stumbled upon an experiment demonstrating the difficulty of fitting an S-curve on noisy data. It’s like one of those magic tricks that leaves you scratching your head, except instead of pulling rabbits out of hats, it’s pulling wildly inaccurate predictions out of seemingly innocent data.\nThe punchline? Even with just a smidgen of noise, trying to predict a sigmoid model by fitting past values is about as reliable as using a Magic 8-Ball to plan your retirement. And here’s the kicker: that maximum value, N? It can be off by orders of magnitude. It’s like guessing someone’s age and being off by centuries."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#lets-play-guess-the-curve",
    "href": "posts/exponentials-everywhere/index.html#lets-play-guess-the-curve",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "Let’s Play: “Guess the Curve”",
    "text": "Let’s Play: “Guess the Curve”\nAlright, folks. Time to put on your fortune-teller hats. Below is a little interactive playground where you can mess around with a sigmoid curve. Go ahead, tweak those knobs. Feel the power. Embrace your inner oracle.\n\n\nviewof noise = Inputs.range(\n  [0, 1],\n  {step: 0.01, value: 0.1, label: \"noise level\"}\n)\nviewof t0 = Inputs.range(\n  [0, 1],\n  {step: 0.01, value: 0.3, label: \"t0 (inflection point)\"}\n)\nviewof k = Inputs.range(\n  [0, 40],\n  {step: 0.1, value: 20, label: \"k (slope)\"}\n)\nviewof L = Inputs.range(\n  [0, 10],\n  {step: 0.1, value: 8, label: \"N (plateau)\"}\n)\nviewof replay = html`&lt;button&gt;Replay&lt;/button&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd3 = require('d3@7')\n\nsigmoid = (L, t0, k) =&gt; t =&gt; L / (1 + Math.exp(-k * (t - t0)))\n\nobjective = n =&gt; ([L, t0, k]) =&gt; {\n  const S = sigmoid(L, t0, k);\n  return d3.sum(data.slice(0, n).map(([t, v]) =&gt; (S(t) - v) ** 2));\n}\n\n\nfmin = require(\"fmin\")\n\nsolution = n =&gt; fmin.nelderMead(objective(n), [10, 1, 1])\n\ndata = {\n  replay;\n  const S = sigmoid(L, t0, k);\n  const length = 100;\n  return Array.from({ length }, (_, i) =&gt; [\n    i / length,\n    S(i / length) + noise * (Math.random() - 0.5)\n  ]);\n}\n\nwidth = 800\n\nfunction curvePoints(radius = 0.5) {\n  const tau = 2 * Math.PI;\n  return function(context) {\n    return {\n      lineStart: () =&gt; {},\n      lineEnd: () =&gt; {},\n      point: (x, y) =&gt; {\n        context.moveTo(x + radius, y);\n        context.arc(x, y, radius, 0, tau);\n      }\n    };\n  };\n}\n\n{\n  const height = 360,\n    context = DOM.context2d(width, height);\n\n  const x = d3\n      .scaleLinear()\n      .domain([0, 1])\n      .range([20, width - 20]),\n    y = d3\n      .scaleLinear()\n      .domain([0, 10])\n      .range([height - 20, 20])\n      .nice(),\n    line = d3\n      .line()\n      .context(context)\n      .x(d =&gt; x(d[0]))\n      .y(d =&gt; y(d[1]));\n\n  let maxL = 0;\n  const starti = 10;\n  const solutions = [];\n\n  function draw(currentIndex) {\n    context.clearRect(0, 0, width, height);\n    \n    // Draw all data points\n    context.beginPath();\n    line.curve(curvePoints(.5))(data);\n    \n    // Draw points up to current index\n    line.curve(curvePoints(2))(data.slice(0, currentIndex));\n    context.fillStyle = \"#000\";\n    context.fill();\n\n    if (currentIndex &gt; 0) {\n      const { x: solutionX } = solution(currentIndex);\n      solutions.push(solutionX);\n\n      // Update maxL if necessary\n      if (solutionX[0] &gt; maxL) maxL = solutionX[0];\n\n      // Draw text information\n      context.fillStyle = \"#000\";\n      context.fillText(\n        `N = ${L.toFixed(2)}; est. ${solutionX[0].toFixed(2)} (max ${maxL.toFixed(2)})`,\n        10,\n        10\n      );\n      context.fillText(`t0 = ${t0.toFixed(2)}; est. ${solutionX[1].toFixed(2)}`, 10, 24);\n      context.fillText(`k = ${k.toFixed(2)}; est. ${solutionX[2].toFixed(2)}`, 10, 38);\n\n      // Draw solution curves\n      context.globalAlpha = 0.2;\n      let solutionIndex = starti;\n      for (const x of solutions) {\n        const S = sigmoid(...x),\n          points = data.slice(solutionIndex, ++solutionIndex + 30).map(([t]) =&gt; [t, S(t)]);\n        context.beginPath();\n        line.curve(d3.curveLinear)(points);\n        context.strokeStyle = \"steelblue\";\n        context.stroke();\n      }\n      context.globalAlpha = 1;\n      context.stroke();\n\n      // Draw current point\n      context.beginPath();\n      line.curve(curvePoints(2))([data[currentIndex]]);\n      context.fillStyle = \"steelblue\";\n      context.fill();\n    }\n  }\n\n  yield context.canvas;\n\n  let stop = false;\n  invalidation.then(() =&gt; (stop = true));\n\n  draw(0);\n  await Promises.delay(1200);\n\n  for (let currentIndex = starti; currentIndex &lt; data.length; currentIndex++) {\n    if (stop) break;\n    draw(currentIndex);\n    await Promises.delay(15000 / data.length);\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, what’s the deal with this finicky curve? Let’s break it down:\n\nNoise level: This is like the static on an old TV. A little bit, and you’re squinting. Too much, and you’re watching snow.\nt0 (inflection point): The curve’s midlife crisis. Where it decides to flip from “growing like crazy” to “slowing down.”\nk (slope): How steep is this rollercoaster? Adjust this, and you go from kiddie ride to “I want my mommy!”\nN (plateau): The VIP section. How high can this party go?\n\nPlay around with these, hit that “Replay” button, and watch chaos ensue. Each faint line represents what our best guess was at that time on how the world will go. On the top left you can see the maximum L it thought over time."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#the-plot-thickens-when-noise-isnt-the-culprit",
    "href": "posts/exponentials-everywhere/index.html#the-plot-thickens-when-noise-isnt-the-culprit",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "The Plot Thickens: When Noise Isn’t the Culprit",
    "text": "The Plot Thickens: When Noise Isn’t the Culprit\nNow, here’s where things get really interesting. Set that noise level to zero. Go ahead, I’ll wait. What do you see?\nYou might expect a perfect, smooth curve with spot-on predictions, right? But nope! Even with zero noise, you’ll notice something curious:\n\nThe black dots form a perfect, smooth S-curve. No surprises there.\nThe estimated parameters (L, t0, k) match the true values exactly. So far, so good.\nBut wait, what’s with those light blue curves shooting off into the stratosphere?\n\nThat last point is the kicker. Even in our perfect, noise-free world, we’re seeing wildly different predictions for where the curve might end up. And that max L value? It’s through the roof!\nThis isn’t some glitch in the Matrix or a quirk of computer arithmetic. It’s a glimpse into a fundamental truth about these curves that’s got mathematicians scratching their heads."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#diving-deeper-the-math-on-why-were-so-bad-at-this",
    "href": "posts/exponentials-everywhere/index.html#diving-deeper-the-math-on-why-were-so-bad-at-this",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "Diving Deeper: The Math on Why We’re So Bad at This",
    "text": "Diving Deeper: The Math on Why We’re So Bad at This\nNow, if you thought I was just making this up as I go along (which, let’s be honest, is sometimes true), hold onto your hats. There’s some serious math backing up our struggles with these slippery S-curves.\nThe research paper “The Limits to Learning a Diffusion Model” 1 by Jackie Baek and friends goes deep on this, I won’t talk about the nitty gritty details here, but here is what these math wizards discovered:\n\nSize Matters, A Lot: Remember that ‘N’ parameter in our little game above? The one that kept running away from us? Well, it turns out that estimating the total population size (N) is the real troublemaker here. It’s like trying to guess how many jelly beans are in a jar, except the jar keeps changing size when you’re not looking.\nThe Two-Thirds Rule: Here’s where it gets wild. To get a decent estimate of N, you need to observe about N^(2/3) events. In English? If you’re dealing with a population of 1 million, you need to see about 10,000 events before your guess is even in the ballpark. It’s like needing to eat two-thirds of a cake to guess its flavor. By that point, why even guess?\nEarly Bird Doesn’t Get the Worm: All those early predictions? Basically fancy guesswork. The paper shows that you can’t reliably predict the eventual number of infections (or product adoptions, for you business folks) until you’re about two-thirds of the way to peak infection rate. It’s like trying to guess the ending of a movie when you’re only 20 minutes in.\nSome Things Are Easier Than Others: Not all is lost! The paper found that other parameters, like the rate of spread, are easier to estimate. It’s like being able to guess the speed of the car, but not how far it’ll go before running out of gas.\n\n\nWhat This Means for Our Little Experiment\nRemember our interactive playground above, especially with the noise set to zero? Now you know why those light blue curves are going bananas. When you’re adjusting that ‘L’ parameter, you’re not just fighting noise – you’re up against the laws of mathematics themselves. Those diverging curves are showing us all the possible futures that fit our data so far. It’s not you, it’s not the noise, it’s not even a computer glitch. It’s the fundamental nature of these curves conspiring to keep us guessing."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#why-should-you-care-besides-impressing-people-at-parties",
    "href": "posts/exponentials-everywhere/index.html#why-should-you-care-besides-impressing-people-at-parties",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "Why Should You Care? (Besides Impressing People at Parties)",
    "text": "Why Should You Care? (Besides Impressing People at Parties)\nNow, you might be thinking, “Cool trick, but so what?” Well, my friend, this little experiment has some big implications:\n\nEpidemic Modeling: Remember when everyone became an armchair epidemiologist in 2020? This is why even the pros were struggling. Early predictions in disease spread are about as reliable as a weather forecast for next year. Next time you see an early prediction about how bad a disease outbreak will be, take it with a grain of salt.\nProduct Adoption: If you’re in business, dreaming of that hockey stick growth, remember this curve. Your early numbers might be lying to you. Don’t count your exponential chickens before they hatch.\nData Addiction: In a world obsessed with data, this is a humbling reminder. Sometimes, more data is just more confusion. Quality over quantity, folks.\nPolicy Making: For the folks in charge, this is a reminder that early interventions are tricky. You’re often working with less information than you think you have.\nModel Complexity: Simple models are great… until they’re not. This shows why sometimes you need to break out the big guns (and the supercomputers). It’s like the difference between a tricycle and a mountain bike – both have their place, but you wouldn’t take a tricycle off-roading.\nUncertainty is Certain: Next time you see a prediction with pinpoint accuracy, remember this curve. The only thing certain about the future is uncertainty. The question should always be How uncertain?"
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#the-take-home-message",
    "href": "posts/exponentials-everywhere/index.html#the-take-home-message",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "The Take-Home Message",
    "text": "The Take-Home Message\nHere’s the deal: predicting the future is hard. Like, really hard. Even with fancy math, shiny tools, and zero noise, we’re often just sophisticated guessers. So what do we do?\n\nStay humble. Your model might be wrong. Heck, it’s probably wrong.\nCollect data like it’s going out of style. But remember, even a mountain of perfect data can mislead you.\nBe flexible. The future has a nasty habit of not conforming to our expectations.\nCommunicate uncertainty. Don’t just give a number. Give a range, a confidence interval, a ¯\\_(ツ)_/¯ – whatever conveys “This is our best guess, but…”\nRemember the two-thirds rule. If you’re not at least that far into your S-curve journey, take your predictions with a hefty dose of skepticism."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#in-conclusion-the-beauty-of-unpredictable-s-curves",
    "href": "posts/exponentials-everywhere/index.html#in-conclusion-the-beauty-of-unpredictable-s-curves",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "In Conclusion: The Beauty of Unpredictable S-Curves",
    "text": "In Conclusion: The Beauty of Unpredictable S-Curves\nSo there you have it, folks. S-curves are trickier than a magician’s rabbit. The next time someone hands you a long-term prediction with pinpoint accuracy, feel free to raise an eyebrow (or two, and tilt your head a little).\nAs Yogi Berra wisely said, “It’s tough to make predictions, especially about the future.” Turns out, even our fancy math agrees with baseball wisdom.\nBut here’s the kicker: there’s beauty in this unpredictability. These slippery sigmoids keep scientists on their toes and entrepreneurs on the edge of their seats.\nSo whether you’re modeling pandemics or predicting startup growth, embrace the challenge. As statistician George Box put it, “All models are wrong, but some are useful.” Our S-curves, quirks and all, showcase our relentless quest to understand an uncooperative universe.\nHere’s to the curves that humble us, the data that surprises us, and the inflection points we’re all trying to pinpoint. May your predictions be insightful, your error bars honest, and your sense of wonder endless.\nIn the grand sigmoid of life, we’re all still climbing."
  },
  {
    "objectID": "posts/exponentials-everywhere/index.html#footnotes",
    "href": "posts/exponentials-everywhere/index.html#footnotes",
    "title": "Exponentials Everywhere: The S-Curve Challenge in Predicting the Future",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRead the paper or listen to the podcast↩︎"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html",
    "href": "posts/game-of-thrones-quiz/index.html",
    "title": "Game Of Thrones quiz",
    "section": "",
    "text": "Game Of Thrones Quiz\nThis quiz was created as part of cultural week, right as Game of Thrones season 7 ended.\nIt is broken into 3 seperate rounds"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html#first-round",
    "href": "posts/game-of-thrones-quiz/index.html#first-round",
    "title": "Game Of Thrones quiz",
    "section": "First round",
    "text": "First round\nQuestions\nAnswers"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html#second-round",
    "href": "posts/game-of-thrones-quiz/index.html#second-round",
    "title": "Game Of Thrones quiz",
    "section": "Second round",
    "text": "Second round\nSlides\nvideo for question 1\nmusic for question 2\nvideo for question 3"
  },
  {
    "objectID": "posts/game-of-thrones-quiz/index.html#third-round",
    "href": "posts/game-of-thrones-quiz/index.html#third-round",
    "title": "Game Of Thrones quiz",
    "section": "Third round",
    "text": "Third round\nConnect"
  },
  {
    "objectID": "posts/newbie-quiz/index.html",
    "href": "posts/newbie-quiz/index.html",
    "title": "Newbie quiz",
    "section": "",
    "text": "Newbie Quiz\n\n\nIndraneel and I had prepared this quiz as a way of introducting quizzing to new people and narrowing down potential recruits. The quiz covers a broad range of topics, from pop culture, sports, technology, politics\nLots of fun!\n\nQuestions\nGet the questions.\n\n\nAnswers\nGet the answers."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I believe we are still in the early stages of algorithmic progress in deep learning systems. We have been able to make useful models by scaling up, we are yet to discover far more compute and data-efficient approaches. We are in the early phase of “Simplicity does not precede complexity, but follows it” cycle, and we will soon see a wave of simpler more cost-effective and performant models.\nMy current interests lie in the inference and training of large language models on smaller machines, mechanisitic-interpretability, and in-context learning.\nI’m also fascinated by marketplaces and mechanism design, influenced by books like “Dataclysm” by Christian Rudder and “Who Gets What–And Why” by Alvin E. Roth I am interested in how we can leverage ML to create better marketplaces (reputation systems, matching algorithms, spam detection, pricing algorithms)."
  },
  {
    "objectID": "about.html#woven-by-toyota",
    "href": "about.html#woven-by-toyota",
    "title": "About",
    "section": "Woven By Toyota",
    "text": "Woven By Toyota\nAt Woven, I lead multiple projects in machine learning and infrastructure for things like adaptive assessments, and course recommendation, talent matching systems, as one of the founding engineers of MS1, a startup focused on talent mobility."
  },
  {
    "objectID": "about.html#bookmyshow",
    "href": "about.html#bookmyshow",
    "title": "About",
    "section": "BookMyShow",
    "text": "BookMyShow\nPreviously, at BookMyShow, I worked on the development of real-time data pipelines for discovery and personalization systems, helping millions of users find entertainment experiences."
  },
  {
    "objectID": "posts/harry-potter-quiz/index.html",
    "href": "posts/harry-potter-quiz/index.html",
    "title": "Harry Potter quiz",
    "section": "",
    "text": "Pottermania 2018\n\n\nThe goal of the is to be something easy that anybody with little experience in quizzing can take part in. created with Bitan, and Keyur as part of Quiz club.\n\nQuestions\nGet the questions.\nGet the video for question number 14\nGet the Music for question number 20\n\n\nAnswers\nGet the answers."
  },
  {
    "objectID": "posts/friend-or-foe/index.html",
    "href": "posts/friend-or-foe/index.html",
    "title": "Friend Or Foe",
    "section": "",
    "text": "Co-opeartive and Adversarial Environments\n\n\nThis is a mini interactive “game”, I made inspired by the paper, without giving too much away, I urge you to play.\nAI Safety Grid Worlds,"
  },
  {
    "objectID": "posts/state-of-reading/index.html",
    "href": "posts/state-of-reading/index.html",
    "title": "The state of reading in 2018 and beyond.",
    "section": "",
    "text": "Audio books are seeing a resurgence, because of companies like Audible.\nText to speech is good, and is getting better .\nWireless Bluetooth earphones are becoming common place, the increase in convenience and battery life allows people to have them on longer.\nUnlike content that you have to watch/read, you can work on other stuff while you listen.\nI see a time where we can listen to most of the things we read.\nCreates a whole new kind of medium.\nNews, blog posts and Stories most affected.\nMost of the speech generated by machines.\nMost of the classic texts available for free on the internet."
  },
  {
    "objectID": "posts/about-this-website/index.html",
    "href": "posts/about-this-website/index.html",
    "title": "About this website",
    "section": "",
    "text": "This post is a bit of a meta post about arjunsriva.com\nA lot of my thinking on personal websites has been influenced by other great websites like Gwern’s, please read this if you’re interested in it.\nMy goals for this websites are:\n\nTo share things I learned that I personally found useful\nTo provide a playground for me to flesh out rough ideas and speculate.\nTo help me increase the clarity of my thinking by the act of writing something out.\n\non the implementation side my goals are\n\nMake it easy for me to never lose data\n\nmost writing is stored as simple text files, version controlled by git\n\nMake it easy to mix code and prose to explain certain concepts\n\nSome things that I plan on implementing later as I write more are:\n\nAutomatic link archiving to prevent link rot\nConfidence tags to show how certain I am of different things\nBetter sections / tagging to differentiate different kinds of posts, eg. reading list and notes"
  },
  {
    "objectID": "posts/course-schedule-generator/index.html",
    "href": "posts/course-schedule-generator/index.html",
    "title": "Course Schedule Generator",
    "section": "",
    "text": "A website which allows students of IIT Indore to add courses they are interested in to their calendar.\nI was tired of doing this manually every time.\nUse it here"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Thoughts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nOn SaaS and Swimming: Hidden Factors of Success\n\n\n\n\n\n\nBusiness\n\n\n\nHow to figure out what matters in business and sports\n\n\n\n\n\nAug 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExponentials Everywhere: The S-Curve Challenge in Predicting the Future\n\n\n\n\n\n\nForecasting\n\n\nMath\n\n\n\nS-curves are way trickier than you’d think. Here’s why predicting pandemics, product adoption, and other exponential phenomena is far more challenging than it seems – and what that means for all of us.\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentation Platforms\n\n\n\n\n\n\nExperimentation\n\n\n\nAn Overview of Experimentation Platforms.\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Schedule Generator\n\n\n\n\n\n\nIIT Indore\n\n\n\nA website which adds courses to your personal calendar, for students of IIT Indore.\n\n\n\n\n\nJan 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHarry Potter quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA beginner friendly quiz on the wizarding world\n\n\n\n\n\nAug 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nThe state of reading in 2018 and beyond.\n\n\n\n\n\n\nFuture\n\n\n\n\n\n\n\n\n\nAug 5, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nFriend Or Foe\n\n\n\n\n\n\nAI\n\n\nProjects\n\n\n\nAn interactive game inspired by the paper “AI Safety Grid Worlds”\n\n\n\n\n\nJul 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nInfant Mortality In India\n\n\n\n\n\n\nAnalysis\n\n\n\nAn analysis of infant mortality rates across India.\n\n\n\n\n\nJun 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nJargonizer\n\n\n\n\n\n\nProjects\n\n\n\nGenerate sentences that nobody can read.\n\n\n\n\n\nJun 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMulti Armed Bandits\n\n\n\n\n\n\nAI\n\n\n\nor how to balance exploration and exploitation more formally\n\n\n\n\n\nNov 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Machine Learning\n\n\n\n\n\n\nAI\n\n\n\n\n\n\n\n\n\nNov 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGame Of Thrones quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz made for cultural week\n\n\n\n\n\nSep 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nNewbie quiz\n\n\n\n\n\n\nIIT Indore\n\n\nQuiz\n\n\n\nA quiz to introduce quizzing to new people and narrowing down potential recruits.\n\n\n\n\n\nAug 20, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMental Health Checklists\n\n\n\n\n\n\nProjects\n\n\n\nA website which allows quick tracking of mental health.\n\n\n\n\n\nJan 8, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nAbout this website\n\n\n\n\n\n\nMeta\n\n\n\na meta post about this website\n\n\n\n\n\nJan 1, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html",
    "title": "Multi Armed Bandits",
    "section": "",
    "text": "Imagine you are at a casino, and you have N slot machines to play, each slot machine gives rewards according to a fixed probability distribution. What strategy should you play with to maximise your total reward ?\nThis problem is known as Multi Armed Bandit problem.\n\n# Importing numpy for math, and matplotlib for plots\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#problem",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#problem",
    "title": "Multi Armed Bandits",
    "section": "",
    "text": "Imagine you are at a casino, and you have N slot machines to play, each slot machine gives rewards according to a fixed probability distribution. What strategy should you play with to maximise your total reward ?\nThis problem is known as Multi Armed Bandit problem.\n\n# Importing numpy for math, and matplotlib for plots\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#arms",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#arms",
    "title": "Multi Armed Bandits",
    "section": "Arms",
    "text": "Arms\nAn arm when pulled, gives a random number from a normal distribution with fixed mean(mu) and deviation(sigma). When pulled many times the frequency of the rewards look like this:\n X axis is the magnitude of reward\nY axis is it’s frequency.\nThe Arm class provides an arm with these properties.\n\nclass Arm:\n\n    def __init__(self, mu=None, sigma=None):\n        if mu is None:\n            self.mu = np.absolute(np.random.uniform())\n        else:\n            self.mu = mu\n        \n        \n        if sigma is None:\n            self.sigma=np.absolute(np.random.uniform())\n        else:\n            self.sigma = sigma\n\n\n    def pull(self):\n        reward = np.random.normal(self.mu, self.sigma, 1)\n        return reward\n\n\ndef get_arms(k):\n    # returns a list of arms\n    arms = []\n    for i in range(k):\n        arms.append(Arm())\n    return arms"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#agents",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#agents",
    "title": "Multi Armed Bandits",
    "section": "Agents",
    "text": "Agents\nAn agent here is a player who pulls arms to play. It has a policy, which is a list of probabilities associated with each arm.\nThe agent class makes designing agents fast. The object is initialised with arms and whether it should play all arms once as part of the initialisation.\nFeatures provided by this class:\nAttributes: * expectations[i]: gives the expected reward on playing arm[i] * times_played[i]: gives the number of times the agent has played arm[i] * N = Total number of times agent has played * reward_history : list of rewards earned by the agent * choice_history : list of choices made by the agent\nMethods: * gamble(i): Plays for i iterations while updating it’s policy. * play(i): Pulls arm[i] and updates reward_history, N , times_played * select_arm(): returns index of an arm by sampling probability distribution given by the policy\n\nclass agent:\n    def __init__(self, arms, play_once=1):\n        self.expectations = np.zeros(len(arms))\n        self.times_played = np.zeros(len(arms))\n        self.arms = arms\n\n        self.number_of_arms = len(arms)\n        self.N = 0\n\n        self.reward_history = []\n        self.choice_history = []\n\n        if play_once == 1:\n            for i in range(self.number_of_arms):\n                self.expectations[i] = self.play(i)\n\n    def play(self, index):\n        reward = self.arms[index].pull()\n\n        self.times_played[index] += 1\n        self.N += 1\n\n        self.choice_history.append(index)\n        self.reward_history.append(reward)\n\n        return reward\n\n    def policy(self):\n        pass\n\n    def update_expectations(self, reward, index):\n        self.expectations[index] += (reward - self.expectations[index])/self.N\n\n    def select_arm(self):\n        options = range(self.number_of_arms)\n        i = np.random.choice(options, p=self.policy(), replace=False)\n        return i\n\n    def gamble(self, iterations):\n        for i in range(iterations):\n            index = self.select_arm()\n            reward = self.play(index)\n            self.update_expectations(reward, index)\n\n\nExample agents\nTo make a new agent we inherit the agent class.\nTime to make some agents!\n\n\nFirst up: epsilon-greedy\nThis agent plays the arm with the highest expected reward with 1 - epsilon probability, and plays a random arm with epsilon probability\nSo\nepsilon = 1 =&gt; random choices\nepsilon = 0 =&gt; greedy choices\n\nclass epsilon_greedy(agent):\n\n    def __init__(self, arms, play_once=1, epsilon=0.1):\n        super().__init__(arms, play_once)\n        self.epsilon = epsilon\n        \n    def __str__(self):\n        return \"Epsilon-Greedy Agent, epsilon= \"+str(self.epsilon)\n    \n    def policy(self):\n        temp = np.zeros_like(self.expectations)\n        temp[np.argmax(self.expectations)] = 1-self.epsilon\n        ans = temp + self.epsilon/self.number_of_arms\n        return ans\n\n\n\nBeta-Softmax\nThis agent plays an arm[i] with probability proportional to: e^(expected_reward(arm[i])/beta)\nWe normalise the whole thing by the sum over all the arms.\n\nclass softmax(agent):\n\n    def __init__(self, arms, play_once=1, beta=1):\n        super().__init__(arms, play_once)\n        self.beta = beta\n        \n    def __str__(self):\n        return \"Softmax agent, beta= \"+ str(self.beta)\n\n    def policy(self):\n        temp = np.exp(self.expectations/self.beta)\n        ans = temp / np.sum(temp, axis=0)\n        return ans\n\n\n\nUpper Confidence Bound (UCB1)\nUCB1 agent plays the arm with the highest metric, where metric of arm i is : metric[i] = expected_reward[i] + sqrt(2*log(N)/times_played[i])\nNote Best peformance when rewards are between 0 and 1\n\nclass ucb(agent):\n\n    def __init__(self, arms, play_once=1):\n        super().__init__(arms, play_once)\n\n    def __str__(self):\n        return \"UCB1 agent\"\n    \n    def policy(self):\n        temp = self.expectations + np.sqrt(2*np.log(self.N)/self.times_played)\n        ans = np.zeros_like(temp)\n        ans[np.argmax(temp)] = 1\n        return ans"
  },
  {
    "objectID": "posts/multi-armed-bandits/multi-armed-bandits.html#metrics",
    "href": "posts/multi-armed-bandits/multi-armed-bandits.html#metrics",
    "title": "Multi Armed Bandits",
    "section": "Metrics",
    "text": "Metrics\nMetric : A scalar number, makes comparison easier.\nTo compare the performance of our agents we can use these metrics\n\navg_reward[i] : this gives the average reward till i+1 iteration.\nmax_reward : this tells us the maximum expected reward\neuclid_distance : we can think of as learnt policy and optimal policy as vectors and compute the distance between them , smaller is better\ncosine_simmilarity : compute the cos(q) between the policies. larger is better\n\n\ndef maxreward(arms):\n    #Max rewards\n    a= [arm.mu for arm in arms]\n    return max(a)\n\ndef avg_reward(rewards):\n    ans = []\n    ans.append(rewards[0])\n    for i in range(1,len(rewards)):\n        ans.append(ans[i-1]+rewards[i])\n    for i in range(len(ans)):\n        ans[i]/=i+1\n    return ans\n\ndef cosine_similarity(a,b):\n    temp = a*b\n    temp/=(euclid_distance(a)* euclid_distance(b))\n    return np.sum(temp, axis=0)\n    \ndef euclid_distance(a):\n    return np.sqrt(np.sum(a*a, axis=0))\n\n\nTest\nThis function takes a list of agents and the number of iterations. Makes each agent play, and prints its metrics.\n\ndef test(agents, iterations):\n    for agent in agents:\n        \n        agent.gamble(iterations)\n        \n        temp = [ arm.mu for arm in levers] \n        optimal_policy = np.zeros_like(agent.expectations)\n        optimal_policy[temp.index(max(temp))] = 1\n        \n        avg_rewards_earned = avg_reward(agent.reward_history)\n        \n        print(agent)\n        print(\"maximum possible reward:\", maxreward(levers))\n        print(\"average reward:\", avg_rewards_earned[-1])\n        print(\"cosine similarity\" ,cosine_similarity(agent.policy(), optimal_policy))\n        euclid_norm = euclid_distance(agent.policy()-optimal_policy)/len(optimal_policy)\n        print(\"euclidian norm \",euclid_norm)\n        \n        \n        plt.plot(avg_rewards_earned)\n        plt.ylabel('Average Reward')\n        plt.xlabel('Iteration')\n        plt.show()\n        print(\"\\n\")\n    \n        # print(\"optimal policy:\" , optimal)\n        # print(\"learnt policy:\" ,agent.policy())\n        \n    \n        \n        # plt.scatter(range(len(agent.choice_history)),y=agent.choice_history)\n        # plt.title(\"Choices\")\n        # plt.xlabel(\"time\")\n        # plt.ylabel(\"arm\")\n        # plt.show()\n        # print(\"\\n\")\n    \n    \n\n\nlevers = get_arms(10)\n\nagents = [\n    epsilon_greedy(levers, epsilon=1),\n    epsilon_greedy(levers, epsilon=0),\n    softmax(levers, beta=0.1),\n    ucb(levers)\n\n]\n\n\nplt.plot([ arm.mu for arm in levers] )\nplt.title(\"distribution of expected value of arms\")\n\nText(0.5, 1.0, 'distribution of expected value of arms')\n\n\n\n\n\n\n\n\n\n\ntest(agents, 5000)\n\nEpsilon-Greedy Agent, epsilon= 1\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.47962497]\ncosine similarity 0.3162277660168379\neuclidian norm  0.09486832980505139\n\n\n\n\n\n\n\n\n\n\n\nEpsilon-Greedy Agent, epsilon= 0\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.98686237]\ncosine similarity 1.0\neuclidian norm  0.0\n\n\n\n\n\n\n\n\n\n\n\nSoftmax agent, beta= 0.1\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.91348264]\ncosine similarity 0.9992727823574249\neuclidian norm  0.008915931500017809\n\n\n\n\n\n\n\n\n\n\n\nUCB1 agent\nmaximum possible reward: 0.9851042878107023\naverage reward: [0.89258379]\ncosine similarity 0.0\neuclidian norm  0.1414213562373095\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimental stuff:\nBelow are a few agents I wrote for fun.\n\n\nclass softmax_with_exponentiation(agent):\n\n    def __init__(self, arms, play_once=1, beta=1, exp=1):\n        super().__init__(arms, play_once)\n        self.beta = beta\n        self.exp = exp\n\n    def policy(self):\n        temp = np.exp(self.expectations/self.beta)\n        ans = temp / np.sum(temp, axis=0)\n        ans = ans**self.exp\n        ans /= np.sum(ans, axis=0)\n        return ans\n\n\nclass softmax_with_reccurence(agent):\n\n    def __init__(self, arms, play_once=1, beta=1):\n        super().__init__(arms, play_once)\n        self.old_policy = np.ones_like(self.expectations)/self.l\n        self.beta = beta\n\n    def policy(self):\n        temp = np.exp(self.expectations/self.beta)\n        new_policy = temp / np.sum(temp, axis=0)\n\n        result = np.multiply(new_policy, self.old_policy)\n        result /= np.sum(result, axis=0)\n        self.old_policy = result\n\n        return result\n\n\nclass greedy_with_reccurence(agent):\n    # alpha = number &lt; 1; will sum over a number of observations and will keep\n    # osiclating.\n    # alpha = N will allow the algo to converge to an arm, greedy doesn't\n    # really need this, kind of always give one answer.\n\n    def __init__(self, arms, play_once=1, alpha=1):\n        super().__init__(arms, play_once)\n        self.old_policy = np.ones_like(self.expectations)\n        self.alpha = alpha\n\n    def policy(self):\n        new_policy = np.zeros_like(self.expectations)\n        new_policy[np.argmax(self.expectations)] = 1\n\n        new_policy = (1-self.alpha)*new_policy + self.alpha*self.old_policy\n\n        new_policy /= np.sum(new_policy, axis=0)\n        self.old_policy = new_policy\n\n        return new_policy\n\n# class magic(agent):\n#    def __init__(self, arms, play_once=1, exp=1):\n#        super().__init__(arms, play_once)\n#        self.old_policy = np.ones_like(self.expectations)/self.l\n#        self.exp = exp\n#\n#    def policy(self):\n#        new_policy = f(old_policy, g(expectations))"
  },
  {
    "objectID": "posts/learning-machine-learning/index.html",
    "href": "posts/learning-machine-learning/index.html",
    "title": "Learning Machine Learning",
    "section": "",
    "text": "This post will talk about resources for how I’m going about learning machine learning.\n\nBooks\n\n“Hands-On Machine Learning with Scikit-Learn and TensorFlow” by Aurélien Géron\n“Deep Learning” by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\n“The Elements of Statistical Learning” by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\n\n\n\nBlogs\n\nDistill.pub: Clear, interactive explanations of machine learning concepts\nSebastian Ruder’s blog: In-depth articles on NLP and deep learning\nAndrej Karpathy’s blog: Excellent posts on deep learning\n\n\n\nPodcasts\nThese podcasts are amazing,and what got me interested in the first place. Get a podcast app, I love podcast addict (android). Some awesome podcasts:\n\nPartially Derivative\nLinear Digressions\nData Skeptic\n\nSome that are supposed to be good but never tried:\n\nNot so standard deviations\nData science at home\nTalking machines\n\n\n\nOnline Courses\nA few awesome courses.\n\nAndrew Ng Coursera\nA good first course, which teaches you bottom up, from basics to advanced techniques. Matlab/Octave.\nFast.ai\nA course which aims to teach by coding, and takes a top down approach.\nCS231n: Convolutional Neural Networks for Visual Recognition While focused on computer vision, this Stanford course provides an excellent introduction to deep learning concepts.\n\n\n\nStaying Up-to-Date\nOne of the best things about the machine learning field is how much work happens in the open.\nMany researchers publish their work on arXiv months or even years before it appears in journals or at conferences. Following key researchers and institutions on Twitter is an excellent way to stay informed about the latest developments as they happen.\nSome great accounts to follow: @goodfellow_ian, @ylecun, @karpathy, @gwern.\nDon’t be afraid of reading arXiv papers, they might seem intimidating in the beginning but they get easier over time."
  },
  {
    "objectID": "podcast/index.html",
    "href": "podcast/index.html",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "",
    "text": "I love reading research papers, but often find myself with pockets of time when I can’t sit down and read. This podcast bridges that gap, offering bite-sized explorations of individual papers."
  },
  {
    "objectID": "podcast/index.html#latest-episodes",
    "href": "podcast/index.html#latest-episodes",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "Latest Episodes",
    "text": "Latest Episodes\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNative Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention\n\n\n\n\n\n\nArtificial Intelligence\n\n\nSparse Attention\n\n\nLong-Context Modeling\n\n\nTransformer Models\n\n\nTraining Efficiency\n\n\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\nStreaming DiLoCo: Efficient Distributed Training of Large Language Models\n\n\n\n\n\n\nDistributed Training\n\n\nLarge Language Models\n\n\nMachine Learning\n\n\nCommunication Efficiency\n\n\nGradient Compression\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\nEfficiently Scaling Transformer Inference\n\n\n\n\n\n\nNatural Language Processing\n\n\nMachine Learning\n\n\nDistributed Computing\n\n\nModel Deployment\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\nTülu 3: Pushing Frontiers in Open Language Model Post-Training\n\n\n\n\n\n\nArtificial Intelligence\n\n\nLanguage Models\n\n\nOpen Source\n\n\nReinforcement Learning\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\nBytedance: UI-TARS: End-to-End Model for Automated GUI Interaction\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine Learning\n\n\nHuman-Computer Interaction\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n\n\n\n\n\n\nArtificial Intelligence\n\n\nReinforcement Learning\n\n\nLanguage Models\n\n\nReasoning\n\n\nSupervised Fine-Tuning\n\n\nDistillation\n\n\n\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\nDeepSeek-V3: Advancements in Open-Source Large Language Models\n\n\n\n\n\n\nDeep Learning\n\n\nNatural Language Processing\n\n\nNeural Networks\n\n\nMachine Learning\n\n\n\n\n\n\nJan 19, 2025\n\n\n\n\n\n\n\nTitans: Learning to Memorize at Test Time\n\n\n\n\n\n\nMachine Learning\n\n\nArtificial Intelligence\n\n\nNeural Networks\n\n\nMemory Modules\n\n\n\n\n\n\nJan 18, 2025\n\n\n\n\n\n\n\nTransformer2: Self-Adaptive Large Language Models\n\n\n\n\n\n\nArtificial Intelligence\n\n\nNatural Language Processing\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nAdaptive Systems\n\n\n\n\n\n\nJan 18, 2025\n\n\n\n\n\n\n\nLearning to Learn Optimization Algorithms with LSTM Networks\n\n\n\n\n\n\nMachine Learning\n\n\nMeta-Learning\n\n\nOptimization Algorithms\n\n\nRecurrent Neural Networks\n\n\n\n\n\n\nJan 18, 2025\n\n\n\n\n\n\n\nTrust Region Policy Optimization\n\n\n\n\n\n\nReinforcement Learning\n\n\nPolicy Optimization\n\n\nTrust Region Methods\n\n\nArtificial Intelligence\n\n\n\n\n\n\nJan 18, 2025\n\n\n\n\n\n\n\nEfficient Deep Learning Parallelization using SOAP Search Space and FlexFlow Framework\n\n\n\n\n\n\nDeep Learning\n\n\nParallelization\n\n\nDistributed Computing\n\n\nNeural Networks\n\n\nOptimization\n\n\n\n\n\n\nAug 31, 2024\n\n\n\n\n\n\n\nDeep Retrieval: Learning Efficient Structures for Large-Scale Recommendation Systems\n\n\n\n\n\n\nMachine Learning\n\n\nRecommendation Systems\n\n\nInformation Retrieval\n\n\nDeep Learning\n\n\n\n\n\n\nAug 31, 2024\n\n\n\n\n\n\n\nScaling User Modeling for Personalized Advertising at Meta\n\n\n\n\n\n\nPersonalized Advertising\n\n\nUser Modeling\n\n\nDeep Learning\n\n\nNeural Networks\n\n\n\n\n\n\nAug 31, 2024\n\n\n\n\n\n\n\nLiNR: Revolutionizing Large-Scale Retrieval for Recommendation Systems\n\n\n\n\n\n\nMachine Learning\n\n\nInformation Retrieval\n\n\nRecommender Systems\n\n\nDeep Learning\n\n\nGPU-based Systems\n\n\n\n\n\n\nAug 31, 2024\n\n\n\n\n\n\n\nComprehensive Guide to Real-Time Bidding (RTB): Challenges and Opportunities\n\n\n\n\n\n\nOnline Advertising\n\n\nReal-Time Bidding\n\n\nDigital Auctions\n\n\nUser Response Prediction\n\n\nBidding Strategies\n\n\nDynamic Pricing\n\n\nAd Fraud Detection\n\n\n\n\n\n\nAug 31, 2024\n\n\n\n\n\n\n\nEfficient Inference for Large Language Models with LLM.int8()\n\n\n\n\n\n\nArtificial Intelligence\n\n\nNatural Language Processing\n\n\n8-bit Quantization\n\n\nTransformer Models\n\n\n\n\n\n\nAug 14, 2024\n\n\n\n\n\n\n\nEnhancing Language Models with a Massive Datastore\n\n\n\n\n\n\nArtificial Intelligence\n\n\nLanguage Models\n\n\nData Retrieval\n\n\nNatural Language Processing\n\n\n\n\n\n\nAug 14, 2024\n\n\n\n\n\n\n\nIn-Context Policy Iteration: Enhancing Reinforcement Learning with Large Language Models\n\n\n\n\n\n\nReinforcement Learning\n\n\nLarge Language Models\n\n\nAI\n\n\nPolicy Iteration\n\n\n\n\n\n\nAug 14, 2024\n\n\n\n\n\n\n\nOptimizing Quantization of Large Language Models for Efficiency and Accuracy\n\n\n\n\n\n\nMachine Learning\n\n\nNatural Language Processing\n\n\nQuantization\n\n\nEfficiency\n\n\nModel Compression\n\n\n\n\n\n\nAug 12, 2024\n\n\n\n\n\n\n\nAutoPruner: End-to-End Trainable Filter Pruning for Efficient Deep Neural Networks\n\n\n\n\n\n\nDeep Learning\n\n\nNeural Networks\n\n\nModel Compression\n\n\n\n\n\n\nAug 11, 2024\n\n\n\n\n\n\n\nSparseGPT: One-shot Pruning of Large Language Models\n\n\n\n\n\n\nArtificial Intelligence\n\n\nNatural Language Processing\n\n\nModel Compression\n\n\n\n\n\n\nAug 11, 2024\n\n\n\n\n\n\n\nEfficient Compression of Large Language Models using LLM-Pruner\n\n\n\n\n\n\nArtificial Intelligence\n\n\nNatural Language Processing\n\n\nModel Compression\n\n\n\n\n\n\nAug 11, 2024\n\n\n\n\n\n\n\nScreenAgent: A Vision Language Model-driven Computer Control Agent\n\n\n\n\n\n\nArtificial Intelligence\n\n\nComputer Vision\n\n\nNatural Language Processing\n\n\nArtificial GUI Interaction\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\nSupervised Pretraining for In-Context Reinforcement Learning with Transformers\n\n\n\n\n\n\nReinforcement Learning\n\n\nTransformers\n\n\nMeta-Learning\n\n\nDeep Neural Networks\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\nDecision-Pretrained Transformer: Bridging Supervised Learning and Reinforcement Learning\n\n\n\n\n\n\nReinforcement Learning\n\n\nTransformer Models\n\n\nDecision-Making\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\nHow Transformers Learn In-Context Beyond Simple Functions\n\n\n\n\n\n\nArtificial Intelligence\n\n\nDeep Learning\n\n\nTransformers\n\n\nIn-Context Learning\n\n\nRepresentation Learning\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\nIn-Context Learning Capabilities of Transformers\n\n\n\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nTransformer Models\n\n\nIn-Context Learning\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\nSpider2-V: Automated Multimodal Agents for Data Science Workflows\n\n\n\n\n\n\nArtificial Intelligence\n\n\nArtificial GUI Interaction\n\n\nData Science\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\nGeneralization Patterns of Transformers in In-Weights Learning and In-Context Learning\n\n\n\n\n\n\nArtificial Intelligence\n\n\nDeep Learning\n\n\nMachine Learning\n\n\n\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\nUnmasking the Lottery Ticket Hypothesis\n\n\n\n\n\n\nDeep Learning\n\n\nNeural Networks\n\n\nNetwork Pruning\n\n\nMachine Learning\n\n\n\n\n\n\nAug 9, 2024\n\n\n\n\n\n\n\nRethinking Scale for In-Context Learning in Large Language Models\n\n\n\n\n\n\nNatural Language Processing\n\n\nLarge Language Models\n\n\nTransformer Architecture\n\n\nIn-Context Learning\n\n\nModel Pruning\n\n\n\n\n\n\nAug 9, 2024\n\n\n\n\n\n\n\nFerret-UI: Multimodal Large Language Model for Mobile User Interface Understanding\n\n\n\n\n\n\nArtificial Intelligence\n\n\nArtificial GUI Interaction\n\n\nMobile Applications\n\n\n\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\nGrounded SAM: A Novel Approach to Open-Set Segmentation\n\n\n\n\n\n\nComputer Vision\n\n\nOpen-World Visual Perception\n\n\nSegmentation Models\n\n\n\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\nSAM 2: Segment Anything in Images and Videos\n\n\n\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nVideo Segmentation\n\n\nSAM 2\n\n\nVisual Perception\n\n\n\n\n\n\nAug 6, 2024\n\n\n\n\n\n\n\nRL^2: Fast Reinforcement Learning via Slow Reinforcement Learning\n\n\n\n\n\n\nArtificial Intelligence\n\n\nReinforcement Learning\n\n\nDeep Learning\n\n\n\n\n\n\nAug 5, 2024\n\n\n\n\n\n\n\nEvolutionary Optimization of Model Merging Recipes\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine Learning\n\n\nNatural Language Processing\n\n\n\n\n\n\nAug 5, 2024\n\n\n\n\n\n\n\nExploring Weight Agnostic Neural Networks\n\n\n\n\n\n\nDeep Learning\n\n\nNeural Networks\n\n\nEvolutionary Algorithms\n\n\n\n\n\n\nAug 5, 2024\n\n\n\n\n\n\n\nSpeculative Execution for Efficient Inference in Large Language Models on Consumer Devices\n\n\n\n\n\n\nArtificial Intelligence\n\n\nLarge Language Models\n\n\nSystems and Performance\n\n\n\n\n\n\nAug 5, 2024\n\n\n\n\n\n\n\nIn-context Learning and Induction Heads\n\n\n\n\n\n\nNatural Language Processing\n\n\nDeep Learning\n\n\nExplainable AI\n\n\nAI Safety\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nOn the Measure of Intelligence\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine Learning\n\n\nExplainable AI\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nGeometric Properties of Data Representations in Deep Neural Networks\n\n\n\n\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nExplainable AI\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nThe Case for Learned Index Structures\n\n\n\n\n\n\nMachine Learning\n\n\nSystems and Performance\n\n\nAI for Science\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\n\n\n\n\n\n\n3D Vision\n\n\nComputer Vision\n\n\nDeep Learning\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nConstitutional AI: Harmlessness from AI Feedback\n\n\n\n\n\n\nAI Safety\n\n\nMachine Learning\n\n\nArtificial Intelligence\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nProximal Policy Optimization Algorithms\n\n\n\n\n\n\nReinforcement Learning\n\n\nOptimization\n\n\nMachine Learning\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nGraph Isomorphism Networks: A Theoretical Framework and Architecture\n\n\n\n\n\n\nGraph Neural Networks\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nRethinking the Value of Network Pruning\n\n\n\n\n\n\nDeep Learning\n\n\nOptimization\n\n\nSystems and Performance\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\n\n\n\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nOptimization\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nAdding Conditional Control to Text-to-Image Diffusion Models\n\n\n\n\n\n\nGenerative Models\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nMultimodal AI\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nDenoising Diffusion Probabilistic Models\n\n\n\n\n\n\nGenerative Models\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nPractical Research Problems in AI Safety\n\n\n\n\n\n\nAI Safety\n\n\nMachine Learning\n\n\nArtificial Intelligence\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nSegment Anything: A Paradigm Shift in Image Segmentation\n\n\n\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nMachine Learning\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nLearning Transferable Visual Models From Natural Language Supervision\n\n\n\n\n\n\nComputer Vision\n\n\nNatural Language Processing\n\n\nMultimodal AI\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nLanguage Models are Few-Shot Learners\n\n\n\n\n\n\nNatural Language Processing\n\n\nFew-Shot/Meta-Learning\n\n\nDeep Learning\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nTraining Deep Reinforcement Learning Systems with Human Preferences\n\n\n\n\n\n\nReinforcement Learning\n\n\nDeep Learning\n\n\nAI Safety\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nPlaying Atari with Deep Reinforcement Learning\n\n\n\n\n\n\nDeep Learning\n\n\nReinforcement Learning\n\n\nArtificial Intelligence\n\n\n\n\n\n\nAug 2, 2024\n\n\n\n\n\n\n\nSingle Path One-Shot (SPOS): Efficient Neural Architecture Search with Simplified Supernet\n\n\n\n\n\n\nDeep Learning\n\n\nOptimization\n\n\nMachine Learning\n\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\nLong-CLIP: Extending Text Length for Improved Vision-Language Modeling\n\n\n\n\n\n\nMultimodal AI\n\n\nNatural Language Processing\n\n\nComputer Vision\n\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\n𝑓VDB: A Deep-Learning Framework for Sparse, Large-Scale, and High-Performance Spatial Intelligence\n\n\n\n\n\n\n3D Vision\n\n\nDeep Learning\n\n\nSystems and Performance\n\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\nUnraveling the Connection between In-Context Learning and Gradient Descent in Transformers\n\n\n\n\n\n\nNatural Language Processing\n\n\nDeep Learning\n\n\nExplainable AI\n\n\n\n\n\n\nJul 24, 2024\n\n\n\n\n\n\n\nGradient Low-Rank Projection (GaLore): Revolutionizing Memory-Efficient LLM Training\n\n\n\n\n\n\nNatural Language Processing\n\n\nOptimization\n\n\nSystems and Performance\n\n\n\n\n\n\nJul 24, 2024\n\n\n\n\n\n\n\nRetrieval-Enhanced Transformers (RETRO): A Semi-Parametric Approach to Enhance Performance of Large Language Models\n\n\n\n\n\n\nNatural Language Processing\n\n\nDeep Learning\n\n\nSystems and Performance\n\n\n\n\n\n\nJul 20, 2024\n\n\n\n\n\n\n\nFoundation Models in Decision Making: Roles, Challenges, and Opportunities\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine Learning\n\n\nExplainable AI\n\n\n\n\n\n\nJul 20, 2024\n\n\n\n\n\n\n\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n\n\n\n\n\nDeep Learning\n\n\nTransformers\n\n\nSystems and Performance\n\n\n\n\n\n\nJul 19, 2024\n\n\n\n\n\n\n\nPyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\n\n\n\n\n\n\nSystems and Performance\n\n\nDeep Learning\n\n\nMachine Learning\n\n\n\n\n\n\nJul 19, 2024\n\n\n\n\n\n\n\nHyper Networks: A Novel Approach to Learning Weights in Deep Neural Networks\n\n\n\n\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nNeural Networks\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nDARTS: Differentiable Architecture Search\n\n\n\n\n\n\nDeep Learning\n\n\nOptimization\n\n\nMachine Learning\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nTiTok: A Transformer-based 1D Tokenization Approach for Image Generation\n\n\n\n\n\n\nGenerative Models\n\n\nComputer Vision\n\n\nTransformers\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nNerfBaselines: A Framework for Standardized Evaluation of Novel View Synthesis Methods in Computer Vision\n\n\n\n\n\n\n3D Vision\n\n\nComputer Vision\n\n\nSystems and Performance\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nSurvey on reinforcement learning in reccomender systems\n\n\n\n\n\n\nReinforcement Learning\n\n\nRecommender Systems\n\n\nMachine Learning\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nModels tell you what to discard\n\n\n\n\n\n\nSystems and Performance\n\n\nMachine Learning\n\n\nOptimization\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nTraining Large Language Models for Compiler Optimization\n\n\n\n\n\n\nNatural Language Processing\n\n\nSystems and Performance\n\n\nAI for Science\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nMetadata-based Color Harmonization for Multi-camera Surround View Systems\n\n\n\n\n\n\nComputer Vision\n\n\nAutonomous Driving\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nExtrapolated View Synthesis for Urban Scene Reconstruction\n\n\n\n\n\n\n3D Vision\n\n\nComputer Vision\n\n\nGenerative Models\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nPlanning-Oriented Autonomous Driving\n\n\n\n\n\n\nAutonomous Driving\n\n\nArtificial Intelligence\n\n\nMachine Learning\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nSafePathNet: Learning a Distribution of Trajectories for Safe and Comfortable Autonomous Driving\n\n\n\n\n\n\nAutonomous Driving\n\n\nAI Safety\n\n\nMachine Learning\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nUnsupervised Occupancy Fields for Perception and Forecasting\n\n\n\n\n\n\nComputer Vision\n\n\nMachine Learning\n\n\nAutonomous Driving\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nUniPAD: A Universal Pre-training Paradigm for Autonomous Driving\n\n\n\n\n\n\nAutonomous Driving\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nRT-DETR: Real-Time Object Detection with Transformer\n\n\n\n\n\n\nComputer Vision\n\n\nTransformers\n\n\nDeep Learning\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nRobustness Evaluation of HD Map Constructors under Sensor Corruptions for Autonomous Driving\n\n\n\n\n\n\nAutonomous Driving\n\n\nComputer Vision\n\n\nAI Safety\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nDriveVLM: Vision-Language Models for Autonomous Driving in Urban Environments\n\n\n\n\n\n\nAutonomous Driving\n\n\nComputer Vision\n\n\nMultimodal AI\n\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\nTransAct Transformer-based Realtime User Action Model for Recommendation at Pinterest\n\n\n\n\n\n\nRecommender Systems\n\n\nTransformers\n\n\nSystems and Performance\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nZero Bubble Pipeline Parallelism\n\n\n\n\n\n\nSystems and Performance\n\n\nDeep Learning\n\n\nMachine Learning\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nThe limits to learning a diffusion model\n\n\n\n\n\n\nGenerative Models\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nAutoEmb Automated Embedding Dimensionality Searchg in Streaming Recommendations\n\n\n\n\n\n\nDeep Learning\n\n\nRecommender Systems\n\n\nOptimization\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nZeRO Memory Optimizations: Toward Training Trillion Parameter Models\n\n\n\n\n\n\nSystems and Performance\n\n\nDeep Learning\n\n\nNatural Language Processing\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nA Better Match for Drivers and Riders Reinforcement Learning at Lyft\n\n\n\n\n\n\nReinforcement Learning\n\n\nRecommender Systems\n\n\nMachine Learning\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nNeuralProphet Explainable Forecasting at Scale\n\n\n\n\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nExplainable AI\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nNo-Transaction Band Network A Neural Network Architecture for Efficient Deep Hedging\n\n\n\n\n\n\nDeep Learning\n\n\nAI for Science\n\n\nMachine Learning\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "podcast/index.html#about",
    "href": "podcast/index.html#about",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "About",
    "text": "About\n\nThe voices you’ll hear are AI-generated, not real people (though names from papers might appear).\nWhile we strive for accuracy, these are complex topics. Our current AI systems aren’t perfect, so approach with a critical mind.\nConsider this a starting point. For deeper understanding, always refer to the original paper.\nThe papers featured are ones I’m personally interested in or have been wanting to read. It’s a curated selection based on my interests.\n\nThis podcast aims to spark curiosity and make cutting-edge research more accessible. It’s perfect for those moments when you want to learn but can’t dive into a full paper.\nEnjoy the exploration of ideas, and let it fuel your interest in further reading!"
  },
  {
    "objectID": "podcast/index.html#the-ai-team",
    "href": "podcast/index.html#the-ai-team",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "The (AI) Team",
    "text": "The (AI) Team\n\nAlex Askwell: Our curious and knowledgeable moderator, always ready with the right questions to guide our exploration.\nDr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and results.\nProf. Wyd Spectrum: Our field expert, providing broader context and critical insights.\n\nJoin them as they break down complex research into byte-sized breakthroughs!"
  },
  {
    "objectID": "podcast/index.html#request-a-paper",
    "href": "podcast/index.html#request-a-paper",
    "title": "Byte-Sized Breakthroughs Podcast",
    "section": "Request a paper",
    "text": "Request a paper\nIf you have a paper you’d like to see discussed, please fill this form"
  },
  {
    "objectID": "podcast/podcasts/2310.08566/index.html",
    "href": "podcast/podcasts/2310.08566/index.html",
    "title": "Supervised Pretraining for In-Context Reinforcement Learning with Transformers",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper are: Supervised pretraining with transformers can efficiently approximate prevalent RL algorithms, transformers demonstrate the potential for near-optimal regret bounds, and the research highlights the importance of model capacity and distribution divergence in in-context reinforcement learning.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2303.04129/index.html",
    "href": "podcast/podcasts/2303.04129/index.html",
    "title": "Foundation Models in Decision Making: Roles, Challenges, and Opportunities",
    "section": "",
    "text": "The paper proposes a framework for understanding the various roles of foundation models in decision making, including conditional generative models, representation learners, and interactive agents. Key takeaways include the use of foundation models for behavioral priors, world modeling, and generalization of knowledge across tasks and environments.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2408.00714/index.html",
    "href": "podcast/podcasts/2408.00714/index.html",
    "title": "SAM 2: Segment Anything in Images and Videos",
    "section": "",
    "text": "SAM 2 outperformed previous approaches in video segmentation by achieving higher accuracy with fewer user interactions, making it faster and more accurate. The model shows promise in tasks like interactive video object segmentation and long-term video object segmentation, demonstrating its efficiency and ability to handle diverse objects and scenarios.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2306.00248v1/index.html",
    "href": "podcast/podcasts/2306.00248v1/index.html",
    "title": "TransAct Transformer-based Realtime User Action Model for Recommendation at Pinterest",
    "section": "",
    "text": "Pinterest home feed reccomendation system. Needs to react to both long term interests + short term (even single session only) interests.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2210.05675/index.html",
    "href": "podcast/podcasts/2210.05675/index.html",
    "title": "Generalization Patterns of Transformers in In-Weights Learning and In-Context Learning",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper are: 1. In-context learning in large language models tends to be rule-based, suggesting the influence of language structure. 2. Model size and training data structure play crucial roles in shaping the inductive biases of transformers. 3. Pretraining strategies can be used to induce rule-based generalization from context.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1606.06565/index.html",
    "href": "podcast/podcasts/1606.06565/index.html",
    "title": "Practical Research Problems in AI Safety",
    "section": "",
    "text": "The key takeaways for engineers/specialists are: the need for focused research on practical AI safety problems, the importance of developing robust and scalable oversight mechanisms, safe exploration strategies, and systems that are robust to changes in data distribution. The paper provides a valuable framework for addressing these crucial concerns.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2209.11895/index.html",
    "href": "podcast/podcasts/2209.11895/index.html",
    "title": "In-context Learning and Induction Heads",
    "section": "",
    "text": "The emergence of induction heads in transformer models is strongly correlated with a significant improvement in in-context learning abilities. Directly manipulating the formation of induction heads in models led to changes in their in-context learning performance, highlighting the crucial role of these mechanisms in adapting to new tasks without explicit retraining.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1810.05270/index.html",
    "href": "podcast/podcasts/1810.05270/index.html",
    "title": "Rethinking the Value of Network Pruning",
    "section": "",
    "text": "Key takeaways for engineers and specialists include the importance of shifting focus from weight selection to architecture search in network pruning. Training pruned models from scratch can often yield comparable or better results than fine-tuning, particularly for structured pruning methods. Automatic pruning methods offer an efficient way to identify more parameter-efficient network structures, potentially leading to the development of more scalable and powerful deep learning models.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2412.19437/index.html",
    "href": "podcast/podcasts/2412.19437/index.html",
    "title": "DeepSeek-V3: Advancements in Open-Source Large Language Models",
    "section": "",
    "text": "Key takeaways include the introduction of innovative techniques such as the auxiliary-loss-free load balancing method for Mixture-of-Experts models, the multi-token prediction training objective for densified training and faster inference, FP8 mixed-precision training for reduced memory usage, and the optimized DualPipe algorithm for efficient distributed training. The performance of DeepSeek-V3 on coding and math tasks surpasses leading closed-source models at a lower training cost, making it a significant contribution to the open-source community.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1810.00826/index.html",
    "href": "podcast/podcasts/1810.00826/index.html",
    "title": "Graph Isomorphism Networks: A Theoretical Framework and Architecture",
    "section": "",
    "text": "Engineers and specialists should take note of the importance of designing GNN architectures with highly expressive aggregation schemes like the injective multiset functions used in GIN. Understanding the theoretical underpinnings of GNNs and their limitations is crucial for developing more powerful and sophisticated models in the future.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2304.02643/index.html",
    "href": "podcast/podcasts/2304.02643/index.html",
    "title": "Segment Anything: A Paradigm Shift in Image Segmentation",
    "section": "",
    "text": "The key takeaways for engineers/specialists include the innovative concept of promptable segmentation, the development of SAM with components like Image Encoder, Prompt Encoder, and Mask Decoder, and the significant results showcasing SAM’s impressive zero-shot transfer capabilities in various image segmentation tasks. It highlights the potential impact of SAM on generalizing to new tasks and datasets efficiently while providing insights into addressing limitations through future research areas.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2006.06373/index.html",
    "href": "podcast/podcasts/2006.06373/index.html",
    "title": "The limits to learning a diffusion model",
    "section": "",
    "text": "Don’t be confused by the title, diffusion here is not referring to diffusion as we use it today in context of image generation process, but more about modelling diffusive processes (like virus spread)\n  This paper answers the question about ‘how much data do we need, before we can figure out the final affected value’ turns out this is a lot more thant people expect.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1706.03741/index.html",
    "href": "podcast/podcasts/1706.03741/index.html",
    "title": "Training Deep Reinforcement Learning Systems with Human Preferences",
    "section": "",
    "text": "The paper introduces a method that significantly reduces the need for human oversight in training deep RL agents, allowing them to learn complex behaviors with minimal human input. This approach has shown promising results in both simulated robotics and Atari games, achieving human-level performance with a fraction of the human effort required by traditional RL methods.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2006.11239/index.html",
    "href": "podcast/podcasts/2006.11239/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The paper leverages denoising score matching to simplify the training objective for diffusion models, leading to faster and more stable training processes and higher-quality image generation results. Additionally, the paper highlights the potential of diffusion models as efficient lossy compressors, opening up possibilities in data compression applications.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1606.04474/index.html",
    "href": "podcast/podcasts/1606.04474/index.html",
    "title": "Learning to Learn Optimization Algorithms with LSTM Networks",
    "section": "",
    "text": "Engineers and specialists can learn from this paper that training an LSTM-based optimizer can outperform traditional hand-crafted optimization algorithms across various tasks. The use of coordinatewise LSTMs and backpropagation through time for training provides scalability, efficiency, and generalizability. The approach shows promise for automating hyperparameter tuning, developing specialized optimizers, and enhancing the robustness of neural networks.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1502.05477/index.html",
    "href": "podcast/podcasts/1502.05477/index.html",
    "title": "Trust Region Policy Optimization",
    "section": "",
    "text": "Key takeaways: TRPO offers monotonic policy improvements by using a trust region constraint controlled by KL divergence, which leads to more robust and reliable learning. The paper demonstrated the algorithm’s success in complex tasks like robotic locomotion and Atari games, highlighting its flexibility and effectiveness.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2402.12289/index.html",
    "href": "podcast/podcasts/2402.12289/index.html",
    "title": "DriveVLM: Vision-Language Models for Autonomous Driving in Urban Environments",
    "section": "",
    "text": "The paper introduces DriveVLM, a system that leverages Vision-Language Models for scene understanding in autonomous driving. It comprises modules for Scene Description, Scene Analysis, and Hierarchical Planning to handle complex driving scenarios. DriveVLM outperformed other models in handling uncommon objects and unexpected events, while DriveVLM-Dual achieved state-of-the-art performance in planning tasks, showing promise for future improvements in autonomous driving.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2003.08934/index.html",
    "href": "podcast/podcasts/2003.08934/index.html",
    "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
    "section": "",
    "text": "Key takeaways for engineers and specialists from the paper include the efficiency of using a continuous 5D representation instead of discrete meshes or voxel grids, the importance of differentiable volume rendering in training neural networks for scene representation, and the potential of NeRF to revolutionize how 3D content is created and experienced.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2407.10956/index.html",
    "href": "podcast/podcasts/2407.10956/index.html",
    "title": "Spider2-V: Automated Multimodal Agents for Data Science Workflows",
    "section": "",
    "text": "The paper highlights that even advanced VLMs struggle to automate full data workflows, especially in GUI-intensive tasks, with a low success rate of 14%. The study emphasizes the need for improvements in action grounding and training data quality to enhance the performance of AI agents in complex data tasks.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2406.08691/index.html",
    "href": "podcast/podcasts/2406.08691/index.html",
    "title": "Unsupervised Occupancy Fields for Perception and Forecasting",
    "section": "",
    "text": "The paper ‘UnO: Unsupervised Occupancy Fields for Perception and Forecasting’ introduces a novel approach to perception and forecasting in self-driving vehicles using unsupervised learning from raw LiDAR data. By leveraging occupancy fields and deformable attention mechanisms, the UnO model outperformed existing methods on point cloud forecasting and semantic occupancy tasks, showing promise for enhancing the robustness and safety of autonomous systems especially in scenarios where labeled data is limited or rare events occur.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2212.09720/index.html",
    "href": "podcast/podcasts/2212.09720/index.html",
    "title": "Optimizing Quantization of Large Language Models for Efficiency and Accuracy",
    "section": "",
    "text": "Engineers and specialists can leverage 4-bit precision quantization with techniques such as quantile quantization and floating-point representation to significantly reduce the memory footprint and improve inference speed of large language models. Understanding the trade-off between accuracy and efficiency is crucial for deploying powerful NLP technologies in resource-constrained environments and expanding their applications to real-world scenarios.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2005.14165/index.html",
    "href": "podcast/podcasts/2005.14165/index.html",
    "title": "Language Models are Few-Shot Learners",
    "section": "",
    "text": "Key takeaways include the model’s ability to generalize from a few examples (few-shot learning), the comprehensive evaluation of GPT-3’s performance across various NLP tasks, and the importance of responsible research and development to address ethical challenges and risks associated with advanced language models.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2311.09544/index.html",
    "href": "podcast/podcasts/2311.09544/index.html",
    "title": "Scaling User Modeling for Personalized Advertising at Meta",
    "section": "",
    "text": "Key takeaways for engineers/specialists include the importance of efficient sharing of user representations in personalized advertising systems, the benefits of utilizing upstream models for downstream tasks, and the significance of handling dynamic user features and maintaining embedding freshness for improved performance.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1803.03635/index.html",
    "href": "podcast/podcasts/1803.03635/index.html",
    "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
    "section": "",
    "text": "Engineers and specialists can explore the potential of training more efficient, smaller neural networks by identifying and utilizing winning tickets. The iterative pruning with resetting technique can help in finding these winning tickets, showcasing the importance of proper initialization in network efficiency. Additionally, the use of dropout in conjunction with pruning can enhance the effectiveness of the process, leading to more resource-friendly and faster AI models.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2310.10616/index.html",
    "href": "podcast/podcasts/2310.10616/index.html",
    "title": "How Transformers Learn In-Context Beyond Simple Functions",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper include the development of theoretical constructions for transformers to implement in-context ridge regression on representations efficiently. This research showcases the modularity of transformers in decomposing complex tasks into distinct learnable modules, providing strong evidence for their adaptability in handling complex learning scenarios.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2210.03044/index.html",
    "href": "podcast/podcasts/2210.03044/index.html",
    "title": "Unmasking the Lottery Ticket Hypothesis",
    "section": "",
    "text": "The key takeaways for engineers/specialists include understanding the role of the pruning mask in guiding training, the importance of SGD robustness in navigating the error landscape, and the relationship between the Hessian eigenspectrum and the maximum pruning ratio for efficient network pruning.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2502.11089/index.html",
    "href": "podcast/podcasts/2502.11089/index.html",
    "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
    "section": "",
    "text": "Engineers and specialists can learn about the importance of hardware alignment in designing sparse attention mechanisms, the benefits of training sparse attention models from scratch instead of applying sparsity post-hoc, and the significant speedups in training and inference efficiency achieved by Native Sparse Attention compared to Full Attention and other sparse attention methods.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2404.05719/index.html",
    "href": "podcast/podcasts/2404.05719/index.html",
    "title": "Ferret-UI: Multimodal Large Language Model for Mobile User Interface Understanding",
    "section": "",
    "text": "Ferret-UI is the first UI-centric MLLM capable of executing referring, grounding, and reasoning tasks, making it adept at identifying specific UI elements, understanding relationships, and deducing overall screen function. It breaks down screens into sub-images using the ‘any resolution’ approach, providing detailed understanding of UI elements and interactions.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2407.12854/index.html",
    "href": "podcast/podcasts/2407.12854/index.html",
    "title": "Enhancing Language Models with a Massive Datastore",
    "section": "",
    "text": "Key takeaways include the importance of diverse, large datastores for enhancing language model performance, the cost efficiency of constructing datastores compared to training models, and the potential for smaller models with access to large datastores to outperform larger models with limited data access.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2103.00020/index.html",
    "href": "podcast/podcasts/2103.00020/index.html",
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "section": "",
    "text": "Engineers and specialists can utilize CLIP’s contrastive learning approach to create more efficient and scalable computer vision systems. The paper highlights the importance of ethical considerations and bias mitigation strategies in developing AI technologies.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1312.5602/index.html",
    "href": "podcast/podcasts/1312.5602/index.html",
    "title": "Playing Atari with Deep Reinforcement Learning",
    "section": "",
    "text": "The key takeaways for engineers/specialists from this paper are: 1. Deep Q-learning (DQN) with a convolutional neural network can successfully learn to control agents directly from high-dimensional sensory input 2. The combination of deep learning with reinforcement learning showcased human-level performance on Atari games, surpassing traditional methods and even expert human players. 3. The paper laid the foundation for developing more general, adaptable AI systems that can learn and adapt to various complex tasks.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2212.09095/index.html",
    "href": "podcast/podcasts/2212.09095/index.html",
    "title": "Rethinking Scale for In-Context Learning in Large Language Models",
    "section": "",
    "text": "Engineers and specialists can consider the findings of this research to explore the efficiency of large language models. By identifying key components like ‘induction heads’ critical for in-context learning, there is potential to optimize model design for better performance. The study indicates that a focus on enhancing these crucial components could lead to more resource-friendly and effective language models.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1806.09055/index.html",
    "href": "podcast/podcasts/1806.09055/index.html",
    "title": "DARTS: Differentiable Architecture Search",
    "section": "",
    "text": "Key takeaways for engineers/specialists: DARTS introduces a continuous relaxation approach to architecture search, leveraging gradient descent for efficient optimization. It achieves state-of-the-art results on image classification and language modeling tasks with significantly less computational cost. Challenges include the gap between continuous and discrete architecture representation, computational cost of second-order approximation, and sensitivity to hyperparameters.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1910.02054/index.html",
    "href": "podcast/podcasts/1910.02054/index.html",
    "title": "ZeRO Memory Optimizations: Toward Training Trillion Parameter Models",
    "section": "",
    "text": "The paper introduces ZeRO, a novel approach to optimize memory usage when training massive language models. ZeRO-DP and ZeRO-R components effectively reduce memory redundancy and allow for training models with up to 170 billion parameters efficiently. The technique shows superlinear scalability, user-friendly implementation, and has the potential to democratize large model training in AI research.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2403.03507/index.html",
    "href": "podcast/podcasts/2403.03507/index.html",
    "title": "Gradient Low-Rank Projection (GaLore): Revolutionizing Memory-Efficient LLM Training",
    "section": "",
    "text": "GaLore offers a breakthrough in memory-efficient LLM training by reducing memory usage significantly while achieving performance comparable to full-rank training. It enables training of large models on limited hardware resources, democratizing LLM research and development. Future research directions include applying GaLore to various model architectures, enhancing memory efficiency further, and exploring elastic data distributed training using consumer-grade hardware.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2401.14159/index.html",
    "href": "podcast/podcasts/2401.14159/index.html",
    "title": "Grounded SAM: A Novel Approach to Open-Set Segmentation",
    "section": "",
    "text": "The key takeaways for engineers/specialists from the paper are: 1. Grounded SAM combines the strengths of Grounding DINO for object detection and SAM for zero-shot segmentation, outperforming existing models. 2. The model’s potential extends beyond segmentation, enabling integration with other models for tasks like image annotation, image editing, and human motion analysis.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2109.10665/index.html",
    "href": "podcast/podcasts/2109.10665/index.html",
    "title": "Survey on reinforcement learning in reccomender systems",
    "section": "",
    "text": "Goes over some of the different places RL can be used in RecSys.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1609.09106/index.html",
    "href": "podcast/podcasts/1609.09106/index.html",
    "title": "Hyper Networks: A Novel Approach to Learning Weights in Deep Neural Networks",
    "section": "",
    "text": "The key takeaways for engineers/specialists are: Hyper Networks introduce a meta-network (hypernetwork) that learns to generate weight structures for deep neural networks, providing flexibility and efficiency. Dynamic hypernetworks allow weights to adapt to input sequences, improving performance on sequential tasks. End-to-end training of hypernetworks with the main network leads to collaborative optimization and comparable or better performance with fewer parameters.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2407.13218/index.html",
    "href": "podcast/podcasts/2407.13218/index.html",
    "title": "LiNR: Revolutionizing Large-Scale Retrieval for Recommendation Systems",
    "section": "",
    "text": "LiNR’s key contributions include model-based retrieval with pre-filtering, quantization techniques for memory optimization, and integration of GPU capabilities. It outperformed traditional systems, leading to significant increases in user interactions, unique users, and content engagement.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2211.05102/index.html",
    "href": "podcast/podcasts/2211.05102/index.html",
    "title": "Efficiently Scaling Transformer Inference",
    "section": "",
    "text": "Engineers and specialists can take away the importance of considering partitioning strategies and low-level optimizations for efficiently scaling Transformer inference. The use of an analytical cost model, multi-query attention, and batch-wise sharding are highlighted as crucial for scaling context length and maximizing hardware utilization.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2301.00774/index.html",
    "href": "podcast/podcasts/2301.00774/index.html",
    "title": "SparseGPT: One-shot Pruning of Large Language Models",
    "section": "",
    "text": "SparseGPT offers a one-shot pruning approach that avoids costly retraining, making it significantly more efficient for compressing large language models like GPT variants. The method can achieve high sparsity levels while maintaining minimal accuracy loss, providing a promising solution for improving the deployment of powerful language models.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2210.03821/index.html",
    "href": "podcast/podcasts/2210.03821/index.html",
    "title": "In-Context Policy Iteration: Enhancing Reinforcement Learning with Large Language Models",
    "section": "",
    "text": "Engineers and specialists can benefit from the paper’s insights by understanding how ICPI outperforms traditional RL methods through prompt-based learning, the role of rollout policy and world model in guiding the LLM’s decision-making, and the impact of model size on ICPI’s performance in handling complex RL tasks.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2407.01781/index.html",
    "href": "podcast/podcasts/2407.01781/index.html",
    "title": "𝑓VDB: A Deep-Learning Framework for Sparse, Large-Scale, and High-Performance Spatial Intelligence",
    "section": "",
    "text": "Engineers and specialists can benefit from 𝑓VDB by leveraging its memory-efficient IndexGrid structure and specialized convolution kernels optimized for different sparsity patterns. The framework provides significant speed and memory efficiency improvements over existing frameworks, enabling more effective handling of large-scale, sparse 3D datasets in deep learning applications.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/1906.04358/index.html",
    "href": "podcast/podcasts/1906.04358/index.html",
    "title": "Exploring Weight Agnostic Neural Networks",
    "section": "",
    "text": "The research presents a paradigm shift towards designing networks with inherent capabilities, emphasizing architecture over weight optimization. WANNs demonstrate high performance on various tasks with random weights, suggesting potential for efficient learning and broader generalization in deep learning applications.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  },
  {
    "objectID": "podcast/podcasts/2103.01775/index.html",
    "href": "podcast/podcasts/2103.01775/index.html",
    "title": "No-Transaction Band Network A Neural Network Architecture for Efficient Deep Hedging",
    "section": "",
    "text": "The paper introduces a deep hedging approach using neural networks to optimize hedging strategies for derivatives in imperfect markets. The key takeaway is the development of the ‘no-transaction band network’ to address action dependence and improve efficiency in hedging, showcasing superior performance compared to traditional methods in terms of expected utility and price efficiency, and faster training. Future research focuses on addressing limitations such as non-linear transaction costs and discontinuous payoffs, as well as challenges in data availability and model explainability for real-world applications.\n  \n  \n\n\n\n  \n\n    Listen on your favorite platforms\n\n    \n    \n    \n    \n\n\n\n    Listen to the Episode\n    \n      \n        \n        Your browser does not support the audio element.\n      \n    \n\n    \n      Related Links\n      \n        \n        Read transcript\n        Read original paper\n      \n    \n\n    The (AI) Team\n    \n      Alex Askwell: Our curious and knowledgeable moderator, always ready with the right\n        questions\n        to guide our exploration.\n      Dr. Paige Turner: Our lead researcher and paper expert, diving deep into the methods and\n        results.\n      Prof. Wyd Spectrum: Our field expert, providing broader context and critical insights."
  }
]