<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Byte Sized Breakthroughs</title><link>https://arjunsriva.com/podcast/</link><description>Research papers, long form content, converted to podcasts</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://arjunsriva.com/static/podcast_data/coverart.jpg</url><title>Byte Sized Breakthroughs</title><link>https://arjunsriva.com/podcast/</link></image><language>en</language><lastBuildDate>Wed, 24 Jul 2024 05:59:45 +0000</lastBuildDate><itunes:author>Arjun Srivastava</itunes:author><itunes:category text="Technology"><itunes:category text="AI &amp; Machine Learning"/></itunes:category><itunes:image href="https://arjunsriva.com/static/podcast_data/coverart.jpg"/><itunes:owner><itunes:name>Arjun Srivastava</itunes:name><itunes:email>arjunsriva@gmail.com</itunes:email></itunes:owner><item><title>Survey on reinforcement learning in reccomender systems</title><link>https://arjunsriva.com/podcast/podcasts/2109.10665/</link><description>

Goes over some of the different places RL can be used in RecSys.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2109.10665/</guid><category>Reccomender Systems</category><category>Reinforcement Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2109.10665.mp3" length="17304480" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>TransAct Transformer-based Realtime User Action Model for Recommendation at Pinterest</title><link>https://arjunsriva.com/podcast/podcasts/2306.00248v1/</link><description>

Pinterest home feed reccomendation system.
Needs to react to both long term interests + short term (even single session only) interests.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2306.00248v1/</guid><category>Reccomender Systems</category><category>Transformer</category><category>Systems</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2306.00248v1.mp3" length="12047520" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Models tell you what to discard</title><link>https://arjunsriva.com/podcast/podcasts/2310.01801/</link><description>

This paper introduces FastGen, a novel method that uses lightweight model profiling and adaptive key-value caching to significantly reduce memory footprint without noticeable quality loss.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2310.01801/</guid><category>Performance</category><category>Systems</category><category>Memory</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2310.01801.mp3" length="7944480" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Zero Bubble Pipeline Parallelism</title><link>https://arjunsriva.com/podcast/podcasts/2401.10241/</link><description>

Core idea is think about backward pass into two flows, one to compute grad wrt to parameters, and one to compute grad wrt to output of last layer, 
schedule so that you are always working instead of waiting (bubble).</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2401.10241/</guid><category>Deep Learning</category><category>Performance</category><category>Systems</category><category>Throughput</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2401.10241.mp3" length="9619200" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>The limits to learning a diffusion model</title><link>https://arjunsriva.com/podcast/podcasts/2006.06373/</link><description>

Don't be confused by the title, diffusion here is not referring to diffusion as we use it today
in context of image generation process, but more about modelling diffusive processes (like virus spread)

This paper answers the question about 'how much data do we need, before we can figure out the final affected value'
turns out this is a lot more thant people expect.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2006.06373/</guid><category>Machine Learning</category><category>Statistics</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2006.06373.mp3" length="8771520" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>A Better Match for Drivers and Riders Reinforcement Learning at Lyft</title><link>https://arjunsriva.com/podcast/podcasts/2310.13810/</link><description>

The paper demonstrates the successful application of reinforcement learning to improve the efficiency of driver-rider matching in ride-sharing platforms. The use of online RL allows for real-time adaptation, resulting in decreased wait times for riders, increased earnings for drivers, and overall higher user satisfaction. The research paves the way for more intelligent systems in the ride-sharing industry, with potential for further optimization and expansion into various other aspects of the ecosystem.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2310.13810/</guid><category>Reccomender Systems</category><category>Reinforcement Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2310.13810.mp3" length="9926400" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>AutoEmb Automated Embedding Dimensionality Searchg in Streaming Recommendations</title><link>https://arjunsriva.com/podcast/podcasts/2002.11252/</link><description>

AutoEmb is about using different lenghts of embedding vectors for different items,
use less memory + potentially learn more robust stuff for items with less data, and learn
more nuanced stuff for popular items.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2002.11252/</guid><category>Deep Learning</category><category>Reccomender Systems</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2002.11252.mp3" length="15328320" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>NeuralProphet Explainable Forecasting at Scale</title><link>https://arjunsriva.com/podcast/podcasts/2111.15397/</link><description>

'_Successor_' of Prophet (by facebook) for time series modelling.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2111.15397/</guid><category>Deep Learning</category><category>Forecasting</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2111.15397.mp3" length="16233600" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>No-Transaction Band Network A Neural Network Architecture for Efficient Deep Hedging</title><link>https://arjunsriva.com/podcast/podcasts/2103.01775/</link><description>

The paper introduces a deep hedging approach using neural networks to optimize hedging strategies for derivatives in imperfect markets. The key takeaway is the development of the 'no-transaction band network' to address action dependence and improve efficiency in hedging, showcasing superior performance compared to traditional methods in terms of expected utility and price efficiency, and faster training. Future research focuses on addressing limitations such as non-linear transaction costs and discontinuous payoffs, as well as challenges in data availability and model explainability for real-world applications.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2103.01775/</guid><category>Transformer</category><category>Finance</category><category>Deep Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2103.01775.mp3" length="11212320" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>ZeRO Memory Optimizations: Toward Training Trillion Parameter Models</title><link>https://arjunsriva.com/podcast/podcasts/1910.02054/</link><description>

The paper introduces ZeRO, a novel approach to optimize memory usage when training massive language models. ZeRO-DP and ZeRO-R components effectively reduce memory redundancy and allow for training models with up to 170 billion parameters efficiently. The technique shows superlinear scalability, user-friendly implementation, and has the potential to democratize large model training in AI research.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/1910.02054/</guid><category>Performance</category><category>Systems</category><category>Memory</category><category>Large Scale Model Training</category><category>Deep Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/1910.02054.mp3" length="8355360" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>DriveVLM: Vision-Language Models for Autonomous Driving in Urban Environments</title><link>https://arjunsriva.com/podcast/podcasts/2402.12289/</link><description>

The paper introduces DriveVLM, a system that leverages Vision-Language Models for scene understanding in autonomous driving. It comprises modules for Scene Description, Scene Analysis, and Hierarchical Planning to handle complex driving scenarios. DriveVLM outperformed other models in handling uncommon objects and unexpected events, while DriveVLM-Dual achieved state-of-the-art performance in planning tasks, showing promise for future improvements in autonomous driving.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2402.12289/</guid><category>Autonomous Driving</category><category>Vision-Language Models</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2402.12289.mp3" length="9219840" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>RT-DETR: Real-Time Object Detection with Transformer</title><link>https://arjunsriva.com/podcast/podcasts/2304.08069/</link><description>

RT-DETR is a groundbreaking end-to-end real-time object detector based on Transformers that combines the speed of YOLO with the accuracy of DETR. Key takeaways for engineers include the efficient hybrid encoder approach, which improves multi-scale feature interactions, and the uncertainty-minimal query selection scheme, enhancing accuracy in both classification and localization. Despite outperforming traditional CNN-based methods, RT-DETR faces challenges in detecting small objects, prompting future research directions like knowledge distillation.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2304.08069/</guid><category>Computer Vision</category><category>Object Detection</category><category>Transformer</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2304.08069.mp3" length="8927040" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2310.08370/</link><description>

UniPAD is a novel self-supervised learning framework designed for autonomous driving, focusing on learning effective representations from 3D data such as LiDAR point clouds and multi-view images. The framework consists of a modality-specific encoder, a mask generator for challenging training, a unified 3D volumetric representation, and a neural rendering decoder. UniPAD showed promising results in improving performance on tasks like 3D object detection and semantic segmentation, outperforming other pre-training methods and offering potential for broader applications beyond autonomous driving.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2310.08370/</guid><category>Autonomous Driving</category><category>Self-supervised Learning</category><category>3D Data Processing</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2310.08370.mp3" length="14966400" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Metadata-based Color Harmonization for Multi-camera Surround View Systems</title><link>https://arjunsriva.com/podcast/podcasts/2406.11066/</link><description>

The paper introduces a metadata-based approach to address color inconsistencies in multi-camera surround view systems, crucial for accurate perception in autonomous driving. The method significantly outperforms traditional techniques in visual quality and runtime, making it more efficient and robust for real-time applications.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.11066/</guid><category>Autonomous Driving</category><category>Color Harmonization</category><category>Computer Vision</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.11066.mp3" length="9720000" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>SafePathNet: Learning a Distribution of Trajectories for Safe and Comfortable Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2211.02131/</link><description>

SafePathNet introduces a novel approach that models the distribution of future trajectories for both the self-driving vehicle and other road agents using a unified neural network architecture. By incorporating a 'Mixture of Experts' framework, the model can learn diverse driving strategies and prioritize safety in real-time decision-making. The use of Transformer networks and imitation learning further enhances the model's ability to handle complex and unpredictable driving scenarios.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2211.02131/</guid><category>Autonomous Driving</category><category>Forecasting</category><category>Transformer</category><category>Imitation Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2211.02131.mp3" length="14214240" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Planning-Oriented Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2212.10156/</link><description>

The paper introduces UniAD, a planning-oriented framework for autonomous driving that focuses on integrating perception, prediction, and planning tasks to optimize for safe and efficient driving. UniAD outperforms existing state-of-the-art methods in motion forecasting, occupancy prediction, and planning, showcasing the benefits of joint optimization and query-based communication between modules. Key challenges for future research include addressing computational complexity, handling long-tail scenarios, and exploring additional tasks like depth estimation and behavior prediction.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2212.10156/</guid><category>Autonomous Driving</category><category>Computer Vision</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2212.10156.mp3" length="13392480" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>NerfBaselines: A Framework for Standardized Evaluation of Novel View Synthesis Methods in Computer Vision</title><link>https://arjunsriva.com/podcast/podcasts/2406.17345/</link><description>

NerfBaselines addresses the inconsistent evaluation protocols in comparing novel view synthesis methods by providing a unified interface, ensuring reproducibility through containerization, and standardizing the evaluation protocol. By enabling the sharing of pre-trained checkpoints, it reduces computational costs and environmental impact. However, it relies on methods exposing the same interface and future directions involve exploring advanced evaluation metrics and addressing the computational cost of training.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.17345/</guid><category>Computer Vision</category><category>Neural Radiance Fields</category><category>3D Gaussian Splatting</category><category>Standardized Evaluation</category><category>Reproducibility</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.17345.mp3" length="9757440" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Unsupervised Occupancy Fields for Perception and Forecasting</title><link>https://arjunsriva.com/podcast/podcasts/2406.08691/</link><description>

The paper 'UnO: Unsupervised Occupancy Fields for Perception and Forecasting' introduces a novel approach to perception and forecasting in self-driving vehicles using unsupervised learning from raw LiDAR data. By leveraging occupancy fields and deformable attention mechanisms, the UnO model outperformed existing methods on point cloud forecasting and semantic occupancy tasks, showing promise for enhancing the robustness and safety of autonomous systems especially in scenarios where labeled data is limited or rare events occur.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.08691/</guid><category>Computer Vision</category><category>Forecasting</category><category>Unsupervised learning</category><category>LiDAR</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.08691.mp3" length="12446880" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Extrapolated View Synthesis for Urban Scene Reconstruction</title><link>https://arjunsriva.com/podcast/podcasts/2407.02945/</link><description>

The paper introduces Extrapolated View Synthesis (EVS) for urban scene reconstruction, addressing limitations in current methods by using 3D Gaussian Splatting for scene representation. By incorporating surface normal information and leveraging diffusion models, the proposed method, VEGS, outperforms existing approaches in generating visually realistic and accurate renderings for urban environments.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2407.02945/</guid><category>Computer Vision</category><category>Urban Scene Reconstruction</category><category>3D Reconstruction</category><category>Gaussian Splatting</category><category>Diffusion Models</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2407.02945.mp3" length="13698720" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Robustness Evaluation of HD Map Constructors under Sensor Corruptions for Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2406.12214/</link><description>

The paper focuses on evaluating the robustness of HD map constructors under various sensor corruptions using a comprehensive benchmark called MapBench. It highlights the vulnerability of existing methods to real-world challenges and suggests the importance of advanced data augmentation techniques and new network architectures to enhance robustness for autonomous driving applications.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.12214/</guid><category>Autonomous Driving</category><category>Robustness</category><category>Evaluation</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.12214.mp3" length="10693440" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Training Large Language Models for Compiler Optimization</title><link>https://arjunsriva.com/podcast/podcasts/2407.02524/</link><description>

The research paper discusses the development of LLM Compiler, a model specifically trained on compiler IRs and assembly code for optimizing code efficiently. This approach outperforms traditional techniques and existing LLMs in tasks like flag tuning and disassembly, showing potential for automating and improving the optimization process in software engineering.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2407.02524/</guid><category>LLM</category><category>Compiler Optimization</category><category>Software Engineering</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2407.02524.mp3" length="15504000" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>DARTS: Differentiable Architecture Search</title><link>https://arjunsriva.com/podcast/podcasts/1806.09055/</link><description>

Key takeaways for engineers/specialists: DARTS introduces a continuous relaxation approach to architecture search, leveraging gradient descent for efficient optimization. It achieves state-of-the-art results on image classification and language modeling tasks with significantly less computational cost. Challenges include the gap between continuous and discrete architecture representation, computational cost of second-order approximation, and sensitivity to hyperparameters.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/1806.09055/</guid><category>Neural Architecture Search</category><category>Deep Learning</category><category>Gradient-Based Optimization</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/1806.09055.mp3" length="15036960" type="audio/mpeg"/><pubDate>Fri, 19 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>TiTok: A Transformer-based 1D Tokenization Approach for Image Generation</title><link>https://arjunsriva.com/podcast/podcasts/2406.07550/</link><description>

TiTok introduces a novel 1D tokenization method for image generation, enabling the representation of images with significantly fewer tokens while maintaining or surpassing the performance of existing 2D grid-based methods. The approach leverages a Vision Transformer architecture, two-stage training with proxy codes, and achieves remarkable speedup in training and inference. The research opens up new possibilities for efficient and high-quality image generation, with implications for various applications in computer vision and beyond.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.07550/</guid><category>Image Generation</category><category>Computer Vision</category><category>Transformer</category><category>Generative Models</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.07550.mp3" length="12322560" type="audio/mpeg"/><pubDate>Fri, 19 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Hyper Networks: A Novel Approach to Learning Weights in Deep Neural Networks</title><link>https://arjunsriva.com/podcast/podcasts/1609.09106/</link><description>

The key takeaways for engineers/specialists are: Hyper Networks introduce a meta-network (hypernetwork) that learns to generate weight structures for deep neural networks, providing flexibility and efficiency. Dynamic hypernetworks allow weights to adapt to input sequences, improving performance on sequential tasks. End-to-end training of hypernetworks with the main network leads to collaborative optimization and comparable or better performance with fewer parameters.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/1609.09106/</guid><category>Deep Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/1609.09106.mp3" length="17034240" type="audio/mpeg"/><pubDate>Fri, 19 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Foundation Models in Decision Making: Roles, Challenges, and Opportunities</title><link>https://arjunsriva.com/podcast/podcasts/2303.04129/</link><description>

The paper proposes a framework for understanding the various roles of foundation models in decision making, including conditional generative models, representation learners, and interactive agents. Key takeaways include the use of foundation models for behavioral priors, world modeling, and generalization of knowledge across tasks and environments.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2303.04129/</guid><category>Decision Making</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2303.04129.mp3" length="15606240" type="audio/mpeg"/><pubDate>Sat, 20 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Retrieval-Enhanced Transformers (RETRO): A Semi-Parametric Approach to Enhance Performance of Large Language Models</title><link>https://arjunsriva.com/podcast/podcasts/2112.04426/</link><description>

The paper introduces the RETRO model, which leverages retrieval from a massive text database to enhance large language model performance without increasing model size. Key takeaways include the benefits of linear time complexity for retrieval, the use of frozen BERT for efficient retrieval, and the importance of addressing test set leakage in evaluation.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2112.04426/</guid><category>Retrieval-Based Models</category><category>Deep Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2112.04426.mp3" length="21521760" type="audio/mpeg"/><pubDate>Sat, 20 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://arjunsriva.com/podcast/podcasts/2205.14135/</link><description>

FlashAttention is a novel algorithm that addresses the efficiency of Transformer models by improving speed and memory efficiency through IO-awareness. It reduces the number of memory accesses by dividing data into smaller blocks and loading them into fast memory, achieving practical speedups and enabling training on longer sequences. The algorithm also incorporates recomputation during the backward pass to minimize memory usage, delivering significant improvements in training large models like BERT and GPT-2.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2205.14135/</guid><category>Deep Learning</category><category>Transformer</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2205.14135.mp3" length="9953280" type="audio/mpeg"/><pubDate>Sat, 20 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</title><link>https://arjunsriva.com/podcast/podcasts/2304.11277/</link><description>

FSDP addresses memory capacity challenges by sharding parameters across devices, employs communication optimizations to enhance efficiency, includes a rate limiter feature to control memory impact, offers user-friendly APIs for easy integration, achieved promising results on large models, enables broader applications in various domains, faces challenges in mathematical equivalence and handling shared parameters, and has potential research directions in adaptive sharding strategies, new communication primitives, and combining with other parallelism paradigms.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2304.11277/</guid><category>Large-scale model training</category><category>Deep Learning</category><category>Distributed computing</category><category>PyTorch</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2304.11277.mp3" length="14442720" type="audio/mpeg"/><pubDate>Sat, 20 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Gradient Low-Rank Projection (GaLore): Revolutionizing Memory-Efficient LLM Training</title><link>https://arjunsriva.com/podcast/podcasts/2403.03507/</link><description>The paper introduces a new approach named Gradient Low-Rank Projection (GaLore) to train large language models (LLMs) with full parameter learning while being significantly more memory-efficient than existing techniques. GaLore dynamically switches between multiple low-rank subspaces to represent the gradient during training, enabling the exploration of different directions while maintaining memory savings.

GaLore offers a breakthrough in memory-efficient LLM training by reducing memory usage significantly while achieving performance comparable to full-rank training. It enables training of large models on limited hardware resources, democratizing LLM research and development. Future research directions include applying GaLore to various model architectures, enhancing memory efficiency further, and exploring elastic data distributed training using consumer-grade hardware.</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2403.03507/</guid><category>Machine Learning</category><category>Large Language Models (LLMs)</category><category>Memory Efficiency</category><category>Gradient Low-Rank Methods</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2403.03507.mp3" length="12060960" type="audio/mpeg"/><pubDate>Wed, 24 Jul 2024 00:00:00 +0900</pubDate><itunes:author>Arjun Srivastava</itunes:author></item></channel></rss>