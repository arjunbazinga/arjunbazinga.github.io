<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Byte Sized Breakthroughs</title><link>https://arjunsriva.com/static/podcast_data/feed.xml</link><description>
Byte-Sized Breakthroughs offers concise audio summaries of recent AI research papers. Each episode breaks down a single paper in areas like machine learning, computer vision, or natural language processing, making it easier to stay current with AI advancements.

The podcast covers topics such as large language models, mechanistic interpretability, and in-context learning. Episodes feature clear explanations of complex concepts, designed for efficient listening.

Ideal for researchers, engineers, and AI enthusiasts with limited time, Byte-Sized Breakthroughs provides a starting point for exploring cutting-edge AI research. While offering overviews, listeners are encouraged to refer to original papers for comprehensive understanding.

Curated by Arjun Srivastava, an engineer in the field, this podcast transforms spare moments into opportunities for learning about the latest in AI. Note: The voices you hear are not real people, but the content is carefully curated and reviewed.
</description><atom:link href="https://arjunsriva.com/static/podcast_data/feed.xml" rel="self"/><copyright>Â© 2024 Arjun Srivastava</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://arjunsriva.com/static/podcast_data/coverart.jpg</url><title>Byte Sized Breakthroughs</title><link>https://arjunsriva.com/static/podcast_data/feed.xml</link></image><language>en</language><lastBuildDate>Sun, 04 Aug 2024 15:59:59 +0000</lastBuildDate><itunes:author>Arjun Srivastava</itunes:author><itunes:category text="Technology"><itunes:category text="Technology"/></itunes:category><itunes:image href="https://arjunsriva.com/static/podcast_data/coverart.jpg"/><itunes:explicit>no</itunes:explicit><itunes:owner><itunes:name>Arjun Srivastava</itunes:name><itunes:email>arjunsriva@gmail.com</itunes:email></itunes:owner><item><title>TransAct Transformer-based Realtime User Action Model for Recommendation at Pinterest</title><link>https://arjunsriva.com/podcast/podcasts/2306.00248v1/</link><description>


Pinterest home feed reccomendation system.
Needs to react to both long term interests + short term (even single session only) interests.

Read full paper: https://arxiv.org/abs/2306.00248v1

Tags: Recommender Systems, Transformers, Systems and Performance
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2306.00248v1/</guid><category>Recommender Systems</category><category>Transformers</category><category>Systems and Performance</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2306.00248v1.mp3" length="12047520" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Zero Bubble Pipeline Parallelism</title><link>https://arjunsriva.com/podcast/podcasts/2401.10241/</link><description>


Core idea is think about backward pass into two flows, one to compute grad wrt to parameters, and one to compute grad wrt to output of last layer, 
schedule so that you are always working instead of waiting (bubble).

Read full paper: https://arxiv.org/abs/2401.10241

Tags: Systems and Performance, Deep Learning, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2401.10241/</guid><category>Systems and Performance</category><category>Deep Learning</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2401.10241.mp3" length="9619200" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>The limits to learning a diffusion model</title><link>https://arjunsriva.com/podcast/podcasts/2006.06373/</link><description>


Don't be confused by the title, diffusion here is not referring to diffusion as we use it today
in context of image generation process, but more about modelling diffusive processes (like virus spread)

This paper answers the question about 'how much data do we need, before we can figure out the final affected value'
turns out this is a lot more thant people expect.

Read full paper: https://arxiv.org/abs/2006.06373

Tags: Generative Models, Machine Learning, Deep Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2006.06373/</guid><category>Generative Models</category><category>Machine Learning</category><category>Deep Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2006.06373.mp3" length="8771520" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>A Better Match for Drivers and Riders Reinforcement Learning at Lyft</title><link>https://arjunsriva.com/podcast/podcasts/2310.13810/</link><description>


The paper demonstrates the successful application of reinforcement learning to improve the efficiency of driver-rider matching in ride-sharing platforms. The use of online RL allows for real-time adaptation, resulting in decreased wait times for riders, increased earnings for drivers, and overall higher user satisfaction. The research paves the way for more intelligent systems in the ride-sharing industry, with potential for further optimization and expansion into various other aspects of the ecosystem.

Read full paper: https://arxiv.org/abs/2310.13810

Tags: Reinforcement Learning, Recommender Systems, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2310.13810/</guid><category>Reinforcement Learning</category><category>Recommender Systems</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2310.13810.mp3" length="9926400" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>AutoEmb Automated Embedding Dimensionality Searchg in Streaming Recommendations</title><link>https://arjunsriva.com/podcast/podcasts/2002.11252/</link><description>


AutoEmb is about using different lenghts of embedding vectors for different items,
use less memory + potentially learn more robust stuff for items with less data, and learn
more nuanced stuff for popular items.

Read full paper: https://arxiv.org/abs/2002.11252

Tags: Deep Learning, Recommender Systems, Optimization
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2002.11252/</guid><category>Deep Learning</category><category>Recommender Systems</category><category>Optimization</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2002.11252.mp3" length="15328320" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>NeuralProphet Explainable Forecasting at Scale</title><link>https://arjunsriva.com/podcast/podcasts/2111.15397/</link><description>


'_Successor_' of Prophet (by facebook) for time series modelling.

Read full paper: https://arxiv.org/abs/2111.15397

Tags: Deep Learning, Machine Learning, Explainable AI
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2111.15397/</guid><category>Deep Learning</category><category>Machine Learning</category><category>Explainable AI</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2111.15397.mp3" length="16233600" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>No-Transaction Band Network A Neural Network Architecture for Efficient Deep Hedging</title><link>https://arjunsriva.com/podcast/podcasts/2103.01775/</link><description>


The paper introduces a deep hedging approach using neural networks to optimize hedging strategies for derivatives in imperfect markets. The key takeaway is the development of the 'no-transaction band network' to address action dependence and improve efficiency in hedging, showcasing superior performance compared to traditional methods in terms of expected utility and price efficiency, and faster training. Future research focuses on addressing limitations such as non-linear transaction costs and discontinuous payoffs, as well as challenges in data availability and model explainability for real-world applications.

Read full paper: https://arxiv.org/abs/2103.01775

Tags: Deep Learning, AI for Science, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2103.01775/</guid><category>Deep Learning</category><category>AI for Science</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2103.01775.mp3" length="11212320" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>ZeRO Memory Optimizations: Toward Training Trillion Parameter Models</title><link>https://arjunsriva.com/podcast/podcasts/1910.02054/</link><description>


The paper introduces ZeRO, a novel approach to optimize memory usage when training massive language models. ZeRO-DP and ZeRO-R components effectively reduce memory redundancy and allow for training models with up to 170 billion parameters efficiently. The technique shows superlinear scalability, user-friendly implementation, and has the potential to democratize large model training in AI research.

Read full paper: https://arxiv.org/abs/1910.02054

Tags: Systems and Performance, Deep Learning, Natural Language Processing
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/1910.02054/</guid><category>Systems and Performance</category><category>Deep Learning</category><category>Natural Language Processing</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/1910.02054.mp3" length="8355360" type="audio/mpeg"/><pubDate>Mon, 08 Jul 2024 19:18:11 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>DriveVLM: Vision-Language Models for Autonomous Driving in Urban Environments</title><link>https://arjunsriva.com/podcast/podcasts/2402.12289/</link><description>


The paper introduces DriveVLM, a system that leverages Vision-Language Models for scene understanding in autonomous driving. It comprises modules for Scene Description, Scene Analysis, and Hierarchical Planning to handle complex driving scenarios. DriveVLM outperformed other models in handling uncommon objects and unexpected events, while DriveVLM-Dual achieved state-of-the-art performance in planning tasks, showing promise for future improvements in autonomous driving.

Read full paper: https://arxiv.org/abs/2402.12289

Tags: Autonomous Driving, Computer Vision, Multimodal AI
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2402.12289/</guid><category>Autonomous Driving</category><category>Computer Vision</category><category>Multimodal AI</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2402.12289.mp3" length="9219840" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:02:19 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Robustness Evaluation of HD Map Constructors under Sensor Corruptions for Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2406.12214/</link><description>


The paper focuses on evaluating the robustness of HD map constructors under various sensor corruptions using a comprehensive benchmark called MapBench. It highlights the vulnerability of existing methods to real-world challenges and suggests the importance of advanced data augmentation techniques and new network architectures to enhance robustness for autonomous driving applications.

Read full paper: https://arxiv.org/abs/2406.12214

Tags: Autonomous Driving, Computer Vision, AI Safety
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.12214/</guid><category>Autonomous Driving</category><category>Computer Vision</category><category>AI Safety</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.12214.mp3" length="10693440" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:16:07 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>RT-DETR: Real-Time Object Detection with Transformer</title><link>https://arjunsriva.com/podcast/podcasts/2304.08069/</link><description>


RT-DETR is a groundbreaking end-to-end real-time object detector based on Transformers that combines the speed of YOLO with the accuracy of DETR. Key takeaways for engineers include the efficient hybrid encoder approach, which improves multi-scale feature interactions, and the uncertainty-minimal query selection scheme, enhancing accuracy in both classification and localization. Despite outperforming traditional CNN-based methods, RT-DETR faces challenges in detecting small objects, prompting future research directions like knowledge distillation.

Read full paper: https://arxiv.org/abs/2304.08069

Tags: Computer Vision, Transformers, Deep Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2304.08069/</guid><category>Computer Vision</category><category>Transformers</category><category>Deep Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2304.08069.mp3" length="8927040" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:17:01 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2310.08370/</link><description>


UniPAD is a novel self-supervised learning framework designed for autonomous driving, focusing on learning effective representations from 3D data such as LiDAR point clouds and multi-view images. The framework consists of a modality-specific encoder, a mask generator for challenging training, a unified 3D volumetric representation, and a neural rendering decoder. UniPAD showed promising results in improving performance on tasks like 3D object detection and semantic segmentation, outperforming other pre-training methods and offering potential for broader applications beyond autonomous driving.

Read full paper: https://arxiv.org/abs/2310.08370

Tags: Autonomous Driving, Deep Learning, Computer Vision
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2310.08370/</guid><category>Autonomous Driving</category><category>Deep Learning</category><category>Computer Vision</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2310.08370.mp3" length="14966400" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:22:59 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Unsupervised Occupancy Fields for Perception and Forecasting</title><link>https://arjunsriva.com/podcast/podcasts/2406.08691/</link><description>


The paper 'UnO: Unsupervised Occupancy Fields for Perception and Forecasting' introduces a novel approach to perception and forecasting in self-driving vehicles using unsupervised learning from raw LiDAR data. By leveraging occupancy fields and deformable attention mechanisms, the UnO model outperformed existing methods on point cloud forecasting and semantic occupancy tasks, showing promise for enhancing the robustness and safety of autonomous systems especially in scenarios where labeled data is limited or rare events occur.

Read full paper: https://arxiv.org/abs/2406.08691

Tags: Computer Vision, Machine Learning, Autonomous Driving
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.08691/</guid><category>Computer Vision</category><category>Machine Learning</category><category>Autonomous Driving</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.08691.mp3" length="12446880" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:25:02 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>SafePathNet: Learning a Distribution of Trajectories for Safe and Comfortable Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2211.02131/</link><description>


SafePathNet introduces a novel approach that models the distribution of future trajectories for both the self-driving vehicle and other road agents using a unified neural network architecture. By incorporating a 'Mixture of Experts' framework, the model can learn diverse driving strategies and prioritize safety in real-time decision-making. The use of Transformer networks and imitation learning further enhances the model's ability to handle complex and unpredictable driving scenarios.

Read full paper: https://arxiv.org/abs/2211.02131

Tags: Autonomous Driving, AI Safety, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2211.02131/</guid><category>Autonomous Driving</category><category>AI Safety</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2211.02131.mp3" length="14214240" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:36:00 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Planning-Oriented Autonomous Driving</title><link>https://arjunsriva.com/podcast/podcasts/2212.10156/</link><description>


The paper introduces UniAD, a planning-oriented framework for autonomous driving that focuses on integrating perception, prediction, and planning tasks to optimize for safe and efficient driving. UniAD outperforms existing state-of-the-art methods in motion forecasting, occupancy prediction, and planning, showcasing the benefits of joint optimization and query-based communication between modules. Key challenges for future research include addressing computational complexity, handling long-tail scenarios, and exploring additional tasks like depth estimation and behavior prediction.

Read full paper: https://arxiv.org/abs/2212.10156

Tags: Autonomous Driving, Artificial Intelligence, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2212.10156/</guid><category>Autonomous Driving</category><category>Artificial Intelligence</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2212.10156.mp3" length="13392480" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:36:51 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Extrapolated View Synthesis for Urban Scene Reconstruction</title><link>https://arjunsriva.com/podcast/podcasts/2407.02945/</link><description>


The paper introduces Extrapolated View Synthesis (EVS) for urban scene reconstruction, addressing limitations in current methods by using 3D Gaussian Splatting for scene representation. By incorporating surface normal information and leveraging diffusion models, the proposed method, VEGS, outperforms existing approaches in generating visually realistic and accurate renderings for urban environments.

Read full paper: https://arxiv.org/abs/2407.02945

Tags: 3D Vision, Computer Vision, Generative Models
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2407.02945/</guid><category>3D Vision</category><category>Computer Vision</category><category>Generative Models</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2407.02945.mp3" length="13698720" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:39:56 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Metadata-based Color Harmonization for Multi-camera Surround View Systems</title><link>https://arjunsriva.com/podcast/podcasts/2406.11066/</link><description>


The paper introduces a metadata-based approach to address color inconsistencies in multi-camera surround view systems, crucial for accurate perception in autonomous driving. The method significantly outperforms traditional techniques in visual quality and runtime, making it more efficient and robust for real-time applications.

Read full paper: https://arxiv.org/abs/2406.11066

Tags: Computer Vision, Autonomous Driving
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.11066/</guid><category>Computer Vision</category><category>Autonomous Driving</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.11066.mp3" length="9720000" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:47:18 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Training Large Language Models for Compiler Optimization</title><link>https://arjunsriva.com/podcast/podcasts/2407.02524/</link><description>


The research paper discusses the development of LLM Compiler, a model specifically trained on compiler IRs and assembly code for optimizing code efficiently. This approach outperforms traditional techniques and existing LLMs in tasks like flag tuning and disassembly, showing potential for automating and improving the optimization process in software engineering.

Read full paper: https://arxiv.org/abs/2407.02524

Tags: Natural Language Processing, Systems and Performance, AI for Science
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2407.02524/</guid><category>Natural Language Processing</category><category>Systems and Performance</category><category>AI for Science</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2407.02524.mp3" length="15504000" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 19:49:21 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Models tell you what to discard</title><link>https://arjunsriva.com/podcast/podcasts/2310.01801/</link><description>


This paper introduces FastGen, a novel method that uses lightweight model profiling and adaptive key-value caching to significantly reduce memory footprint without noticeable quality loss.

Read full paper: https://arxiv.org/abs/2310.01801

Tags: Systems and Performance, Machine Learning, Optimization
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2310.01801/</guid><category>Systems and Performance</category><category>Machine Learning</category><category>Optimization</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2310.01801.mp3" length="7944480" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 20:05:09 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Survey on reinforcement learning in reccomender systems</title><link>https://arjunsriva.com/podcast/podcasts/2109.10665/</link><description>


Goes over some of the different places RL can be used in RecSys.

Read full paper: https://arxiv.org/abs/2109.10665

Tags: Reinforcement Learning, Recommender Systems, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2109.10665/</guid><category>Reinforcement Learning</category><category>Recommender Systems</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2109.10665.mp3" length="17304480" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 20:05:20 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>NerfBaselines: A Framework for Standardized Evaluation of Novel View Synthesis Methods in Computer Vision</title><link>https://arjunsriva.com/podcast/podcasts/2406.17345/</link><description>


NerfBaselines addresses the inconsistent evaluation protocols in comparing novel view synthesis methods by providing a unified interface, ensuring reproducibility through containerization, and standardizing the evaluation protocol. By enabling the sharing of pre-trained checkpoints, it reduces computational costs and environmental impact. However, it relies on methods exposing the same interface and future directions involve exploring advanced evaluation metrics and addressing the computational cost of training.

Read full paper: https://arxiv.org/abs/2406.17345

Tags: 3D Vision, Computer Vision, Systems and Performance
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.17345/</guid><category>3D Vision</category><category>Computer Vision</category><category>Systems and Performance</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.17345.mp3" length="9757440" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 20:14:41 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>TiTok: A Transformer-based 1D Tokenization Approach for Image Generation</title><link>https://arjunsriva.com/podcast/podcasts/2406.07550/</link><description>


TiTok introduces a novel 1D tokenization method for image generation, enabling the representation of images with significantly fewer tokens while maintaining or surpassing the performance of existing 2D grid-based methods. The approach leverages a Vision Transformer architecture, two-stage training with proxy codes, and achieves remarkable speedup in training and inference. The research opens up new possibilities for efficient and high-quality image generation, with implications for various applications in computer vision and beyond.

Read full paper: https://arxiv.org/abs/2406.07550

Tags: Generative Models, Computer Vision, Transformers
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2406.07550/</guid><category>Generative Models</category><category>Computer Vision</category><category>Transformers</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2406.07550.mp3" length="12322560" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 21:16:30 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>DARTS: Differentiable Architecture Search</title><link>https://arjunsriva.com/podcast/podcasts/1806.09055/</link><description>


Key takeaways for engineers/specialists: DARTS introduces a continuous relaxation approach to architecture search, leveraging gradient descent for efficient optimization. It achieves state-of-the-art results on image classification and language modeling tasks with significantly less computational cost. Challenges include the gap between continuous and discrete architecture representation, computational cost of second-order approximation, and sensitivity to hyperparameters.

Read full paper: https://arxiv.org/abs/1806.09055

Tags: Deep Learning, Optimization, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/1806.09055/</guid><category>Deep Learning</category><category>Optimization</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/1806.09055.mp3" length="15036960" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 21:34:05 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Hyper Networks: A Novel Approach to Learning Weights in Deep Neural Networks</title><link>https://arjunsriva.com/podcast/podcasts/1609.09106/</link><description>


The key takeaways for engineers/specialists are: Hyper Networks introduce a meta-network (hypernetwork) that learns to generate weight structures for deep neural networks, providing flexibility and efficiency. Dynamic hypernetworks allow weights to adapt to input sequences, improving performance on sequential tasks. End-to-end training of hypernetworks with the main network leads to collaborative optimization and comparable or better performance with fewer parameters.

Read full paper: https://arxiv.org/abs/1609.09106

Tags: Deep Learning, Machine Learning, Neural Networks
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/1609.09106/</guid><category>Deep Learning</category><category>Machine Learning</category><category>Neural Networks</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/1609.09106.mp3" length="17034240" type="audio/mpeg"/><pubDate>Thu, 18 Jul 2024 21:55:50 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</title><link>https://arjunsriva.com/podcast/podcasts/2304.11277/</link><description>


FSDP addresses memory capacity challenges by sharding parameters across devices, employs communication optimizations to enhance efficiency, includes a rate limiter feature to control memory impact, offers user-friendly APIs for easy integration, achieved promising results on large models, enables broader applications in various domains, faces challenges in mathematical equivalence and handling shared parameters, and has potential research directions in adaptive sharding strategies, new communication primitives, and combining with other parallelism paradigms.

Read full paper: https://arxiv.org/abs/2304.11277

Tags: Systems and Performance, Deep Learning, Machine Learning
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2304.11277/</guid><category>Systems and Performance</category><category>Deep Learning</category><category>Machine Learning</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2304.11277.mp3" length="14442720" type="audio/mpeg"/><pubDate>Fri, 19 Jul 2024 22:05:19 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://arjunsriva.com/podcast/podcasts/2205.14135/</link><description>


FlashAttention is a novel algorithm that addresses the efficiency of Transformer models by improving speed and memory efficiency through IO-awareness. It reduces the number of memory accesses by dividing data into smaller blocks and loading them into fast memory, achieving practical speedups and enabling training on longer sequences. The algorithm also incorporates recomputation during the backward pass to minimize memory usage, delivering significant improvements in training large models like BERT and GPT-2.

Read full paper: https://arxiv.org/abs/2205.14135

Tags: Deep Learning, Transformers, Systems and Performance
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2205.14135/</guid><category>Deep Learning</category><category>Transformers</category><category>Systems and Performance</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2205.14135.mp3" length="9953280" type="audio/mpeg"/><pubDate>Fri, 19 Jul 2024 22:17:53 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Foundation Models in Decision Making: Roles, Challenges, and Opportunities</title><link>https://arjunsriva.com/podcast/podcasts/2303.04129/</link><description>


The paper proposes a framework for understanding the various roles of foundation models in decision making, including conditional generative models, representation learners, and interactive agents. Key takeaways include the use of foundation models for behavioral priors, world modeling, and generalization of knowledge across tasks and environments.

Read full paper: https://arxiv.org/abs/2303.04129

Tags: Artificial Intelligence, Machine Learning, Explainable AI
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2303.04129/</guid><category>Artificial Intelligence</category><category>Machine Learning</category><category>Explainable AI</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2303.04129.mp3" length="15606240" type="audio/mpeg"/><pubDate>Sat, 20 Jul 2024 08:27:38 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Retrieval-Enhanced Transformers (RETRO): A Semi-Parametric Approach to Enhance Performance of Large Language Models</title><link>https://arjunsriva.com/podcast/podcasts/2112.04426/</link><description>


The paper introduces the RETRO model, which leverages retrieval from a massive text database to enhance large language model performance without increasing model size. Key takeaways include the benefits of linear time complexity for retrieval, the use of frozen BERT for efficient retrieval, and the importance of addressing test set leakage in evaluation.

Read full paper: https://arxiv.org/abs/2112.04426

Tags: Natural Language Processing, Deep Learning, Systems and Performance
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2112.04426/</guid><category>Natural Language Processing</category><category>Deep Learning</category><category>Systems and Performance</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2112.04426.mp3" length="21521760" type="audio/mpeg"/><pubDate>Sat, 20 Jul 2024 08:30:29 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Gradient Low-Rank Projection (GaLore): Revolutionizing Memory-Efficient LLM Training</title><link>https://arjunsriva.com/podcast/podcasts/2403.03507/</link><description>
The paper introduces a new approach named Gradient Low-Rank Projection (GaLore) to train large language models (LLMs) with full parameter learning while being significantly more memory-efficient than existing techniques. GaLore dynamically switches between multiple low-rank subspaces to represent the gradient during training, enabling the exploration of different directions while maintaining memory savings.

GaLore offers a breakthrough in memory-efficient LLM training by reducing memory usage significantly while achieving performance comparable to full-rank training. It enables training of large models on limited hardware resources, democratizing LLM research and development. Future research directions include applying GaLore to various model architectures, enhancing memory efficiency further, and exploring elastic data distributed training using consumer-grade hardware.

Read full paper: https://arxiv.org/abs/2403.03507

Tags: Natural Language Processing, Optimization, Systems and Performance
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2403.03507/</guid><category>Natural Language Processing</category><category>Optimization</category><category>Systems and Performance</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2403.03507.mp3" length="12060960" type="audio/mpeg"/><pubDate>Wed, 24 Jul 2024 09:29:30 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>Unraveling the Connection between In-Context Learning and Gradient Descent in Transformers</title><link>https://arjunsriva.com/podcast/podcasts/2212.07677/</link><description>
The podcast discusses a paper that explores the relationship between in-context learning and gradient descent in Transformer models. It highlights how Transformers learn to learn by mimicking the behavior of gradient descent on input data, leading to improved few-shot learning capabilities and faster adaptation to new tasks.

On how Transformers leverage in-context learning mechanisms through gradient descent, enabling them to adapt to new tasks efficiently. Understanding this connection can help improve model generalization, enhance few-shot learning capabilities, and potentially lead to the development of more intelligent and adaptable AI systems.

Read full paper: https://arxiv.org/abs/2212.07677

Tags: Natural Language Processing, Deep Learning, Explainable AI
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2212.07677/</guid><category>Natural Language Processing</category><category>Deep Learning</category><category>Explainable AI</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2212.07677.mp3" length="11221920" type="audio/mpeg"/><pubDate>Wed, 24 Jul 2024 16:19:56 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item><item><title>ðVDB: A Deep-Learning Framework for Sparse, Large-Scale, and High-Performance Spatial Intelligence</title><link>https://arjunsriva.com/podcast/podcasts/2407.01781/</link><description>
The paper introduces ðVDB, a deep-learning framework designed to handle large-scale, sparse 3D data efficiently. It focuses on the IndexGrid structure and specialized GPU-accelerated operators for tasks like convolution, ray tracing, and sampling.

Engineers and specialists can benefit from ðVDB by leveraging its memory-efficient IndexGrid structure and specialized convolution kernels optimized for different sparsity patterns. The framework provides significant speed and memory efficiency improvements over existing frameworks, enabling more effective handling of large-scale, sparse 3D datasets in deep learning applications.

Read full paper: https://arxiv.org/abs/2407.01781

Tags: 3D Vision, Deep Learning, Systems and Performance
</description><guid isPermaLink="false">https://arjunsriva.com/podcast/podcasts/2407.01781/</guid><category>3D Vision</category><category>Deep Learning</category><category>Systems and Performance</category><enclosure url="https://arjunsriva.com/static/podcast_data/arxiv/audio/2407.01781.mp3" length="13744320" type="audio/mpeg"/><pubDate>Thu, 01 Aug 2024 21:27:09 +0530</pubDate><itunes:author>Arjun Srivastava</itunes:author></item></channel></rss>