male-1: Welcome to Byte-Sized Breakthroughs, the podcast where we dissect complex research and deliver the core insights directly to you. I'm your host, Alex Askwell, and today we're diving into a fascinating paper about distributed training of large language models. The title is 'Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch'. With me are Dr. Paige Turner, the lead researcher on the paper, and Professor Wyd Spectrum, an expert in distributed computing and machine learning. Welcome to the show!

female-1: Thanks, Alex. Great to be here.

female-2: It's a pleasure to join you both.

male-1: Paige, could you start by giving us some historical context? Why is this research important, and what problems is it trying to solve?

female-1: Certainly. As you know, training Large Language Models (LLMs) requires tremendous compute power, often spread across many hardware accelerators like GPUs or TPUs. The standard approach, known as data parallelism, involves distributing the training data across these accelerators. Each accelerator holds a copy of the model, processes a portion of the data, and then synchronizes gradients ‚Äì the direction and magnitude of adjustments to the model's parameters ‚Äì with the other accelerators to ensure everyone's on the same page. The catch is, these synchronizations need to happen frequently and require high bandwidth, low-latency communication links. This forces us to co-locate all these accelerators in a single data center, which is expensive and technically challenging.

male-1: So, it's a logistical bottleneck as much as a computational one?

female-1: Exactly. Recent research has focused on relaxing this co-location requirement. Algorithms like DiLoCo, which we introduced earlier, allow grouping accelerators into 'workers'. These workers train independently for a while and then synchronize less frequently. This reduces the overall communication demand but still requires high peak bandwidth during those synchronization events when *all* parameters need to be exchanged.

female-2: That makes sense. Professor Spectrum, from your perspective, what are the major challenges in this area of distributed training?

female-2: The core challenge is balancing compute efficiency with communication costs. Data parallelism is conceptually simple and well-supported, but its communication overhead scales poorly as model sizes increase. Techniques like gradient compression can help, but often at the cost of numerical stability or convergence speed. Federated learning offers communication-efficient alternatives, but adapting them to the scale and specific requirements of LLM training presents its own set of challenges. For instance, statistical heterogeneity is a common problem of federated learning - where each worker's dataset is significantly different. Furthermore, it is extremely difficult to scale up the number of clients in federated learning, since each client contributes to the training, and the system can be brought to a halt if even one client lags behind. So, we need solutions that address both communication bandwidth and latency while preserving model quality and scaling effectively to thousands of accelerators.

male-1: Okay, so that sets the stage. Paige, what are the key contributions of your paper? What innovations did you introduce with Streaming DiLoCo?

female-1: We improved DiLoCo in three main ways. First, we introduced *streaming synchronization*, where we synchronize only subsets of the model's parameters in sequence, rather than all at once. Imagine dividing the LLM into sections and syncing those sections one at a time instead of all at once. This reduces the peak bandwidth needed.

male-1: How do you divide the model into these subsets?

female-1: We looked at two partitioning patterns: sequential and strided. In sequential partitioning, each subset comprises consecutive transformer blocks. In strided partitioning, each subset is composed of interleaved transformer blocks. We found that the strided version generally offered slightly better compute utilization, so we used it as our default.

male-1: And the second improvement?

female-1: The second is *overlapping communication with computation*. We allow workers to continue training while the synchronization is happening in the background. Instead of waiting for the parameter updates to arrive, the workers keep crunching numbers. This helps hide the communication latency and decreases the overall training time. In practice, it is equivalent to starting the next round of training with the old parameters, and applying the freshly updated parameters only a few steps later.

male-1: And finally?

female-1: Third, we used *quantization* to compress the data exchanged between workers. We reduced the precision of the outer gradients ‚Äì the adjustments made to the model during synchronization ‚Äì to just four bits per parameter, using a float format we call E3M0. Accumulation of outer gradients is still done with higher precision (FP32) to maintain stability, but the bandwidth is drastically reduced. So, less bits transferred per synchronization round.

male-1: E3M0, for our listeners, that's a floating-point representation with 1 sign bit, 3 exponent bits, and zero mantissa bits. It‚Äôs a very compact representation. Professor Spectrum, what's your take on these innovations? Are they truly novel, or do they build on existing techniques?

female-2: They're a clever combination of existing ideas tailored to the specific challenges of LLM training. Streaming synchronization is related to partial update methods in federated learning, where only a subset of the model is exchanged. Overlapping communication is a common technique in high-performance computing to hide latency. Quantization has been used extensively for gradient compression. The novelty here lies in integrating these techniques in a way that addresses both peak bandwidth and communication latency simultaneously, without sacrificing model quality, resulting in a more communication-efficient system.

male-1: Paige, let's delve into the methodology. Can you walk us through the Streaming DiLoCo algorithm and how it differs from existing approaches like Data-Parallel and the original DiLoCo?

female-1: Sure. Let's contrast the new algorithm (Algorithm 2) to the original one (Algorithm 1). The core idea behind DiLoCo is to perform federated learning ‚Äì specifically, a bi-level optimization scheme called FedOpt ‚Äì on language models. We have 'M' replicas, or workers, each training independently on a different subset of the data for 'H' inner steps, where `H` is the synchronization frequency. Every 'H' steps, each replica computes an outer gradient representing the change in its parameters. These outer gradients are then averaged across all replicas and applied to update the shared parameters. DiLoCo synchronizes the *entire* model at once, every H steps. This is as costly as data-parallel training, but it's done far less frequently, thus amortizing the communication cost.

male-1: Okay, got it. And where does Streaming DiLoCo come in?

female-1: Streaming DiLoCo makes three key changes to that synchronization process. First, instead of communicating the entire outer gradient vector every 'H' steps, we only share a fragment 'p' of it more frequently. This reduces the peak bandwidth. Second, we overlap communication with computation using the parameter 'tau' (ùúè). So, at the beginning of the outer step, instead of waiting for the communication, we immediately start the new round of inner optimization. After 'tau' inner steps, we wait for the fragment to be received, apply the outer optimizer, and merge it with the currently optimized fragment using a mixing factor 'alpha'. This way, the training loop is not halted by the communication round.

male-1: I imagine this ‚Äòalpha‚Äô variable, this mixing factor, is pretty important. What values did you experiment with?

female-1: Yes, it dictates how we merge the shared parameters with the local ones. Alpha = 1 means no communication at all. Alpha = 0 means discarding all local inner updates, whereas alpha = 0.5 means uniform averaging of the locally updated values with the globally shared values. This allows practitioners to balance their compute cost, and synchronization cost, to achieve the best of both worlds.

male-1: And the third change is the quantization?

female-1: Correct. To further reduce the total amount of bits exchanged, we use the E3M0 (4-bit) float format for the outer gradients. This compression is applied when sending each replica's outer gradients, but the accumulation ‚Äì the averaging across replicas ‚Äì is done in FP32 for stability.

male-1: Paige, are all the fragments synchronized with the same frequency? I mean, are all those 't sub p' offsets in Algorithm 2, random?

female-1: Good question. The fragment `p` is synchronized if `t - t_p mod H == 0`. This ensures that each fragment is synchronized every 'H' steps, but since the `t_p` offsets are different for each fragment, the model as a whole synchronizes *some* fragment more frequently than every 'H' steps.

male-1: Professor Spectrum, what about the memory overhead of Streaming DiLoCo compared to standard Data-Parallel?

female-2: That‚Äôs an important consideration. In a typical data-parallel setup, you need memory for the model parameters and the Adam optimizer state, which usually doubles the memory footprint. DiLoCo adds the outer global parameters and the outer Nesterov state, increasing the memory overhead. The paper mentions that (streaming or not) DiLoCo requires 66% more memory. However, Streaming DiLoCo only needs a *subset* of the outer parameters at any given time, so they can offload the rest to CPU memory, reducing the HBM footprint at any given moment to a manageable size.

male-1: Let's move on to the experiments. Paige, can you describe the experimental setup, including the datasets, model architectures, and evaluation metrics?

female-1: We used a Chinchilla architecture for our models, ranging from 35 million to 4 billion parameters, all with a sequence length of 1024. We also used techniques like QKNorm and Z-loss for training stability. We trained all our models from scratch. The main dataset we used was the C4 dataset, and for the overtraining analysis, we used Dolma. For the smaller models, we performed a chinchilla-optimal number of steps, according to the model scale. For the architecture, we use from 6 layers (35M parameters) all the way to 36 layers (4B parameters), with a vocabulary size of 32,000. We used two DiLoCo replicas, each with FSDP across their closely located devices. For our DrJax implementation, we use an allreduce implementation for outer gradients, without any central parameter server. Finally, we built a DAG simulator that allowed us to estimate the compute utilization of each method.

male-1: And the evaluation metrics?

female-1: We used evaluation loss on C4, HellaSwag accuracy, Piqa accuracy, and Arc-Easy accuracy. We also used compute utilization, which measures the time spent on computation versus communication. The higher the number, the closer we are to 100% compute time, 0% waiting time. Finally, we also use peak bandwidth for communication, which measures the peak amount of bits transferred at one given moment.

male-1: What did you find?

female-1: Our experiments showed that Streaming DiLoCo achieves similar performance to Data-Parallel, but with significantly reduced bandwidth. For instance, with a 1 billion parameter model, Data-Parallel, DiLoCo, and Streaming DiLoCo all performed similarly in terms of evaluation loss and accuracy. However, Streaming DiLoCo with H=100, which means a synchronization frequency of 100 inner steps, used significantly less bandwidth. With a 4 billion parameter model, Streaming DiLoCo achieved comparable results to Data-Parallel with a reduction of 400x in the total amount of bits exchanged. With a 1 billion parameter model trained on Dolma, we note that both our method and Data-Parallel perform similarly, while the peak bandwidth is 8x lower, and the total bits exchanged is 400x lower for our method. Our most robust model showed 54.24 HellaSwag accuracy, 71.38 Piqa accuracy, and 41.92 Arc Easy Accuracy.

male-1: Impressive numbers! What about the compute utilization simulations? What insights did those provide?

female-1: The simulations highlighted the importance of overlapping communication with computation. We found that only by overlapping communication could we reach close to 100% compute utilization. The simulations also revealed a somewhat counter-intuitive result: that the required bandwidth can become *lower* as the model scale gets *larger* when overlapping communication with computation. This exploits the square-cube law of distributed training, where computation scales worse than communication. It is worth noting that there is a limit to this benefit, but for a wide array of LLM training, the training is compute bound, and we could very well benefit from the communication cost amortizing.

male-1: Professor Spectrum, does this mean the approach could particularly suit training very, very large models, where compute is the dominant factor?

female-2: Absolutely. As models grow, the communication overhead becomes a major bottleneck. If Streaming DiLoCo can effectively hide this overhead, it offers a significant advantage. Also, it will allow models to be trained without requiring extremely expensive supercomputers or data-centers, and allowing engineers to perform compute arbitrage - that is, leverage cheaper compute resources that might not have the best interconnection, and still achieve similar model training quality. Also, we are talking about training very *large* models, which should not be confused with training very *fast* model. In the case of requiring the shortest training time, data-parallelism is still the king of the hill, as it provides a fast synchronization across devices, but at the price of extremely expensive communication bandwidth infrastructure.

male-1: Paige, what were some of the ablation studies you conducted to isolate the impact of each component of Streaming DiLoCo?

female-1: We systematically ablated each of the three key components: streaming synchronization, overlapping communication, and quantization. For streaming synchronization, we varied the fragment size and the synchronization pattern. We found that a fragment size of three layers was a good trade-off between ML performance and bandwidth reduction, and that a strided pattern offered slightly better compute utilization. For overlapping communication, we varied the overlap parameter 'tau'. We found that degradation in evaluation loss was negligible up to an overlap of 10 inner steps. We therefore advise to limit the overlap to 5 inner steps, as there isn't any compute utilization gain in practice, after that. Finally, for quantization, we found that using a 4-bit float format, E3M0, did not affect performance, while value dropping was significantly worse.

male-1: You compared your streaming synchronization approach to a concurrent work called FedPart. What were the key differences and why did your approach perform better?

female-1: Yes, we compared to FedPart, which also shares per-layer fragments. However, their method freezes the non-shared layers during inner optimization. We believe this is inefficient because a large portion of the model is frozen at any point in time, despite doing forward/backward computation on it. Our experiments showed that freezing the non-synchronized layers results in a 20% increase of the evaluation loss.

male-1: Let‚Äôs address limitations. What are the shortcomings of Streaming DiLoCo, and where does it fall short?

female-1: There are a few. First, our simulation only considers the bandwidth *between* data centers and not the local bandwidth between devices *within* a data center. This means the simulation might not accurately reflect all real-world scenarios. Second, we used a fixed fragment size of 3 layers, which may not be optimal for all model scales or architectures. Third, our experiments primarily focused on the C4 and Dolma datasets. Performance on other datasets might vary. And fourth, DiLoCo's memory overhead is still higher (66%) compared to data parallelism, although it can be mitigated by CPU offloading. Also, we focus our work on small number of replicas, and how the results translates to a very large number of replicas remain unknown.

male-1: Professor Spectrum, what are the broader implications of this research, and what future directions do you see?

female-2: The broader impact is potentially significant. By reducing the communication bottleneck, Streaming DiLoCo could democratize access to LLM training, enabling researchers and smaller organizations with limited resources to participate. It could also lead to a shift in distributed training paradigms, promoting compute arbitrage and co-design of architectures. As for future directions, I see several promising avenues: Further investigation into optimal fragment sizes and synchronization patterns, exploring different outer optimizers and adaptive learning rate schedules, scaling the number of DiLoCo replicas, investigating more advanced gradient compression techniques, and applying Streaming DiLoCo to different tasks and datasets.

male-1: Paige, your paper mentions the term 'distributed free lunch'. Can you elaborate on what you mean by that?

female-1: We use that term to highlight that we can reach a similar compute utilization as the widely used Data-Parallel using two orders of magnitude less Gbit/s bandwidth, while performing comparably in terms of training loss and downstream evaluation accuracies as Data-Parallel. So, it‚Äôs a ‚Äòfree lunch‚Äô in the sense that we achieve similar performance with significantly reduced resources.

male-1: Thank you both. It's been enlightening to delve into the intricacies of Streaming DiLoCo. For our listeners, the key takeaway is that this research presents a promising approach to distributed LLM training that significantly reduces communication costs without sacrificing model quality. It leverages streaming synchronization, overlapping communication, and gradient quantization to achieve this efficiency, paving the way for more accessible and scalable LLM training in the future. It may not *actually* be a free lunch, as Dr. Turner jokingly called it, but it is surely getting close to it. It also leverages many exciting avenues of future research, which could enable a more efficient scaling of current and future models. That's all the time we have for today. Thanks for joining us on Byte-Sized Breakthroughs!

